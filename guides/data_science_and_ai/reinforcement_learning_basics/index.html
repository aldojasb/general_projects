
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://aldojasb.github.io/general_projects/guides/data_science_and_ai/reinforcement_learning_basics/">
      
      
        <link rel="prev" href="../the_neural_network_structure/">
      
      
        <link rel="next" href="../handson_rl_application_ppo/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>Reinforcement Learning Basics - Aldo Saltao's Portfolio</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reinforcement-learning-rl" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Aldo Saltao&#39;s Portfolio" class="md-header__button md-logo" aria-label="Aldo Saltao's Portfolio" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Aldo Saltao's Portfolio
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Reinforcement Learning Basics
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../work_with_me/overview/" class="md-tabs__link">
          
  
  Work With Me

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../software_architecture/overview/" class="md-tabs__link">
          
  
  Software Architecture

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../ai_product_development/overview/" class="md-tabs__link">
          
  
  AI Product Management

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../overview/" class="md-tabs__link">
          
  
  Data Science & AI

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../development_and_devops/overview/" class="md-tabs__link">
          
  
  Development & MLOps

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../demos/overview/" class="md-tabs__link">
          
  
  Demos

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Aldo Saltao&#39;s Portfolio" class="md-nav__button md-logo" aria-label="Aldo Saltao's Portfolio" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"/></svg>

    </a>
    Aldo Saltao's Portfolio
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Work With Me
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Work With Me
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../work_with_me/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../work_with_me/ai_systems_that_work/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI Systems That Work
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../work_with_me/ai_foundations_and_capability_building/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Architecture
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Software Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software_architecture/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software_architecture/SOLID_principles/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SOLID Principles
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    AI Product Management
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            AI Product Management
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai_product_development/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai_product_development/framework_that_embraces_uncertainty/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Framework That Embraces Uncertainty
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai_product_development/the_moscow_method/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The MoSCoW Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai_product_development/skateboard_mindset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Skateboard Mindset
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Data Science & AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Data Science & AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../the_neural_network_structure/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Neural Network Structure
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Reinforcement Learning Basics
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning Basics
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#rl-vs-traditional-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      RL vs Traditional Machine Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Rewards
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-hypothesis" class="md-nav__link">
    <span class="md-ellipsis">
      The Reward Hypothesis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#agents-actions" class="md-nav__link">
    <span class="md-ellipsis">
      Agent's Actions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#history-vs-state" class="md-nav__link">
    <span class="md-ellipsis">
      History vs. State
    </span>
  </a>
  
    <nav class="md-nav" aria-label="History vs. State">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-history" class="md-nav__link">
    <span class="md-ellipsis">
      What is the History?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-is-the-state" class="md-nav__link">
    <span class="md-ellipsis">
      What is the State?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-this-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why This Matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-markov-property" class="md-nav__link">
    <span class="md-ellipsis">
      The Markov Property
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Markov Property">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-rat-and-reward-puzzle" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Rat and Reward Puzzle
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-one-step-agentenvironment-loop-based-on-sutton-barto" class="md-nav__link">
    <span class="md-ellipsis">
      The One-Step Agent–Environment Loop (based on Sutton &amp; Barto)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The One-Step Agent–Environment Loop (based on Sutton & Barto)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sequence-of-interactions" class="md-nav__link">
    <span class="md-ellipsis">
      Sequence of Interactions:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#major-components-of-a-reinforcement-learning-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Major Components of a Reinforcement Learning Agent
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Major Components of a Reinforcement Learning Agent">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-policy-what-should-i-do" class="md-nav__link">
    <span class="md-ellipsis">
      1. Policy – “What should I do?”
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-value-function-how-good-is-this-state" class="md-nav__link">
    <span class="md-ellipsis">
      2. Value Function – “How good is this state?”
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-model-what-will-happen-next" class="md-nav__link">
    <span class="md-ellipsis">
      3. Model – “What will happen next?”
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#categorizing-rl-agents" class="md-nav__link">
    <span class="md-ellipsis">
      Categorizing RL Agents
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Categorizing RL Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-value-based-agents" class="md-nav__link">
    <span class="md-ellipsis">
      1. Value-Based Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-policy-based-agents" class="md-nav__link">
    <span class="md-ellipsis">
      2. Policy-Based Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-actor-critic-agents" class="md-nav__link">
    <span class="md-ellipsis">
      3. Actor-Critic Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-model-free-agents" class="md-nav__link">
    <span class="md-ellipsis">
      4. Model-Free Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-model-based-agents" class="md-nav__link">
    <span class="md-ellipsis">
      5. Model-Based Agents
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-main-approaches-to-develop-reinforcement-learning-agents" class="md-nav__link">
    <span class="md-ellipsis">
      Two Main Approaches to Develop Reinforcement Learning Agents
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Two Main Approaches to Develop Reinforcement Learning Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-planning-model-based-learning" class="md-nav__link">
    <span class="md-ellipsis">
      1. Planning (Model-Based Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-reinforcement-learning-model-free-learning" class="md-nav__link">
    <span class="md-ellipsis">
      2. Reinforcement Learning (Model-Free Learning)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Exploration vs. Exploitation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exploration vs. Exploitation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#balance-is-key" class="md-nav__link">
    <span class="md-ellipsis">
      Balance is key
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prediction-vs-control-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Prediction vs. Control Problems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prediction vs. Control Problems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-prediction-problem" class="md-nav__link">
    <span class="md-ellipsis">
      1. Prediction Problem:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-control-problem" class="md-nav__link">
    <span class="md-ellipsis">
      2. Control Problem:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dynamic-programming-dp-principles-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Dynamic Programming (DP) Principles and RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dynamic Programming (DP) Principles and RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-optimal-substructure" class="md-nav__link">
    <span class="md-ellipsis">
      1. Optimal Substructure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-overlapping-subproblems" class="md-nav__link">
    <span class="md-ellipsis">
      2. Overlapping Subproblems
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-dp-is-relevant-to-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Why DP is Relevant to RL?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations-the-heart-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equations – The Heart of RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bellman Equations – The Heart of RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-expectation-equation-for-a-given-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Expectation Equation (for a given policy π)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#planning-approach-by-using-dynamic-programming" class="md-nav__link">
    <span class="md-ellipsis">
      Planning approach by using Dynamic Programming
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Planning approach by using Dynamic Programming">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dp-is-used-for" class="md-nav__link">
    <span class="md-ellipsis">
      DP is used for:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DP is used for:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prediction-policy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Prediction (Policy Evaluation):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#control-policy-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Control (Policy Optimization):
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#which-rl-agent-categories-use-dp" class="md-nav__link">
    <span class="md-ellipsis">
      Which RL Agent Categories Use DP?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policy-based-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Policy-Based Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-you-cant-use-dp-with-pure-policy-based-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Why You Can't Use DP with Pure Policy-Based Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#so-what-do-we-use-instead-of-dp" class="md-nav__link">
    <span class="md-ellipsis">
      So What Do We Use Instead of DP?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="So What Do We Use Instead of DP?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-monte-carlo-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      1. Monte Carlo Policy Gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-actor-critic-methods" class="md-nav__link">
    <span class="md-ellipsis">
      2. Actor-Critic Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../handson_rl_application_ppo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on RL application (PPO) for Chemical Process Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ds_products_as_python_packages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Science Products as Python Packages
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../clustering_algorithms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Clustering Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../k-means_visualization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Clustering Algorithms Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Development & MLOps
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Development & MLOps
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development_and_devops/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development_and_devops/introduction_to_dev_container/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Dev-Container
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development_and_devops/logging_manager_package/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Loggin Manager Package
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development_and_devops/tdd_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Test-Driven Development (TDD)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development_and_devops/versioning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Versioning package on GitHub
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development_and_devops/mkdocs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation with mkdocs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Demos
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Demos
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#rl-vs-traditional-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      RL vs Traditional Machine Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Rewards
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-reward-hypothesis" class="md-nav__link">
    <span class="md-ellipsis">
      The Reward Hypothesis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#agents-actions" class="md-nav__link">
    <span class="md-ellipsis">
      Agent's Actions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#history-vs-state" class="md-nav__link">
    <span class="md-ellipsis">
      History vs. State
    </span>
  </a>
  
    <nav class="md-nav" aria-label="History vs. State">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-history" class="md-nav__link">
    <span class="md-ellipsis">
      What is the History?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-is-the-state" class="md-nav__link">
    <span class="md-ellipsis">
      What is the State?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-this-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why This Matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-markov-property" class="md-nav__link">
    <span class="md-ellipsis">
      The Markov Property
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Markov Property">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-rat-and-reward-puzzle" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Rat and Reward Puzzle
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-one-step-agentenvironment-loop-based-on-sutton-barto" class="md-nav__link">
    <span class="md-ellipsis">
      The One-Step Agent–Environment Loop (based on Sutton &amp; Barto)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The One-Step Agent–Environment Loop (based on Sutton & Barto)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sequence-of-interactions" class="md-nav__link">
    <span class="md-ellipsis">
      Sequence of Interactions:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#major-components-of-a-reinforcement-learning-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Major Components of a Reinforcement Learning Agent
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Major Components of a Reinforcement Learning Agent">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-policy-what-should-i-do" class="md-nav__link">
    <span class="md-ellipsis">
      1. Policy – “What should I do?”
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-value-function-how-good-is-this-state" class="md-nav__link">
    <span class="md-ellipsis">
      2. Value Function – “How good is this state?”
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-model-what-will-happen-next" class="md-nav__link">
    <span class="md-ellipsis">
      3. Model – “What will happen next?”
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#categorizing-rl-agents" class="md-nav__link">
    <span class="md-ellipsis">
      Categorizing RL Agents
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Categorizing RL Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-value-based-agents" class="md-nav__link">
    <span class="md-ellipsis">
      1. Value-Based Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-policy-based-agents" class="md-nav__link">
    <span class="md-ellipsis">
      2. Policy-Based Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-actor-critic-agents" class="md-nav__link">
    <span class="md-ellipsis">
      3. Actor-Critic Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-model-free-agents" class="md-nav__link">
    <span class="md-ellipsis">
      4. Model-Free Agents
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-model-based-agents" class="md-nav__link">
    <span class="md-ellipsis">
      5. Model-Based Agents
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-main-approaches-to-develop-reinforcement-learning-agents" class="md-nav__link">
    <span class="md-ellipsis">
      Two Main Approaches to Develop Reinforcement Learning Agents
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Two Main Approaches to Develop Reinforcement Learning Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-planning-model-based-learning" class="md-nav__link">
    <span class="md-ellipsis">
      1. Planning (Model-Based Learning)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-reinforcement-learning-model-free-learning" class="md-nav__link">
    <span class="md-ellipsis">
      2. Reinforcement Learning (Model-Free Learning)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Exploration vs. Exploitation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exploration vs. Exploitation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#balance-is-key" class="md-nav__link">
    <span class="md-ellipsis">
      Balance is key
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prediction-vs-control-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Prediction vs. Control Problems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prediction vs. Control Problems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-prediction-problem" class="md-nav__link">
    <span class="md-ellipsis">
      1. Prediction Problem:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-control-problem" class="md-nav__link">
    <span class="md-ellipsis">
      2. Control Problem:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dynamic-programming-dp-principles-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Dynamic Programming (DP) Principles and RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dynamic Programming (DP) Principles and RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-optimal-substructure" class="md-nav__link">
    <span class="md-ellipsis">
      1. Optimal Substructure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-overlapping-subproblems" class="md-nav__link">
    <span class="md-ellipsis">
      2. Overlapping Subproblems
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-dp-is-relevant-to-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Why DP is Relevant to RL?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equations-the-heart-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equations – The Heart of RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bellman Equations – The Heart of RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-expectation-equation-for-a-given-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Expectation Equation (for a given policy π)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#planning-approach-by-using-dynamic-programming" class="md-nav__link">
    <span class="md-ellipsis">
      Planning approach by using Dynamic Programming
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Planning approach by using Dynamic Programming">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dp-is-used-for" class="md-nav__link">
    <span class="md-ellipsis">
      DP is used for:
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DP is used for:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prediction-policy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Prediction (Policy Evaluation):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#control-policy-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Control (Policy Optimization):
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#which-rl-agent-categories-use-dp" class="md-nav__link">
    <span class="md-ellipsis">
      Which RL Agent Categories Use DP?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policy-based-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Policy-Based Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-you-cant-use-dp-with-pure-policy-based-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Why You Can't Use DP with Pure Policy-Based Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#so-what-do-we-use-instead-of-dp" class="md-nav__link">
    <span class="md-ellipsis">
      So What Do We Use Instead of DP?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="So What Do We Use Instead of DP?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-monte-carlo-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      1. Monte Carlo Policy Gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-actor-critic-methods" class="md-nav__link">
    <span class="md-ellipsis">
      2. Actor-Critic Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="reinforcement-learning-rl">Reinforcement Learning (RL)<a class="headerlink" href="#reinforcement-learning-rl" title="Permanent link">&para;</a></h1>
<p>Reinforcement Learning is a branch of Machine Learning where an agent <strong>learns by interacting with an environment</strong> to achieve a goal. It’s less about feeding the model labeled data (as in supervised learning) and more about <strong>trial, error, and feedback</strong>.</p>
<h2 id="rl-vs-traditional-machine-learning">RL vs Traditional Machine Learning<a class="headerlink" href="#rl-vs-traditional-machine-learning" title="Permanent link">&para;</a></h2>
<p>There are two key differences between <strong>Reinforcement Learning (RL)</strong> and typical <strong>Machine Learning (ML)</strong>:</p>
<ol>
<li><strong>Agent's Action</strong>
    In RL, the <strong>agent’s actions affect the data it receives next from the environment</strong>. In contrast, ML usually assumes the data is static and independent of it's outputs (predictions).</li>
<li><strong>Reward Signal</strong>
    RL uses a <strong>scalar reward</strong> to guide learning-  what actions lead to good outcomes through trial and error.</li>
</ol>
<h2 id="rewards">Rewards<a class="headerlink" href="#rewards" title="Permanent link">&para;</a></h2>
<p>A <strong>reward</strong> (denoted as <code>Rₜ</code>) is a scalar signal received after taking an action. It tells the agent how well it’s doing <strong>at time step t</strong>.</p>
<p>The goal of the agent is to <strong>maximize cumulative reward over time</strong>, not just perform well in the moment.</p>
<h2 id="the-reward-hypothesis">The Reward Hypothesis<a class="headerlink" href="#the-reward-hypothesis" title="Permanent link">&para;</a></h2>
<p>RL is built on the <strong>Reward Hypothesis</strong>:</p>
<blockquote>
<p>“All goals can be described as the maximization of expected cumulative rewards.”</p>
</blockquote>
<p>Example (Power Plant Control):</p>
<ul>
<li>+1 for producing more power efficiently</li>
<li>-10 for exceeding safety thresholds</li>
</ul>
<h2 id="agents-actions">Agent's Actions<a class="headerlink" href="#agents-actions" title="Permanent link">&para;</a></h2>
<p>The agent must choose <strong>a sequence of actions</strong> to maximize <strong>future rewards</strong>.</p>
<p>Caveats:</p>
<ul>
<li>Immediate rewards might not be the optimal.</li>
<li>Sometimes, <strong>sacrificing short-term gain</strong> leads to <strong>better long-term outcomes</strong>.</li>
</ul>
<h2 id="history-vs-state">History vs. State<a class="headerlink" href="#history-vs-state" title="Permanent link">&para;</a></h2>
<p>One of the most <strong>common sources of confusion</strong> in reinforcement learning is the difference between <strong>history</strong> and <strong>state</strong>.</p>
<blockquote>
<p>It might seem like the <em>state</em> should just be “everything that happened before” (i.e. the history), but in practice, <strong>state is a concise summary</strong> of that history—<strong>not the entire thing</strong>.</p>
</blockquote>
<h3 id="what-is-the-history">What is the History?<a class="headerlink" href="#what-is-the-history" title="Permanent link">&para;</a></h3>
<p>The <strong>history</strong> at time step <code>t</code> is the full record of everything the agent has experienced up to that point:</p>
<div class="highlight"><pre><span></span><code>Hₜ = <span class="nb">{</span>O₁, A₁, R₁, O₂, A₂, R₂, ..., Oₜ, Aₜ, Rₜ<span class="nb">}</span>
</code></pre></div>
<p>This contains:</p>
<ul>
<li>All past <strong>observations</strong> (<code>O</code>)</li>
<li>All taken <strong>actions</strong> (<code>A</code>)</li>
<li>All received <strong>rewards</strong> (<code>R</code>)</li>
</ul>
<p>It’s incredibly detailed—but often too bulky or unnecessary for real-time decision-making.</p>
<h3 id="what-is-the-state">What is the State?<a class="headerlink" href="#what-is-the-state" title="Permanent link">&para;</a></h3>
<p>A <strong>state</strong> <code>Sₜ</code> is a <strong>compressed representation</strong> of the history that contains all the information the agent needs <strong>to decide what to do next</strong>.</p>
<div class="highlight"><pre><span></span><code>Sₜ = f(Hₜ)
</code></pre></div>
<p>This function <code>f</code> summarizes the useful parts of history and discards the rest.</p>
<blockquote>
<p>A good state captures <strong>just enough</strong> about the past to make optimal decisions—nothing more, nothing less.</p>
</blockquote>
<h3 id="why-this-matters">Why This Matters<a class="headerlink" href="#why-this-matters" title="Permanent link">&para;</a></h3>
<ul>
<li>If you mistakenly treat the <strong>history</strong> as the <strong>state</strong>, your system becomes unnecessarily large and computationally expensive.</li>
<li>Worse, the agent may get <strong>distracted by irrelevant past details</strong>—leading to poor learning performance.</li>
</ul>
<p>This brings us to the <strong>Markov property</strong>, which tells us how to define a state that is truly useful for decision-making.</p>
<h2 id="the-markov-property">The Markov Property<a class="headerlink" href="#the-markov-property" title="Permanent link">&para;</a></h2>
<p>A state is <strong>Markov</strong> if it captures <strong>everything</strong> needed to determine the next step:</p>
<div class="highlight"><pre><span></span><code>P[Sₜ₊₁ | Sₜ] = P[Sₜ₊₁ | S₁, S₂, ..., Sₜ]
</code></pre></div>
<blockquote>
<p>“Given the present, the future is independent of the past.”</p>
</blockquote>
<p>This means:</p>
<ul>
<li>The agent doesn’t need full history.</li>
<li>A <strong>well-designed state</strong> simplifies learning and planning.</li>
</ul>
<h3 id="example-rat-and-reward-puzzle">Example: Rat and Reward Puzzle<a class="headerlink" href="#example-rat-and-reward-puzzle" title="Permanent link">&para;</a></h3>
<p><img alt="Rat Example" src="../../images/rat_example.png" /></p>
<p>Let’s say a <strong>rat agent</strong> presses levers and hears bells and sees lights before receiving cheese (reward).
Depending on how we define the state:</p>
<table>
<thead>
<tr>
<th><strong>State Design</strong></th>
<th><strong>What the Agent Sees</strong></th>
<th><strong>Pros / Cons</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Last 3 items in sequence</td>
<td>"Bell → Lever → Light"</td>
<td>Small input, fast decisions, might miss context</td>
</tr>
<tr>
<td>Count of bells/lights/levers so far</td>
<td>"3 bells, 4 lights, 5 levers"</td>
<td>Captures trends, not exact order</td>
</tr>
<tr>
<td>Full sequence (entire history)</td>
<td>"Bell, Bell, Light, Lever, Bell, Lever, Light..."</td>
<td>Accurate but computationally heavy</td>
</tr>
</tbody>
</table>
<p><strong>Insight:</strong> How you define the state <strong>greatly affects</strong> learning efficiency.</p>
<h2 id="the-one-step-agentenvironment-loop-based-on-sutton-barto">The One-Step Agent–Environment Loop (based on Sutton &amp; Barto)<a class="headerlink" href="#the-one-step-agentenvironment-loop-based-on-sutton-barto" title="Permanent link">&para;</a></h2>
<p>At the heart of every Reinforcement Learning system lies a <strong>continuous interaction loop</strong> between the <strong>agent</strong> and the <strong>environment</strong>, formalized as a <strong>Markov Decision Process (MDP)</strong>.</p>
<h3 id="sequence-of-interactions">Sequence of Interactions:<a class="headerlink" href="#sequence-of-interactions" title="Permanent link">&para;</a></h3>
<p>At each discrete time step <code>t</code>, the following happens:</p>
<ol>
<li>The <strong>agent</strong> observes the current <strong>state</strong> <code>Sₜ</code>.</li>
<li>Based on <code>Sₜ</code>, the agent chooses and executes an <strong>action</strong> <code>Aₜ</code>.</li>
<li>The <strong>environment</strong> receives the action <code>Aₜ</code>, responds with:</li>
<li>A <strong>new state</strong> <code>Sₜ₊₁</code></li>
<li>A <strong>reward</strong> <code>Rₜ₊₁</code></li>
<li>The agent receives <code>Rₜ₊₁</code> and <code>Sₜ₊₁</code>, then repeats the cycle at <code>t+1</code>.</li>
</ol>
<p>This interaction yields a trajectory like:</p>
<div class="highlight"><pre><span></span><code>S₀, A₀, R₁, S₁, A₁, R₂, S₂, A₂, ...
</code></pre></div>
<p>This diagram, from Sutton &amp; Barto’s book, formalizes the MDP loop:</p>
<ul>
<li>The <strong>agent</strong> produces an action <code>Aₜ</code>.</li>
<li>The <strong>environment</strong> receives <code>Aₜ</code>, and in response:</li>
<li>Emits the <strong>next state</strong> <code>Sₜ₊₁</code></li>
<li>Emits a <strong>reward</strong> <code>Rₜ₊₁</code></li>
</ul>
<p><img alt="RL diagram" src="../../images/rl_one_step.png" /></p>
<blockquote>
<p>Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement Learning: An Introduction</em> (2nd ed., in progress). http://incompleteideas.net/book/the-book-2nd.html</p>
</blockquote>
<p>Each new state influences future actions, forming a feedback loop. Importantly, this formulation obeys the <strong>Markov property</strong>—meaning future outcomes depend only on the current state and action, not on the full history.</p>
<h2 id="major-components-of-a-reinforcement-learning-agent">Major Components of a Reinforcement Learning Agent<a class="headerlink" href="#major-components-of-a-reinforcement-learning-agent" title="Permanent link">&para;</a></h2>
<p>An RL agent can be thought of as a system that makes decisions by learning from interaction. It may be composed of <strong>three core components</strong>:</p>
<hr />
<h3 id="1-policy-what-should-i-do">1. <strong>Policy</strong> – “What should I do?”<a class="headerlink" href="#1-policy-what-should-i-do" title="Permanent link">&para;</a></h3>
<p>A <strong>policy</strong> defines the agent’s behavior: <strong>how it chooses actions</strong> based on the current state.</p>
<ul>
<li><strong>Deterministic Policy</strong>: always chooses the same action for a given state.</li>
</ul>
<div class="highlight"><pre><span></span><code>a = π(s)
</code></pre></div>
<ul>
<li><strong>Stochastic Policy</strong>: chooses actions according to a probability distribution.</li>
</ul>
<div class="highlight"><pre><span></span><code>π(a|s) = P[ Aₜ = a | Sₜ = s ]
</code></pre></div>
<p>Used when exploration is important or the environment is noisy.</p>
<hr />
<h3 id="2-value-function-how-good-is-this-state">2. <strong>Value Function</strong> – “How good is this state?”<a class="headerlink" href="#2-value-function-how-good-is-this-state" title="Permanent link">&para;</a></h3>
<p>A <strong>value function</strong> estimates how much <strong>future reward</strong> the agent can expect from a given state (or state-action pair).</p>
<ul>
<li>Helps the agent choose <strong>better states</strong> over time.</li>
<li>Doesn’t directly pick actions, but it <strong>influences decisions</strong> when paired with a policy.</li>
</ul>
<div class="highlight"><pre><span></span><code>V(s) = expected cumulative reward from state s onward
</code></pre></div>
<hr />
<h3 id="3-model-what-will-happen-next">3. <strong>Model</strong> – “What will happen next?”<a class="headerlink" href="#3-model-what-will-happen-next" title="Permanent link">&para;</a></h3>
<p>A <strong>model</strong> is the agent’s internal understanding of how the environment behaves.</p>
<ul>
<li>It predicts:</li>
<li><strong>Next state</strong>: <code>Sₜ₊₁</code></li>
<li><strong>Reward</strong>: <code>Rₜ₊₁</code></li>
<li>Allows the agent to <strong>simulate outcomes</strong> before acting.</li>
<li>Not all RL agents use a model.</li>
</ul>
<h2 id="categorizing-rl-agents">Categorizing RL Agents<a class="headerlink" href="#categorizing-rl-agents" title="Permanent link">&para;</a></h2>
<p>Let’s now organize RL agents by how they combine the components above, and clarify <strong>pros, cons, and examples</strong> for each category.</p>
<hr />
<h3 id="1-value-based-agents">1. <strong>Value-Based Agents</strong><a class="headerlink" href="#1-value-based-agents" title="Permanent link">&para;</a></h3>
<ul>
<li>Uses: <strong>Value Function</strong></li>
<li>Doesn’t explicitly use a <strong>Policy</strong> (it’s implicit—derived from the value function)</li>
<li>Doesn’t use a <strong>Model</strong></li>
</ul>
<p>Example:</p>
<ul>
<li><strong>Q-Learning</strong> estimates the value of state-action pairs (<code>Q(s,a)</code>) and acts greedily.</li>
<li><strong>SARSA</strong> (State-Action-Reward-State-Action): Like Q-learning, but updates values using the actual next action taken.</li>
</ul>
<p>Pros:</p>
<ul>
<li>Simple and widely used.</li>
<li>Good for discrete action spaces.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Struggles with continuous or high-dimensional actions.</li>
<li>Doesn’t directly represent policy (requires argmax tricks).</li>
</ul>
<hr />
<h3 id="2-policy-based-agents">2. <strong>Policy-Based Agents</strong><a class="headerlink" href="#2-policy-based-agents" title="Permanent link">&para;</a></h3>
<ul>
<li>Uses: <strong>Policy</strong></li>
<li>Doesn’t use a <strong>Value Function</strong></li>
<li>Doesn’t use a <strong>Model</strong></li>
</ul>
<p>Example: </p>
<ul>
<li><strong>REINFORCE</strong>: A basic <strong>policy gradient</strong> method that updates the policy to maximize expected reward.</li>
</ul>
<p>Pros:</p>
<ul>
<li>Naturally handles <strong>continuous action spaces</strong>.</li>
<li>Can learn <strong>stochastic</strong> or <strong>deterministic</strong> policies.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Higher variance in updates.</li>
<li>Often less sample-efficient than value-based methods.</li>
</ul>
<hr />
<h3 id="3-actor-critic-agents">3. <strong>Actor-Critic Agents</strong><a class="headerlink" href="#3-actor-critic-agents" title="Permanent link">&para;</a></h3>
<ul>
<li>Uses: <strong>Policy</strong></li>
<li>Uses: <strong>Value Function</strong></li>
<li>Doesn’t use a <strong>Model</strong></li>
</ul>
<p>Example:</p>
<ul>
<li>
<p><strong>PPO</strong> – Proximal Policy Optimization: Balances learning progress and stability using a <strong>clipped objective</strong>.</p>
</li>
<li>
<p><strong>A2C</strong> – Advantage Actor-Critic: Computes advantage estimates to reduce variance and improve stability.</p>
</li>
<li>
<p><strong>A3C</strong> – Asynchronous Advantage Actor-Critic: Runs multiple agents in parallel with independent environments.</p>
</li>
<li>
<p><strong>DDPG</strong> – Deep Deterministic Policy Gradient: For <strong>continuous action spaces</strong>. Actor-critic with deterministic policies.</p>
</li>
<li>
<p><strong>SAC</strong> – Soft Actor-Critic: Adds <strong>entropy regularization</strong> to encourage exploration.</p>
</li>
</ul>
<p>Pros:</p>
<ul>
<li>Combines <strong>low variance</strong> from value-based with <strong>direct optimization</strong> of policy.</li>
<li>Very popular for complex environments.</li>
</ul>
<p>Cons:</p>
<ul>
<li>More complex architecture.</li>
<li>Balancing value and policy updates can be tricky.</li>
</ul>
<hr />
<h3 id="4-model-free-agents">4. <strong>Model-Free Agents</strong><a class="headerlink" href="#4-model-free-agents" title="Permanent link">&para;</a></h3>
<ul>
<li>Uses: <strong>Policy and/or Value Function</strong></li>
<li>Doesn’t use a <strong>Model</strong></li>
</ul>
<p>Examples:</p>
<ul>
<li>Q-learning, PPO, DQN, A3C, REINFORCE</li>
</ul>
<p>Pros:</p>
<ul>
<li>Easier to implement and train.</li>
<li>No need to learn or assume environment dynamics.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Can be <strong>sample inefficient</strong>—needs lots of interactions.</li>
<li>Less suitable for planning or simulations.</li>
</ul>
<hr />
<h3 id="5-model-based-agents">5. <strong>Model-Based Agents</strong><a class="headerlink" href="#5-model-based-agents" title="Permanent link">&para;</a></h3>
<ul>
<li>Uses: <strong>Model</strong></li>
<li>May also use: <strong>Policy and/or Value Function</strong></li>
</ul>
<p>Pros:</p>
<ul>
<li><strong>Sample efficient</strong>—can plan and simulate.</li>
<li>Useful when real-world data is costly.</li>
</ul>
<p>Cons:</p>
<ul>
<li>Requires learning or designing an accurate model.</li>
<li>Model errors can lead to bad decisions (model bias).</li>
</ul>
<h2 id="two-main-approaches-to-develop-reinforcement-learning-agents">Two Main Approaches to Develop Reinforcement Learning Agents<a class="headerlink" href="#two-main-approaches-to-develop-reinforcement-learning-agents" title="Permanent link">&para;</a></h2>
<p>In reinforcement learning, agents can learn through <strong>two broad approaches</strong>, depending on <strong>how much they know about the environment</strong> at the start:</p>
<hr />
<h3 id="1-planning-model-based-learning">1. <strong>Planning (Model-Based Learning)</strong><a class="headerlink" href="#1-planning-model-based-learning" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The agent has a model of the environment.</p>
</blockquote>
<ul>
<li>In this approach, the agent <strong>knows in advance</strong> how the environment works (i.e., it has a <strong>model</strong> that describes the dynamics: what happens when it takes an action).</li>
<li>Instead of learning through direct interaction, the agent <strong>simulates outcomes internally</strong> using the model.</li>
</ul>
<div class="highlight"><pre><span></span><code>State → Model → Simulated Next State <span class="nb">&amp;</span> Reward → Policy Update
</code></pre></div>
<p>Great for low-risk, fast iteration
Not applicable if the environment is unknown or too complex to model accurately</p>
<hr />
<h3 id="2-reinforcement-learning-model-free-learning">2. <strong>Reinforcement Learning (Model-Free Learning)</strong><a class="headerlink" href="#2-reinforcement-learning-model-free-learning" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The agent learns everything through direct interaction with the environment.</p>
</blockquote>
<ul>
<li>The agent <strong>doesn't know the rules</strong> of the world it lives in.</li>
<li>It must <strong>explore</strong>, <strong>collect experience</strong>, and <strong>learn from trial and error</strong>.</li>
<li>This is the most common setup in real-world problems where dynamics are unknown or too complex to define upfront.</li>
</ul>
<div class="highlight"><pre><span></span><code>State → Real Action → Observation <span class="nb">&amp;</span> Reward → Policy Update
</code></pre></div>
<p>More flexible and general
Typically requires a lot more data (sample inefficient)</p>
<hr />
<p><strong>Example 1: Learning to Play Tic-Tac-Toe</strong></p>
<table>
<thead>
<tr>
<th><strong>Planning Approach</strong></th>
<th><strong>Reinforcement Learning Approach</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>The agent is given <strong>all game rules</strong>, and uses <strong>search algorithms</strong> (like Minimax) to simulate future moves and choose the best one.</td>
<td>The agent <strong>plays thousands of games</strong>, learns from wins/losses, and gradually discovers winning strategies via trial and error.</td>
</tr>
<tr>
<td>Efficient learning with a known model</td>
<td>No prior knowledge required</td>
</tr>
</tbody>
</table>
<p><strong>Example 2: A Robot Learning to Navigate a Warehouse</strong></p>
<table>
<thead>
<tr>
<th><strong>Planning Approach</strong></th>
<th><strong>Reinforcement Learning Approach</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>The robot is given a <strong>map of the warehouse</strong>, and simulates paths to find the most efficient one using A* or Dijkstra’s algorithm.</td>
<td>The robot starts without a map, explores randomly, and learns optimal paths based on feedback (e.g., delivery success, collision penalties).</td>
</tr>
<tr>
<td>Works well in static and known settings</td>
<td>Adapts to real-world changes like obstacles or delays</td>
</tr>
</tbody>
</table>
<h2 id="exploration-vs-exploitation">Exploration vs. Exploitation<a class="headerlink" href="#exploration-vs-exploitation" title="Permanent link">&para;</a></h2>
<p>A <strong>core challenge</strong> in RL is choosing between:</p>
<ul>
<li><strong>Exploitation</strong>: Use what you already know to get high reward now</li>
<li><strong>Exploration</strong>: Try new actions to potentially discover better rewards later</li>
</ul>
<h3 id="balance-is-key">Balance is key<a class="headerlink" href="#balance-is-key" title="Permanent link">&para;</a></h3>
<p>If the agent <strong>only exploits</strong>, it may miss better strategies.
 If it <strong>only explores</strong>, it may never get good at anything.</p>
<p>Real-World Analogies:</p>
<table>
<thead>
<tr>
<th>Example</th>
<th><strong>Exploitation</strong></th>
<th><strong>Exploration</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Restaurant Choice</td>
<td>Go to your favorite Italian place</td>
<td>Try a new sushi bar</td>
</tr>
<tr>
<td>Online Ads</td>
<td>Show top-performing ad</td>
<td>Show a new variation</td>
</tr>
<tr>
<td>Oil Drilling</td>
<td>Drill where oil was found before</td>
<td>Explore untested land</td>
</tr>
<tr>
<td>Game Playing</td>
<td>Play the best-known chess move</td>
<td>Try an unconventional strategy</td>
</tr>
</tbody>
</table>
<blockquote>
<p>These examples show how <strong>short-term gain vs. long-term learning</strong> is a universal tension.</p>
</blockquote>
<h2 id="prediction-vs-control-problems">Prediction vs. Control Problems<a class="headerlink" href="#prediction-vs-control-problems" title="Permanent link">&para;</a></h2>
<p>Reinforcement Learning problems fall into two broad types:</p>
<hr />
<h3 id="1-prediction-problem">1. <strong>Prediction Problem</strong>:<a class="headerlink" href="#1-prediction-problem" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>Evaluate</strong> how good a policy is.</p>
</blockquote>
<p><strong>Task:</strong></p>
<p>Given a fixed policy <code>π</code>, estimate the <strong>value function</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Expected</span><span class="w"> </span><span class="n">future</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">starting</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="err">π</span>
</code></pre></div>
<p><strong>Example Use Case:</strong></p>
<ul>
<li>You’ve built a robot's policy and want to estimate <em>how good it is</em>.</li>
<li>You use methods like:</li>
<li><strong>Monte Carlo Estimation</strong></li>
<li><strong>Temporal Difference (TD) Learning</strong></li>
</ul>
<hr />
<h3 id="2-control-problem">2. <strong>Control Problem</strong>:<a class="headerlink" href="#2-control-problem" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>Find the best policy</strong></p>
</blockquote>
<p><strong>Task:</strong></p>
<p>Optimize the policy <code>π</code> to maximize reward.</p>
<p>This is harder because:</p>
<ul>
<li>You’re both <strong>evaluating</strong> and <strong>improving</strong> the policy.</li>
<li>The agent must explore, learn value estimates, and update the policy iteratively.</li>
</ul>
<p><strong>Example Algorithms:</strong></p>
<table>
<thead>
<tr>
<th><strong>Method</strong></th>
<th><strong>Solves</strong></th>
<th><strong>Approach</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Q-learning</strong></td>
<td>Control</td>
<td>Value-based: improve policy via Q-values</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>Control</td>
<td>Value-based with on-policy updates</td>
</tr>
<tr>
<td><strong>REINFORCE</strong></td>
<td>Control</td>
<td>Policy-based: improve directly via gradients</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td>Control</td>
<td>Actor-critic: uses both policy + value function</td>
</tr>
<tr>
<td><strong>Dyna-Q</strong></td>
<td>Control (with model)</td>
<td>Uses planning + learning</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="dynamic-programming-dp-principles-and-rl">Dynamic Programming (DP) Principles and RL<a class="headerlink" href="#dynamic-programming-dp-principles-and-rl" title="Permanent link">&para;</a></h2>
<p>Dynamic Programming (DP) is a <strong>powerful method for solving sequential decision problems</strong>. It forms the theoretical backbone of many reinforcement learning algorithms.</p>
<p>DP is applicable when the problem has <strong>two key properties</strong>:</p>
<hr />
<h3 id="1-optimal-substructure">1. Optimal Substructure<a class="headerlink" href="#1-optimal-substructure" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The solution to the overall problem can be <strong>built by combining solutions to its subproblems</strong>.</p>
</blockquote>
<p>This is the classic <strong>"divide and conquer"</strong> idea.</p>
<p><strong>Example 1: Shortest Path in a Maze</strong></p>
<ul>
<li>To find the shortest path from A to C, you can split the problem:</li>
<li>First solve from A to B</li>
<li>Then from B to C</li>
<li>Combine the results to find the total shortest path from A to C.</li>
</ul>
<hr />
<h3 id="2-overlapping-subproblems">2. Overlapping Subproblems<a class="headerlink" href="#2-overlapping-subproblems" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>Subproblems recur often</strong>, so instead of solving them over and over, we <strong>cache</strong> (store) their solutions for reuse.</p>
</blockquote>
<p><strong>Example: Robot Vacuum Cleaning a House</strong></p>
<p>Imagine a robot vacuum learning the <strong>shortest cleaning path</strong> through a house.</p>
<ul>
<li>It must figure out how to go from Room A → Room D.</li>
<li>But to do that, it repeatedly needs to solve:</li>
<li>What's the best way from Room A → Room B?</li>
<li>And from Room B → Room C?</li>
<li>And so on…</li>
</ul>
<p>Since these room-to-room paths are shared across many larger routes, <strong>the same subpaths appear over and over</strong>.</p>
<blockquote>
<p>Once the robot has figured out how to go from Room A to B efficiently, it can <strong>store that result</strong> and <strong>reuse it</strong> whenever any larger route requires it.</p>
</blockquote>
<p>This makes the learning process <strong>faster</strong> and <strong>more efficient</strong>, just like <strong>memoization</strong> in classic dynamic programming.</p>
<h2 id="why-dp-is-relevant-to-rl">Why DP is Relevant to RL?<a class="headerlink" href="#why-dp-is-relevant-to-rl" title="Permanent link">&para;</a></h2>
<p>Markov Decision Processes (MDPs) - the foundation of RL - satisfy both:</p>
<ul>
<li>Optimal Substructure </li>
<li>Overlapping Subproblems </li>
</ul>
<p>Thus, <strong>we can use DP to solve RL problems</strong> (<em><u>when the model is known</u></em>).</p>
<h2 id="bellman-equations-the-heart-of-rl">Bellman Equations – The Heart of RL<a class="headerlink" href="#bellman-equations-the-heart-of-rl" title="Permanent link">&para;</a></h2>
<p>These equations give RL its <strong>recursive structure</strong>. They define how value functions can be <strong>broken down into immediate reward plus future value</strong>.</p>
<hr />
<h3 id="bellman-expectation-equation-for-a-given-policy">Bellman Expectation Equation (for a given policy π)<a class="headerlink" href="#bellman-expectation-equation-for-a-given-policy" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">V</span><span class="err">π</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E</span><span class="p">[</span><span class="w"> </span><span class="n">R</span><span class="err">ₜ₊₁</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">γ</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">V</span><span class="err">π</span><span class="p">(</span><span class="n">S</span><span class="err">ₜ₊₁</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">S</span><span class="err">ₜ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="p">]</span>
</code></pre></div>
<p><strong>Interpretation:</strong></p>
<blockquote>
<p>The value of being in state <code>s</code> (under policy π) = the <strong>expected reward right now</strong> plus the <strong>discounted value of where you’ll land next</strong>, if you keep following π.</p>
</blockquote>
<p>It answers:</p>
<blockquote>
<p>“If I follow this policy, how much reward can I expect to accumulate starting from this state?”</p>
</blockquote>
<p><strong>Real-World Analogy:</strong></p>
<p>You're using Google Maps to follow a <strong>predefined route</strong> to a destination.</p>
<ul>
<li><code>Rₜ₊₁</code> = how enjoyable or efficient your <strong>next road segment</strong> is (traffic, views, fuel efficiency)</li>
<li><code>Vπ(Sₜ₊₁)</code> = how promising the <strong>rest of the trip</strong> looks from the next junction</li>
<li><code>γ</code> = how far you care about future road conditions (e.g., close trip vs. cross-country)</li>
</ul>
<p>So the value of your current location = how good the next step is + how good things look after that.</p>
<hr />
<h3 id="bellman-optimality-equation">Bellman Optimality Equation<a class="headerlink" href="#bellman-optimality-equation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">V</span><span class="o">*</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max</span><span class="err">ₐ</span><span class="w"> </span><span class="n">E</span><span class="p">[</span><span class="w"> </span><span class="n">R</span><span class="err">ₜ₊₁</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">γ</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">V</span><span class="o">*</span><span class="p">(</span><span class="n">S</span><span class="err">ₜ₊₁</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">S</span><span class="err">ₜ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="err">ₜ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">]</span>
</code></pre></div>
<p><strong>Interpretation:</strong></p>
<blockquote>
<p>The optimal value of a state is the <strong>maximum expected return</strong> achievable by choosing the <strong>best action now</strong>, assuming we act optimally from that point on.</p>
</blockquote>
<p>It tells us how to compute <strong>V*(s)</strong> by evaluating all possible actions and choosing the best.</p>
<p><strong>Real-World Analogy:</strong></p>
<p>You're now <strong>choosing your own route</strong> on Google Maps, not following a fixed one.</p>
<ul>
<li>You consider <strong>all exits from the roundabout</strong>.</li>
<li>For each one, you calculate:</li>
<li>“What’s the reward from taking this exit?”</li>
<li>“How good are things from there on?”</li>
</ul>
<p>Then, you <strong>choose the best path</strong>.</p>
<hr />
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Equation</th>
<th>Purpose</th>
<th>Intuition</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bellman Expectation</strong></td>
<td>Evaluate a given policy π</td>
<td>“What happens if I follow the instructions?”</td>
</tr>
<tr>
<td><strong>Bellman Optimality</strong></td>
<td>Find the best policy</td>
<td>“What’s the smartest action I can take now?”</td>
</tr>
</tbody>
</table>
<h2 id="planning-approach-by-using-dynamic-programming">Planning approach by using Dynamic Programming<a class="headerlink" href="#planning-approach-by-using-dynamic-programming" title="Permanent link">&para;</a></h2>
<p>Because DP assumes <strong>full knowledge of the MDP</strong>  - i.e., transition probabilities <code>P</code>, rewards <code>R</code>, and state/action sets - it’s used primarily for <strong>planning (Model-Based Learning)</strong>.</p>
<hr />
<h3 id="dp-is-used-for">DP is used for:<a class="headerlink" href="#dp-is-used-for" title="Permanent link">&para;</a></h3>
<h4 id="prediction-policy-evaluation">Prediction (Policy Evaluation):<a class="headerlink" href="#prediction-policy-evaluation" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Input</strong>: MDP ⟨S, A, P, R, γ⟩ and policy π</li>
<li><strong>Output</strong>: Value function <code>Vπ(s)</code></li>
</ul>
<h4 id="control-policy-optimization">Control (Policy Optimization):<a class="headerlink" href="#control-policy-optimization" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Input</strong>: MDP ⟨S, A, P, R, γ⟩</li>
<li><strong>Output</strong>: Optimal value function <code>V*</code>, and optimal policy <code>π*</code></li>
</ul>
<h2 id="which-rl-agent-categories-use-dp">Which RL Agent Categories Use DP?<a class="headerlink" href="#which-rl-agent-categories-use-dp" title="Permanent link">&para;</a></h2>
<p>Here’s how DP relates to the <strong>RL agent categories</strong> you explored earlier:</p>
<table>
<thead>
<tr>
<th><strong>Agent Type</strong></th>
<th><strong>Uses DP?</strong></th>
<th><strong>How?</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Value-Based</strong></td>
<td>Yes</td>
<td>Uses Bellman backups to compute value functions (e.g., via Q-learning)</td>
</tr>
<tr>
<td><strong>Policy-Based</strong></td>
<td>No</td>
<td>Optimizes policies directly without relying on value recursion</td>
</tr>
<tr>
<td><strong>Actor-Critic</strong></td>
<td>Partially</td>
<td>Critic uses value estimates based on Bellman updates (TD, advantage)</td>
</tr>
<tr>
<td><strong>Model-Free</strong></td>
<td>Indirectly</td>
<td>Value updates follow Bellman logic, but with sampled experience only</td>
</tr>
<tr>
<td><strong>Model-Based</strong></td>
<td>Fully</td>
<td>Can use <strong>DP-style backups</strong> by simulating transitions from the model</td>
</tr>
</tbody>
</table>
<h2 id="policy-based-algorithms">Policy-Based Algorithms<a class="headerlink" href="#policy-based-algorithms" title="Permanent link">&para;</a></h2>
<p><strong>Policy-based methods</strong>:</p>
<ul>
<li><strong>Don’t use value functions</strong> explicitly (though they sometimes do, like in actor-critic).</li>
<li><strong>Don’t perform backups or rely on recursive decomposition via the Bellman equation.</strong></li>
<li>Instead, they treat the <strong>policy itself as the thing to optimize</strong>.</li>
</ul>
<h2 id="why-you-cant-use-dp-with-pure-policy-based-algorithms">Why You Can't Use DP with Pure Policy-Based Algorithms<a class="headerlink" href="#why-you-cant-use-dp-with-pure-policy-based-algorithms" title="Permanent link">&para;</a></h2>
<p>Dynamic Programming works by recursively estimating values of states or actions.
But <strong>in policy gradient methods</strong>, the policy is <strong>parameterized (e.g., with a neural network)</strong>, and we <strong>directly adjust the parameters to maximize expected return</strong>—not via value recursion.</p>
<p>So:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>DP-based methods</strong></th>
<th><strong>Policy-based methods</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>What is updated?</td>
<td><code>V(s)</code> or <code>Q(s,a)</code> using Bellman equations</td>
<td><code>π(a)</code></td>
</tr>
<tr>
<td>What is needed?</td>
<td>Full model (<code>P</code>, <code>R</code>)</td>
<td>No model needed (can use sampled rewards)</td>
</tr>
<tr>
<td>Algorithm type</td>
<td>Planning</td>
<td>Optimization via stochastic gradient ascent</td>
</tr>
<tr>
<td>Backup type</td>
<td>Recursive</td>
<td>Sample-based Monte Carlo or TD estimates</td>
</tr>
</tbody>
</table>
<h2 id="so-what-do-we-use-instead-of-dp">So What Do We Use Instead of DP?<a class="headerlink" href="#so-what-do-we-use-instead-of-dp" title="Permanent link">&para;</a></h2>
<p>Policy-based algorithms use <strong>gradient-based optimization</strong>, not Bellman backups.</p>
<p>There are two major categories:</p>
<hr />
<h3 id="1-monte-carlo-policy-gradient">1. <strong>Monte Carlo Policy Gradient</strong><a class="headerlink" href="#1-monte-carlo-policy-gradient" title="Permanent link">&para;</a></h3>
<ul>
<li>Estimate total return from sampled episodes.</li>
<li>Use this to compute the gradient of the expected return w.r.t. the policy parameters θ.</li>
</ul>
<p><strong>Classic example: REINFORCE</strong></p>
<div class="highlight"><pre><span></span><code><span class="err">θ</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="err">θ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">α</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">∇θ</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="err">πθ</span><span class="p">(</span><span class="n">a</span><span class="o">|</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">G</span><span class="err">ₜ</span>
</code></pre></div>
<p>Where:</p>
<ul>
<li><code>Gₜ</code> is the return from time <code>t</code> onward.</li>
<li><code>∇θ log πθ(a|s)</code> is the gradient of the log-policy.</li>
</ul>
<p>Doesn't require value function or model
High variance in gradient estimates</p>
<hr />
<h3 id="2-actor-critic-methods">2. <strong>Actor-Critic Methods</strong><a class="headerlink" href="#2-actor-critic-methods" title="Permanent link">&para;</a></h3>
<p>Hybrid methods where:</p>
<ul>
<li>The <strong>actor</strong> is the policy being optimized.</li>
<li>The <strong>critic</strong> estimates a value function (usually via TD learning) to reduce variance.</li>
</ul>
<p>Examples:</p>
<ul>
<li><strong>A2C</strong> (Advantage Actor-Critic)</li>
<li><strong>PPO</strong> (Proximal Policy Optimization)</li>
<li><strong>DDPG</strong>, <strong>SAC</strong></li>
</ul>
<p>Here, the <strong>critic may use Bellman-style updates</strong>, but the policy update is <strong>still gradient-based</strong>, not DP.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "navigation.tabs", "navigation.top", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../../assets/javascripts/extra.js"></script>
      
    
  </body>
</html>