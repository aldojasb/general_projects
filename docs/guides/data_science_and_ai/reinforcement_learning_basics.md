# ðŸ¤– Reinforcement Learning (RL)

Reinforcement Learning is a branch of Machine Learning where an agent **learns by interacting with an environment** to achieve a goal. Itâ€™s less about feeding the model labeled data (as in supervised learning) and more about **trial, error, and feedback**.



## RL vs Traditional Machine Learning

There are two key differences between **Reinforcement Learning (RL)** and typical **Machine Learning (ML)**:

1. **Agent's Action**
    In RL, the **agentâ€™s actions affect the data it receives next from the environment**. In contrast, ML usually assumes the data is static and independent of it's outputs (predictions).
2. **Reward Signal**
    RL uses a **scalar reward** to guide learning-  what actions lead to good outcomes through trial and error.



## Rewards

A **reward** (denoted as `Râ‚œ`) is a scalar signal received after taking an action. It tells the agent how well itâ€™s doing **at time step t**.

The goal of the agent is to **maximize cumulative reward over time**, not just perform well in the moment.



## The Reward Hypothesis

RL is built on the **Reward Hypothesis**:

> â€œAll goals can be described as the maximization of expected cumulative rewards.â€

Example (Power Plant Control):

- âœ… +1 for producing more power efficiently
- âŒ -10 for exceeding safety thresholds



## Agent's Actions

The agent must choose **a sequence of actions** to maximize **future rewards**.

Caveats:

- Immediate rewards might not be the optimal.
- Sometimes, **sacrificing short-term gain** leads to **better long-term outcomes**.



## History vs. State

One of the most **common sources of confusion** in reinforcement learning is the difference between **history** and **state**.

> It might seem like the *state* should just be â€œeverything that happened beforeâ€ (i.e. the history), but in practice, **state is a concise summary** of that historyâ€”**not the entire thing**.



### What is the History?

The **history** at time step `t` is the full record of everything the agent has experienced up to that point:

```tex
Hâ‚œ = {Oâ‚, Aâ‚, Râ‚, Oâ‚‚, Aâ‚‚, Râ‚‚, ..., Oâ‚œ, Aâ‚œ, Râ‚œ}
```

This contains:

- All past **observations** (`O`)
- All taken **actions** (`A`)
- All received **rewards** (`R`)

Itâ€™s incredibly detailedâ€”but often too bulky or unnecessary for real-time decision-making.



### What is the State?

A **state** `Sâ‚œ` is a **compressed representation** of the history that contains all the information the agent needs **to decide what to do next**.

```tex
Sâ‚œ = f(Hâ‚œ)
```

This function `f` summarizes the useful parts of history and discards the rest.

> âœ… A good state captures **just enough** about the past to make optimal decisionsâ€”nothing more, nothing less.



### Why This Matters

- If you mistakenly treat the **history** as the **state**, your system becomes unnecessarily large and computationally expensive.
- Worse, the agent may get **distracted by irrelevant past details**â€”leading to poor learning performance.

This brings us to the **Markov property**, which tells us how to define a state that is truly useful for decision-making.



## The Markov Property

A state is **Markov** if it captures **everything** needed to determine the next step:

```tex
P[Sâ‚œâ‚Šâ‚ | Sâ‚œ] = P[Sâ‚œâ‚Šâ‚ | Sâ‚, Sâ‚‚, ..., Sâ‚œ]
```

> â€œGiven the present, the future is independent of the past.â€

This means:

- The agent doesnâ€™t need full history.
- A **well-designed state** simplifies learning and planning.



### Example: Rat and Reward Puzzle

![Rat Example](../../images/rat_example.png)

Letâ€™s say a **rat agent** presses levers and hears bells and sees lights before receiving cheese (reward).
Depending on how we define the state:

| **State Design**                    | **What the Agent Sees**                           | **Pros / Cons**                                 |
| ----------------------------------- | ------------------------------------------------- | ----------------------------------------------- |
| Last 3 items in sequence            | "Bell â†’ Lever â†’ Light"                            | Small input, fast decisions, might miss context |
| Count of bells/lights/levers so far | "2 bells, 3 lights, 1 lever"                      | Captures trends, not exact order                |
| Full sequence (entire history)      | "Bell, Bell, Light, Lever, Bell, Lever, Light..." | Accurate but computationally heavy              |

ðŸ“Œ **Insight:** How you define the state **greatly affects** learning efficiency.



## â±ï¸The One-Step Agentâ€“Environment Loop (based on Sutton & Barto)

At the heart of every Reinforcement Learning system lies a **continuous interaction loop** between the **agent** and the **environment**, formalized as a **Markov Decision Process (MDP)**.

### Sequence of Interactions:

At each discrete time step `t`, the following happens:

1. The **agent** observes the current **state** `Sâ‚œ`.
2. Based on `Sâ‚œ`, the agent chooses and executes an **action** `Aâ‚œ`.
3. The **environment** receives the action `Aâ‚œ`, responds with:
   - A **new state** `Sâ‚œâ‚Šâ‚`
   - A **reward** `Râ‚œâ‚Šâ‚`
4. The agent receives `Râ‚œâ‚Šâ‚` and `Sâ‚œâ‚Šâ‚`, then repeats the cycle at `t+1`.

This interaction yields a trajectory like:

```tex
Sâ‚€, Aâ‚€, Râ‚, Sâ‚, Aâ‚, Râ‚‚, Sâ‚‚, Aâ‚‚, ...
```

This diagram, from Sutton & Bartoâ€™s book, formalizes the MDP loop:

- The **agent** produces an action `Aâ‚œ`.
- The **environment** receives `Aâ‚œ`, and in response:
  - Emits the **next state** `Sâ‚œâ‚Šâ‚`
  - Emits a **reward** `Râ‚œâ‚Šâ‚`

![RL diagram](../../images/rl_one_step.png)

> Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed., in progress). http://incompleteideas.net/book/the-book-2nd.html

Each new state influences future actions, forming a feedback loop. Importantly, this formulation obeys the **Markov property**â€”meaning future outcomes depend only on the current state and action, not on the full history.



## ðŸ§© Major Components of a Reinforcement Learning Agent

An RL agent can be thought of as a system that makes decisions by learning from interaction. It may be composed of **three core components**:

------

### 1. **Policy** â€“ â€œWhat should I do?â€

A **policy** defines the agentâ€™s behavior: **how it chooses actions** based on the current state.

- **Deterministic Policy**: always chooses the same action for a given state.

  ```tex
  a = Ï€(s)
  ```

- **Stochastic Policy**: chooses actions according to a probability distribution.

  ```tex
  Ï€(a|s) = P[ Aâ‚œ = a | Sâ‚œ = s ]
  ```

Used when exploration is important or the environment is noisy.

------

### 2. **Value Function** â€“ â€œHow good is this state?â€

A **value function** estimates how much **future reward** the agent can expect from a given state (or state-action pair).

- Helps the agent choose **better states** over time.
- Doesnâ€™t directly pick actions, but it **influences decisions** when paired with a policy.

```tex
V(s) = expected cumulative reward from state s onward
```

------

### 3. **Model** â€“ â€œWhat will happen next?â€

A **model** is the agentâ€™s internal understanding of how the environment behaves.

- It predicts:
  - **Next state**: `Sâ‚œâ‚Šâ‚`
  - **Reward**: `Râ‚œâ‚Šâ‚`
- Allows the agent to **simulate outcomes** before acting.
- Not all RL agents use a model.



## Categorizing RL Agents

Letâ€™s now organize RL agents by how they combine the components above, and clarify **pros, cons, and examples** for each category.

------

### 1. **Value-Based Agents**

- âœ… Uses: **Value Function**
- âŒ Doesnâ€™t explicitly use a **Policy** (itâ€™s implicitâ€”derived from the value function)
- âŒ Doesnâ€™t use a **Model**

Example:

- **Q-Learning** estimates the value of state-action pairs (`Q(s,a)`) and acts greedily.
- **SARSA** (State-Action-Reward-State-Action): Like Q-learning, but updates values using the actual next action taken.

Pros:

- Simple and widely used.
- Good for discrete action spaces.

Cons:

- Struggles with continuous or high-dimensional actions.
- Doesnâ€™t directly represent policy (requires argmax tricks).

------

### 2. **Policy-Based Agents**

- âœ… Uses: **Policy**
- âŒ Doesnâ€™t use a **Value Function**
- âŒ Doesnâ€™t use a **Model**

Example: 

- **REINFORCE**: A basic **policy gradient** method that updates the policy to maximize expected reward.

Pros:

- Naturally handles **continuous action spaces**.
- Can learn **stochastic** or **deterministic** policies.

Cons:

- Higher variance in updates.
- Often less sample-efficient than value-based methods.

------

### 3. **Actor-Critic Agents**

- âœ… Uses: **Policy**
- âœ… Uses: **Value Function**
- âŒ Doesnâ€™t use a **Model**

Example:

- **PPO** â€“ Proximal Policy Optimization: Balances learning progress and stability using a **clipped objective**.

- **A2C** â€“ Advantage Actor-Critic: Computes advantage estimates to reduce variance and improve stability.

- **A3C** â€“ Asynchronous Advantage Actor-Critic: Runs multiple agents in parallel with independent environments.

- **DDPG** â€“ Deep Deterministic Policy Gradient: For **continuous action spaces**. Actor-critic with deterministic policies.

- **SAC** â€“ Soft Actor-Critic: Adds **entropy regularization** to encourage exploration.

Pros:

- Combines **low variance** from value-based with **direct optimization** of policy.
- Very popular for complex environments.

Cons:

- More complex architecture.
- Balancing value and policy updates can be tricky.

------

### 4. **Model-Free Agents**

- âœ… Uses: **Policy and/or Value Function**
- âŒ Doesnâ€™t use a **Model**

Examples:

- Q-learning, PPO, DQN, A3C, REINFORCE

Pros:

- Easier to implement and train.
- No need to learn or assume environment dynamics.

Cons:

- Can be **sample inefficient**â€”needs lots of interactions.
- Less suitable for planning or simulations.

------

### ðŸ§  5. **Model-Based Agents**

- âœ… Uses: **Model**
- âœ… May also use: **Policy and/or Value Function**

Pros:

- **Sample efficient**â€”can plan and simulate.
- Useful when real-world data is costly.

Cons:

- Requires learning or designing an accurate model.
- Model errors can lead to bad decisions (model bias).



## Two Main Approaches to Develop Reinforcement Learning Agents

In reinforcement learning, agents can learn through **two broad approaches**, depending on **how much they know about the environment** at the start:

------

### 1. **Planning (Model-Based Learning)**

> The agent has a model of the environment.

- In this approach, the agent **knows in advance** how the environment works (i.e., it has a **model** that describes the dynamics: what happens when it takes an action).
- Instead of learning through direct interaction, the agent **simulates outcomes internally** using the model.

```tex
State â†’ Model â†’ Simulated Next State & Reward â†’ Policy Update
```

âœ… Great for low-risk, fast iteration
 âŒ Not applicable if the environment is unknown or too complex to model accurately

------

### 2. **Reinforcement Learning (Model-Free Learning)**

> The agent learns everything through direct interaction with the environment.

- The agent **doesn't know the rules** of the world it lives in.
- It must **explore**, **collect experience**, and **learn from trial and error**.
- This is the most common setup in real-world problems where dynamics are unknown or too complex to define upfront.

```tex
State â†’ Real Action â†’ Observation & Reward â†’ Policy Update
```

âœ… More flexible and general
 âŒ Typically requires a lot more data (sample inefficient)

------

**Example 1: Learning to Play Tic-Tac-Toe**

| **Planning Approach**                                        | **Reinforcement Learning Approach**                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The agent is given **all game rules**, and uses **search algorithms** (like Minimax) to simulate future moves and choose the best one. | The agent **plays thousands of games**, learns from wins/losses, and gradually discovers winning strategies via trial and error. |
| Efficient learning with a known model                        | No prior knowledge required                                  |

**Example 2: A Robot Learning to Navigate a Warehouse**

| **Planning Approach**                                        | **Reinforcement Learning Approach**                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| The robot is given a **map of the warehouse**, and simulates paths to find the most efficient one using A* or Dijkstraâ€™s algorithm. | The robot starts without a map, explores randomly, and learns optimal paths based on feedback (e.g., delivery success, collision penalties). |
| Works well in static and known settings                      | Adapts to real-world changes like obstacles or delays        |



## Exploration vs. Exploitation

A **core challenge** in RL is choosing between:

- **Exploitation**: Use what you already know to get high reward now
- **Exploration**: Try new actions to potentially discover better rewards later

### Balance is key

If the agent **only exploits**, it may miss better strategies.
 If it **only explores**, it may never get good at anything.

Real-World Analogies:

| Example           | **Exploitation**                  | **Exploration**                |
| ----------------- | --------------------------------- | ------------------------------ |
| Restaurant Choice | Go to your favorite Italian place | Try a new sushi bar            |
| Online Ads        | Show top-performing ad            | Show a new variation           |
| Oil Drilling      | Drill where oil was found before  | Explore untested land          |
| Game Playing      | Play the best-known chess move    | Try an unconventional strategy |



> These examples show how **short-term gain vs. long-term learning** is a universal tension.

