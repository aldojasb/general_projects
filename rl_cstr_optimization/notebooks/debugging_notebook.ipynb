{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from rl.cstr.optimization.base_agent import compute_gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory_data():\n",
    "    \"\"\"\n",
    "    Fixture providing sample trajectory data for testing compute_gae.\n",
    "    \n",
    "    Creates realistic trajectory data that would be returned by collect_trajectories:\n",
    "    - rewards: Conversion efficiency rewards from CSTR control\n",
    "    - dones: Episode termination flags\n",
    "    - values: Critic's value estimates for each state\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (rewards, dones, values) for GAE computation\n",
    "    \"\"\"\n",
    "    # Sample trajectory data (10 timesteps)\n",
    "    rewards = [15.2, 12.8, 18.1, 14.5, 16.3, 13.7, 17.9, 15.8, 14.2, 16.7]\n",
    "    dones = [False, False, False, False, False, False, False, False, False, True]\n",
    "    values = [15.0, 13.0, 17.5, 14.0, 16.0, 13.5, 17.0, 15.5, 14.0, 16.5]\n",
    "    \n",
    "    return rewards, dones, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given: Sample trajectory data (10 timesteps)\n",
    "rewards, dones, values = sample_trajectory_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.2, 12.8, 18.1, 14.5, 16.3, 13.7, 17.9, 15.8, 14.2, 16.7]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, False, True]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0, 13.0, 17.5, 14.0, 16.0, 13.5, 17.0, 15.5, 14.0, 16.5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When: Computing GAE with default parameters\n",
    "gae_lambda = 0.95\n",
    "gae_advantages_normalized, total_expected_future_rewards, raw_gae_advantages = compute_gae(rewards, dones, values, gae_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3105,  1.1703,  0.8678,  0.6285,  0.2950,  0.0233, -0.4003, -0.8421,\n",
       "        -1.2547, -1.7983])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gae_advantages_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([105.8748,  99.7865,  95.4629,  84.9838,  77.2563,  66.8311,  57.9763,\n",
       "         43.5901,  30.0555,  16.7000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_expected_future_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([90.8748, 86.7865, 77.9629, 70.9838, 61.2563, 53.3311, 40.9763, 28.0901,\n",
       "        16.0555,  0.2000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_gae_advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages_normalized_manual = (raw_gae_advantages - raw_gae_advantages.mean()) / (raw_gae_advantages.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2432,  1.1102,  0.8233,  0.5963,  0.2799,  0.0221, -0.3797, -0.7989,\n",
       "        -1.1903, -1.7060])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_normalized_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3105,  1.1703,  0.8678,  0.6285,  0.2950,  0.0233, -0.4003, -0.8421,\n",
       "        -1.2547, -1.7983])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gae_advantages_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages_mean_1 = gae_advantages_normalized.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9605e-09)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_mean_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages_mean = gae_advantages_normalized.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9604645663569045e-09"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert advantages_mean < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages_std_1 = gae_advantages_normalized.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0541)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_std_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages_std = gae_advantages_normalized.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.054092526435852"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05409252643585205"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_std - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(advantages_std - \u001b[32m1.0\u001b[39m) < \u001b[32m1e-6\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert abs(advantages_std - 1.0) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages_std_2 = advantages_normalized_manual.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_std_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # def test_compute_gae_episode_termination(self, sample_trajectory_data):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae handles episode termination correctly.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Sample trajectory data with episode termination\n",
    "    #     rewards, dones, values = sample_trajectory_data\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle episode termination correctly\n",
    "    #     # Check that the last timestep has a done flag\n",
    "    #     assert dones[-1] == True, \\\n",
    "    #         \"Last timestep should have done=True\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are computed correctly\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "\n",
    "    # def test_compute_gae_different_gamma(self, sample_trajectory_data):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with different gamma values.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Sample trajectory data\n",
    "    #     rewards, dones, values = sample_trajectory_data\n",
    "        \n",
    "    #     # When: Computing GAE with different gamma values\n",
    "    #     gamma_0_9 = 0.9\n",
    "    #     gamma_0_99 = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "        \n",
    "    #     gae_values_0_9, gae_advantages_0_9 = compute_gae(rewards, dones, values, gamma_0_9, gae_lambda)\n",
    "    #     gae_values_0_99, gae_advantages_0_99 = compute_gae(rewards, dones, values, gamma_0_99, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should work with different gamma values\n",
    "    #     # Check that both computations produce valid results\n",
    "    #     assert torch.all(torch.isfinite(gae_values_0_9)), \\\n",
    "    #         \"gae_values with gamma=0.9 should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages_0_9)), \\\n",
    "    #         \"gae_advantages with gamma=0.9 should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_values_0_99)), \\\n",
    "    #         \"gae_values with gamma=0.99 should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages_0_99)), \\\n",
    "    #         \"gae_advantages with gamma=0.99 should be finite\"\n",
    "        \n",
    "    #     # Check that results are different for different gamma values\n",
    "    #     assert not torch.allclose(gae_values_0_9, gae_values_0_99), \\\n",
    "    #         \"Different gamma values should produce different results\"\n",
    "\n",
    "    # def test_compute_gae_constant_rewards(self):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with constant rewards.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Trajectory with constant rewards\n",
    "    #     rewards = [10.0, 10.0, 10.0, 10.0, 10.0]\n",
    "    #     dones = [False, False, False, False, True]\n",
    "    #     values = [10.0, 10.0, 10.0, 10.0, 10.0]\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle constant rewards correctly\n",
    "    #     # Check that gae_values and gae_advantages have correct length\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "\n",
    "    # def test_compute_gae_increasing_rewards(self):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with increasing rewards.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Trajectory with increasing rewards\n",
    "    #     rewards = [5.0, 10.0, 15.0, 20.0, 25.0]\n",
    "    #     dones = [False, False, False, False, True]\n",
    "    #     values = [5.0, 10.0, 15.0, 20.0, 25.0]\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle increasing rewards correctly\n",
    "    #     # Check that gae_values and gae_advantages have correct length\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "\n",
    "    # def test_compute_gae_decreasing_rewards(self):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with decreasing rewards.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Trajectory with decreasing rewards\n",
    "    #     rewards = [25.0, 20.0, 15.0, 10.0, 5.0]\n",
    "    #     dones = [False, False, False, False, True]\n",
    "    #     values = [25.0, 20.0, 15.0, 10.0, 5.0]\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle decreasing rewards correctly\n",
    "    #     # Check that gae_values and gae_advantages have correct length\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "\n",
    "    # def test_compute_gae_negative_rewards(self):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with negative rewards.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Trajectory with negative rewards\n",
    "    #     rewards = [-5.0, -10.0, -15.0, -20.0, -25.0]\n",
    "    #     dones = [False, False, False, False, True]\n",
    "    #     values = [-5.0, -10.0, -15.0, -20.0, -25.0]\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle negative rewards correctly\n",
    "    #     # Check that gae_values and gae_advantages have correct length\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "\n",
    "    # def test_compute_gae_mixed_rewards(self):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with mixed positive and negative rewards.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Trajectory with mixed rewards\n",
    "    #     rewards = [5.0, -10.0, 15.0, -20.0, 25.0]\n",
    "    #     dones = [False, False, False, False, True]\n",
    "    #     values = [5.0, -10.0, 15.0, -20.0, 25.0]\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle mixed rewards correctly\n",
    "    #     # Check that gae_values and gae_advantages have correct length\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "\n",
    "    # def test_compute_gae_zero_rewards(self):\n",
    "    #     \"\"\"\n",
    "    #     Test that compute_gae works with zero rewards.\n",
    "    #     \"\"\"\n",
    "    #     # Given: Trajectory with zero rewards\n",
    "    #     rewards = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    #     dones = [False, False, False, False, True]\n",
    "    #     values = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "    #     # When: Computing GAE with default parameters\n",
    "    #     gamma = 0.99\n",
    "    #     gae_lambda = 0.95\n",
    "    #     gae_values, gae_advantages = compute_gae(rewards, dones, values, gamma, gae_lambda)\n",
    "        \n",
    "    #     # Then: GAE should handle zero rewards correctly\n",
    "    #     # Check that gae_values and gae_advantages have correct length\n",
    "    #     assert len(gae_values) == len(rewards), \\\n",
    "    #         \"gae_values length should match rewards length\"\n",
    "    #     assert len(gae_advantages) == len(rewards), \\\n",
    "    #         \"gae_advantages length should match rewards length\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are finite\n",
    "    #     assert torch.all(torch.isfinite(gae_values)), \\\n",
    "    #         \"gae_values should be finite\"\n",
    "    #     assert torch.all(torch.isfinite(gae_advantages)), \\\n",
    "    #         \"gae_advantages should be finite\"\n",
    "        \n",
    "    #     # Check that gae_values and gae_advantages are all zeros\n",
    "    #     assert torch.all(gae_values == 0), \\\n",
    "    #         \"gae_values should be all zeros for zero rewards\"\n",
    "    #     assert torch.all(gae_advantages == 0), \\\n",
    "    #         \"gae_advantages should be all zeros for zero rewards\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # @pytest.mark.parametrize(\n",
    "    #     \"clip_value,description\",\n",
    "    #     [\n",
    "    #         (0.01, \"very_conservative\"),\n",
    "    #         (0.1, \"conservative\"),\n",
    "    #         (0.2, \"standard\"),\n",
    "    #         (0.3, \"aggressive\"),\n",
    "    #         (0.5, \"very_aggressive\"),\n",
    "    #     ],\n",
    "    #     ids=[\"clip_0.01_very_conservative\", \"clip_0.1_conservative\", \"clip_0.2_standard\", \n",
    "    #          \"clip_0.3_aggressive\", \"clip_0.5_very_aggressive\"]\n",
    "    # )\n",
    "    # def test_ppo_update_different_clip_values(self, mock_actor_critic_model, sample_ppo_data, \n",
    "    #                                         clip_value, description):\n",
    "    #     \"\"\"\n",
    "    #     Test that PPO update works correctly with different clipping values.\n",
    "        \n",
    "    #     This test verifies that:\n",
    "    #     1. Different clip values produce different update behaviors\n",
    "    #     2. Conservative clips (small values) produce smaller parameter changes\n",
    "    #     3. Aggressive clips (large values) allow larger parameter changes\n",
    "    #     4. All clip values maintain training stability\n",
    "        \n",
    "    #     For CSTR context: Tests different levels of policy change conservatism\n",
    "    #     in temperature control strategy updates.\n",
    "    #     \"\"\"\n",
    "    #     # Given: A mock model and sample data\n",
    "    #     model = mock_actor_critic_model\n",
    "    #     states, actions, log_probs_old, returns, advantages = sample_ppo_data\n",
    "        \n",
    "    #     # Create optimizers\n",
    "    #     actor_params = list(model.actor.parameters()) + [model.log_std]\n",
    "    #     actor_optimizer = optim.Adam(actor_params, lr=3e-4)\n",
    "    #     critic_optimizer = optim.Adam(model.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "    #     # Store initial parameters\n",
    "    #     initial_actor_params = {name: param.clone() for name, param in model.actor.named_parameters()}\n",
    "    #     initial_critic_params = {name: param.clone() for name, param in model.critic.named_parameters()}\n",
    "    #     initial_log_std = model.log_std.clone()\n",
    "        \n",
    "    #     # When: Performing PPO update with the specified clip value\n",
    "    #     ppo_update(\n",
    "    #         model=model,\n",
    "    #         states=states,\n",
    "    #         actions=actions,\n",
    "    #         log_probs_old=log_probs_old,\n",
    "    #         returns=returns,\n",
    "    #         advantages=advantages,\n",
    "    #         actor_optimizer=actor_optimizer,\n",
    "    #         critic_optimizer=critic_optimizer,\n",
    "    #         clip=clip_value,\n",
    "    #         epochs=3  # Fewer epochs for faster testing\n",
    "    #     )\n",
    "        \n",
    "    #     # Then: Different clip values should produce different behaviors\n",
    "    #     # Check that parameters changed\n",
    "    #     actor_params_changed = False\n",
    "    #     total_actor_change = 0.0\n",
    "        \n",
    "    #     for name, param in model.actor.named_parameters():\n",
    "    #         initial_param = initial_actor_params[name]\n",
    "    #         if not torch.allclose(param, initial_param):\n",
    "    #             actor_params_changed = True\n",
    "    #             # Calculate total parameter change\n",
    "    #             param_change = torch.sum(torch.abs(param - initial_param)).item()\n",
    "    #             total_actor_change += param_change\n",
    "        \n",
    "    #     assert actor_params_changed, \\\n",
    "    #         f\"Actor parameters should be updated with clip={clip_value} ({description})\"\n",
    "        \n",
    "    #     # Check that critic parameters changed\n",
    "    #     critic_params_changed = False\n",
    "    #     for name, param in model.critic.named_parameters():\n",
    "    #         if not torch.allclose(param, initial_critic_params[name]):\n",
    "    #             critic_params_changed = True\n",
    "    #             break\n",
    "        \n",
    "    #     assert critic_params_changed, \\\n",
    "    #         f\"Critic parameters should be updated with clip={clip_value} ({description})\"\n",
    "        \n",
    "    #     # Check that log_std parameter changed\n",
    "    #     log_std_changed = not torch.allclose(model.log_std, initial_log_std)\n",
    "    #     assert log_std_changed, \\\n",
    "    #         f\"Log std parameter should be updated with clip={clip_value} ({description})\"\n",
    "        \n",
    "    #     # Verify model still works after update\n",
    "    #     test_states = torch.FloatTensor([[0.8, 0.2, 350.0], [0.5, 0.5, 340.0]])\n",
    "    #     mean, std, values = model(test_states)\n",
    "        \n",
    "    #     assert torch.all(torch.isfinite(mean)), \\\n",
    "    #         f\"Actor mean output should be finite with clip={clip_value} ({description})\"\n",
    "    #     assert torch.all(torch.isfinite(std)), \\\n",
    "    #         f\"Actor std output should be finite with clip={clip_value} ({description})\"\n",
    "    #     assert torch.all(torch.isfinite(values)), \\\n",
    "    #         f\"Critic values output should be finite with clip={clip_value} ({description})\"\n",
    "        \n",
    "    #     # Store total change for potential comparison (could be used in future tests)\n",
    "    #     # For now, just verify that changes occurred\n",
    "    #     assert total_actor_change > 0, \\\n",
    "    #         f\"Total actor parameter change should be positive with clip={clip_value} ({description})\"\n",
    "\n",
    "    # def test_ppo_update_extreme_advantages(self, mock_actor_critic_model, extreme_advantages_data):\n",
    "    #     \"\"\"\n",
    "    #     Test that PPO update handles extreme advantage values correctly.\n",
    "        \n",
    "    #     This test verifies that:\n",
    "    #     1. Very large positive advantages don't cause numerical instability\n",
    "    #     2. Very large negative advantages don't cause numerical instability\n",
    "    #     3. Zero advantages are handled correctly\n",
    "    #     4. Mixed extreme advantages don't crash the training\n",
    "    #     5. All computations remain finite and stable\n",
    "        \n",
    "    #     For CSTR context: Tests robustness when temperature adjustments\n",
    "    #     have unexpectedly good or bad outcomes.\n",
    "    #     \"\"\"\n",
    "    #     # Given: A mock model and data with extreme advantages\n",
    "    #     model = mock_actor_critic_model\n",
    "    #     states, actions, log_probs_old, returns, advantages = extreme_advantages_data\n",
    "        \n",
    "    #     # Create optimizers\n",
    "    #     actor_params = list(model.actor.parameters()) + [model.log_std]\n",
    "    #     actor_optimizer = optim.Adam(actor_params, lr=3e-4)\n",
    "    #     critic_optimizer = optim.Adam(model.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "    #     # Store initial parameters\n",
    "    #     initial_actor_params = {name: param.clone() for name, param in model.actor.named_parameters()}\n",
    "    #     initial_critic_params = {name: param.clone() for name, param in model.critic.named_parameters()}\n",
    "    #     initial_log_std = model.log_std.clone()\n",
    "        \n",
    "    #     # When: Performing PPO update with extreme advantages\n",
    "    #     ppo_update(\n",
    "    #         model=model,\n",
    "    #         states=states,\n",
    "    #         actions=actions,\n",
    "    #         log_probs_old=log_probs_old,\n",
    "    #         returns=returns,\n",
    "    #         advantages=advantages,\n",
    "    #         actor_optimizer=actor_optimizer,\n",
    "    #         critic_optimizer=critic_optimizer,\n",
    "    #         clip=0.2,\n",
    "    #         epochs=3  # Fewer epochs for faster testing\n",
    "    #     )\n",
    "        \n",
    "    #     # Then: Extreme advantages should be handled without numerical issues\n",
    "    #     # Check that parameters changed (training occurred)\n",
    "    #     actor_params_changed = False\n",
    "    #     for name, param in model.actor.named_parameters():\n",
    "    #         if not torch.allclose(param, initial_actor_params[name]):\n",
    "    #             actor_params_changed = True\n",
    "    #             break\n",
    "        \n",
    "    #     assert actor_params_changed, \\\n",
    "    #         \"Actor parameters should be updated even with extreme advantages\"\n",
    "        \n",
    "    #     # Check that critic parameters changed\n",
    "    #     critic_params_changed = False\n",
    "    #     for name, param in model.critic.named_parameters():\n",
    "    #         if not torch.allclose(param, initial_critic_params[name]):\n",
    "    #             critic_params_changed = True\n",
    "    #             break\n",
    "        \n",
    "    #     assert critic_params_changed, \\\n",
    "    #         \"Critic parameters should be updated even with extreme advantages\"\n",
    "        \n",
    "    #     # Check that log_std parameter changed\n",
    "    #     log_std_changed = not torch.allclose(model.log_std, initial_log_std)\n",
    "    #     assert log_std_changed, \\\n",
    "    #         \"Log std parameter should be updated even with extreme advantages\"\n",
    "        \n",
    "    #     # Verify that all model parameters are finite\n",
    "    #     for name, param in model.named_parameters():\n",
    "    #         assert torch.all(torch.isfinite(param)), \\\n",
    "    #             f\"Parameter {name} should be finite after extreme advantages\"\n",
    "        \n",
    "    #     # Verify model can still perform forward passes\n",
    "    #     test_states = torch.FloatTensor([[0.8, 0.2, 350.0], [0.5, 0.5, 340.0]])\n",
    "    #     mean, std, values = model(test_states)\n",
    "        \n",
    "    #     # Check that outputs are finite\n",
    "    #     assert torch.all(torch.isfinite(mean)), \\\n",
    "    #         \"Actor mean output should be finite after extreme advantages\"\n",
    "    #     assert torch.all(torch.isfinite(std)), \\\n",
    "    #         \"Actor std output should be finite after extreme advantages\"\n",
    "    #     assert torch.all(torch.isfinite(values)), \\\n",
    "    #         \"Critic values output should be finite after extreme advantages\"\n",
    "        \n",
    "    #     # Check that outputs have correct shapes\n",
    "    #     assert mean.shape == (2, 1), \\\n",
    "    #         f\"Actor output should have shape (2, 1), got {mean.shape}\"\n",
    "    #     assert std.shape == (1,), \\\n",
    "    #         f\"Actor std should have shape (1,), got {std.shape}\"\n",
    "    #     assert values.shape == (2, 1), \\\n",
    "    #         f\"Critic output should have shape (2, 1), got {values.shape}\"\n",
    "        \n",
    "    #     # Verify that std is positive (as expected for standard deviation)\n",
    "    #     assert torch.all(std > 0), \\\n",
    "    #         \"Action std should be positive after extreme advantages\"\n",
    "\n",
    "    # def test_ppo_update_clipping_effectiveness(self, mock_actor_critic_model, clipping_test_data):\n",
    "    #     \"\"\"\n",
    "    #     Test that PPO clipping actually prevents excessive policy changes.\n",
    "        \n",
    "    #     This test verifies that:\n",
    "    #     1. The policy doesn't change too drastically between updates\n",
    "    #     2. Clipping actually constrains the policy updates\n",
    "    #     3. The ratio between old and new policies stays within reasonable bounds\n",
    "    #     4. PPO's conservative update mechanism is working\n",
    "        \n",
    "    #     This is a more rigorous test of PPO's core innovation.\n",
    "    #     \"\"\"\n",
    "    #     # Given: A mock model and data designed to trigger clipping\n",
    "    #     model = mock_actor_critic_model\n",
    "    #     states, actions, log_probs_old, returns, advantages = clipping_test_data\n",
    "        \n",
    "    #     # Create optimizers\n",
    "    #     actor_params = list(model.actor.parameters()) + [model.log_std]\n",
    "    #     actor_optimizer = optim.Adam(actor_params, lr=3e-4)\n",
    "    #     critic_optimizer = optim.Adam(model.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "    #     # Convert data to tensors for analysis\n",
    "    #     states_tensor = torch.FloatTensor(np.array(states))\n",
    "    #     actions_tensor = torch.FloatTensor(np.array(actions))\n",
    "    #     log_probs_old_tensor = torch.FloatTensor(log_probs_old)\n",
    "        \n",
    "    #     # Get initial policy predictions\n",
    "    #     with torch.no_grad():\n",
    "    #         initial_mean, initial_std, _ = model(states_tensor)\n",
    "    #         initial_dist = torch.distributions.Normal(initial_mean, initial_std)\n",
    "    #         initial_log_probs = initial_dist.log_prob(actions_tensor).sum(dim=-1)\n",
    "        \n",
    "    #     # Store initial parameters\n",
    "    #     initial_params = {name: param.clone() for name, param in model.named_parameters()}\n",
    "        \n",
    "    #     # When: Performing PPO update with clipping\n",
    "    #     clip_value = 0.2\n",
    "    #     ppo_update(\n",
    "    #         model=model,\n",
    "    #         states=states,\n",
    "    #         actions=actions,\n",
    "    #         log_probs_old=log_probs_old,\n",
    "    #         returns=returns,\n",
    "    #         advantages=advantages,\n",
    "    #         actor_optimizer=actor_optimizer,\n",
    "    #         critic_optimizer=critic_optimizer,\n",
    "    #         clip=clip_value,\n",
    "    #         epochs=3\n",
    "    #     )\n",
    "        \n",
    "    #     # Then: Check that PPO clipping is actually working\n",
    "    #     # Get new policy predictions\n",
    "    #     with torch.no_grad():\n",
    "    #         new_mean, new_std, _ = model(states_tensor)\n",
    "    #         new_dist = torch.distributions.Normal(new_mean, new_std)\n",
    "    #         new_log_probs = new_dist.log_prob(actions_tensor).sum(dim=-1)\n",
    "        \n",
    "    #     # Calculate actual policy ratios\n",
    "    #     actual_ratios = torch.exp(new_log_probs - log_probs_old_tensor)\n",
    "        \n",
    "    #     # Check that ratios are within reasonable bounds (PPO clipping should help here)\n",
    "    #     # Even with extreme log_probs_old, the actual ratios should be reasonable\n",
    "    #     max_ratio = torch.max(actual_ratios).item()\n",
    "    #     min_ratio = torch.min(actual_ratios).item()\n",
    "        \n",
    "    #     # Print debugging information to understand PPO behavior\n",
    "    #     print(f\"\\nPPO Clipping Debug Info:\")\n",
    "    #     print(f\"  Clip value: {clip_value}\")\n",
    "    #     print(f\"  Max ratio: {max_ratio:.4f}\")\n",
    "    #     print(f\"  Min ratio: {min_ratio:.4f}\")\n",
    "    #     print(f\"  Mean ratio: {torch.mean(actual_ratios).item():.4f}\")\n",
    "    #     print(f\"  Ratio std: {torch.std(actual_ratios).item():.4f}\")\n",
    "    #     print(f\"  Policy divergence: {torch.mean(torch.abs(actual_ratios - 1.0)).item():.4f}\")\n",
    "        \n",
    "    #     # PPO should prevent extremely large ratios\n",
    "    #     assert max_ratio < 10.0, \\\n",
    "    #         f\"PPO clipping should prevent extremely large ratios, got max ratio of {max_ratio}\"\n",
    "        \n",
    "    #     # PPO should prevent extremely small ratios (but allow some small ratios)\n",
    "    #     # The clipping test data has extreme log_probs_old values, so some small ratios are expected\n",
    "    #     assert min_ratio > 0.01, \\\n",
    "    #         f\"PPO clipping should prevent extremely small ratios, got min ratio of {min_ratio}\"\n",
    "        \n",
    "    #     # Check that policy changes are reasonable\n",
    "    #     mean_change = torch.mean(torch.abs(new_mean - initial_mean)).item()\n",
    "    #     std_change = torch.mean(torch.abs(new_std - initial_std)).item()\n",
    "        \n",
    "    #     print(f\"  Mean policy change: {mean_change:.4f}\")\n",
    "    #     print(f\"  Std policy change: {std_change:.4f}\")\n",
    "        \n",
    "    #     # Policy changes should be moderate (not extreme)\n",
    "    #     assert mean_change < 5.0, \\\n",
    "    #         f\"Policy mean changes should be moderate, got {mean_change}\"\n",
    "    #     assert std_change < 2.0, \\\n",
    "    #         f\"Policy std changes should be moderate, got {std_change}\"\n",
    "        \n",
    "    #     # Verify that the model still works correctly\n",
    "    #     test_states = torch.FloatTensor([[0.8, 0.2, 350.0], [0.5, 0.5, 340.0]])\n",
    "    #     mean, std, values = model(test_states)\n",
    "        \n",
    "    #     assert torch.all(torch.isfinite(mean)), \\\n",
    "    #         \"Actor mean output should be finite after PPO update\"\n",
    "    #     assert torch.all(torch.isfinite(std)), \\\n",
    "    #         \"Actor std output should be finite after PPO update\"\n",
    "    #     assert torch.all(torch.isfinite(values)), \\\n",
    "    #         \"Critic values output should be finite after PPO update\"\n",
    "        \n",
    "    #     # Check that std is still positive\n",
    "    #     assert torch.all(std > 0), \\\n",
    "    #         \"Action std should remain positive after PPO update"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
