{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CSTR (Continuously Stirred Tank Reactor) Environment Demo\n",
    "========================================================\n",
    "\n",
    "This demo showcases the CSTR environment from pc-gym, which simulates\n",
    "a continuously stirred tank reactor for chemical process control.\n",
    "\n",
    "The CSTR is a well-established model that's thoroughly tested and stable,\n",
    "making it perfect for learning and experimentation.\n",
    "\n",
    "State Variables (3 total):\n",
    "- Ca: Concentration of reactant A (mol/L)\n",
    "- T: Temperature (K)\n",
    "- Cb: Concentration of reactant B (mol/L)\n",
    "\n",
    "Action Variables (1 total):\n",
    "- Tc: Coolant temperature (K)\n",
    "\n",
    "Observations (3 total):\n",
    "- Ca, T, Cb: Concentration A, Temperature, Concentration B\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from pcgym import make_env\n",
    "from rl.cstr.optimization.visualization import (\n",
    "    plot_state_variables,\n",
    "    plot_control_actions,\n",
    "    plot_reward_evolution)\n",
    "from rl.cstr.optimization.load_config_files import load_and_create_env_params\n",
    "from rl.cstr.optimization.base_state_builder import denormalize_observations\n",
    "from rl.cstr.optimization.base_action_adapter import denormalize_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded: {'N': 30, 'tsim': 26, 'SP': {'Ca': [0.85, 0.85, 0.85, 0.9, 0.9, 0.9, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87, 0.87]}, 'o_space': {'low': array([  0.7, 300. ,   0.8]), 'high': array([  1. , 350. ,   0.9])}, 'a_space': {'low': array([295]), 'high': array([302])}, 'x0': array([  0.8, 330. ,   0.8]), 'r_scale': {'ca': 1000}, 'model': 'cstr', 'normalise_a': True, 'normalise_o': True, 'noise': True, 'integration_method': 'casadi', 'noise_percentage': 0.001}\n",
      "Action space: {'low': array([295]), 'high': array([302])}\n",
      "Observation space: {'low': array([  0.7, 300. ,   0.8]), 'high': array([  1. , 350. ,   0.9])}\n",
      "Number of steps: 30\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_path = \"/workspace/general_projects/rl_cstr_optimization/config/environments/cstr_environment.yaml\"\n",
    "env_params = load_and_create_env_params(config_path)\n",
    "\n",
    "# Extract action space bounds from env_params for use in the notebook\n",
    "a_space = env_params['a_space']\n",
    "o_space = env_params['o_space']\n",
    "nsteps = env_params['N']\n",
    "\n",
    "print(f\"Configuration loaded: {env_params}\")\n",
    "print(f\"Action space: {a_space}\")\n",
    "print(f\"Observation space: {o_space}\")\n",
    "print(f\"Number of steps: {nsteps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CSTR REACTOR DEMO - 30 STEP SIMULATION\n",
      "============================================================\n",
      "Initial observation (normalized): [-0.33333328  0.2        -1.00000024]\n",
      "Initial observation (real values):\n",
      "  Ca: 0.8000000079472857\n",
      "  T:  330.0\n",
      "  Cb: 0.7999999880790669\n",
      "Observation shape: (3,)\n",
      "Action space: Box(-1.0, 1.0, (1,), float32)\n",
      "Observation space: Box(-1.0, 1.0, (3,), float32)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/general_projects/rl_cstr_optimization/.venv/lib/python3.11/site-packages/gymnasium/spaces/box.py:236: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/workspace/general_projects/rl_cstr_optimization/.venv/lib/python3.11/site-packages/gymnasium/spaces/box.py:306: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the CSTR environment with proper parameters\n",
    "# The environment simulates a continuously stirred tank reactor for chemical process control\n",
    "env = make_env(env_params)\n",
    "\n",
    "# Reset the environment to get initial state\n",
    "# This returns the initial observation (concentrations at reactor outlet)\n",
    "initial_observation, initial_info = env.reset()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"CSTR REACTOR DEMO - {nsteps} STEP SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Initial observation (normalized): {initial_observation}\")\n",
    "\n",
    "# Denormalize the initial observation for better understanding\n",
    "initial_real = denormalize_observations(initial_observation, o_space)\n",
    "\n",
    "print(f\"Initial observation (real values):\")\n",
    "print(f\"  Ca: {initial_real[0]}\")\n",
    "print(f\"  T:  {initial_real[1]}\")\n",
    "print(f\"  Cb: {initial_real[2]}\")\n",
    "\n",
    "print(f\"Observation shape: {initial_observation.shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA STORAGE FOR ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Lists to store data for visualization and analysis\n",
    "observations = [initial_observation]  # Store all observations (concentrations at outlet)\n",
    "actions = []       # Store all actions taken\n",
    "rewards = []       # Store all rewards received\n",
    "states = []        # Store full state information if available\n",
    "denorm_actions = [] # Store denormalized actions\n",
    "denorm_observations = [] # Store denormalized observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 30-step simulation...\n",
      "----------------------------------------\n",
      "\n",
      "Step 1/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.14653181 -0.02924253  1.00000048]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8719797722442464\n",
      "    T:  324.2689367481782\n",
      "    Cb: 0.9000000238418665\n",
      "  Reward: -0.0008\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.14653181 -0.02924253  1.00000048]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -0.000808509658379334\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 2/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15940202 0.00413603 1.00000048]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8739103034593626\n",
      "    T:  325.10340065309236\n",
      "    Cb: 0.9000000238418665\n",
      "  Reward: -0.0007\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15940202 0.00413603 1.00000048]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -0.000717992176488978\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 3/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15400611 0.00328124 1.00000048]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8731009159564986\n",
      "    T:  325.0820311078521\n",
      "    Cb: 0.9000000238418665\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15400611 0.00328124 1.00000048]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.2401960970509405e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 4/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15424467 0.00445859 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8731367011405374\n",
      "    T:  325.1114648118965\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15424467 0.00445859 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.2082800711567162e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 5/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.1594348  -0.01697332  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.873915219967419\n",
      "    T:  324.57566694403766\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.1594348  -0.01697332  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.1483188838846034e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 6/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.1569717  -0.00093281  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8735457549060118\n",
      "    T:  324.97667977557404\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.1569717  -0.00093281  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.1107702391228907e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 7/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.14520749 -0.02602626  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8717811229966177\n",
      "    T:  324.34934350383685\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.14520749 -0.02602626  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0930121783503518e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 8/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15277684 0.0171177  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8729165266496577\n",
      "    T:  325.4279426237738\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15277684 0.0171177  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0862816224245263e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 9/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.15760301 -0.01124442  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8736404511863485\n",
      "    T:  324.71888949808556\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.15760301 -0.01124442  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0843455782743266e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 10/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15732599 0.01065714 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8735988986585884\n",
      "    T:  325.2664284567922\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15732599 0.01065714 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0840591250990036e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 11/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 1.53461750e-01 -1.38801040e-04  4.00000262e-01]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8730192625305878\n",
      "    T:  324.99652997399903\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 1.53461750e-01 -1.38801040e-04  4.00000262e-01]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0841424759827043e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 12/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.16633232 0.0128608  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.874949847779998\n",
      "    T:  325.32152012272724\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.16633232 0.0128608  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0842603472084015e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 13/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 1.53027056e-01 -2.39076751e-04  4.00000262e-01]\n",
      "  New observation (real values):\n",
      "    Ca: 0.872954058395097\n",
      "    T:  324.99402308123337\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 1.53027056e-01 -2.39076751e-04  4.00000262e-01]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0843429075393613e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 14/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15679723 0.01516349 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8735195847236354\n",
      "    T:  325.3790872029001\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15679723 0.01516349 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.084393787864182e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 15/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15240638 0.00829898 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8728609571140712\n",
      "    T:  325.20747462277643\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15240638 0.00829898 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.084416663509439e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 16/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15997157 0.0231962  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8739957349930675\n",
      "    T:  325.579904878773\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15997157 0.0231962  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844269804512904e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 17/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15771799 0.00317927 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8736576978045618\n",
      "    T:  325.0794817841379\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15771799 0.00317927 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844311257943446e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 18/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.14987829 0.01486739 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.872481744177797\n",
      "    T:  325.3716848445325\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.14987829 0.01486739 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.084432540218656e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 19/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.16369452 0.01407674 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8745541781527006\n",
      "    T:  325.35191840810677\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.16369452 0.01407674 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844328833268547e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 20/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.15771439 -0.00762514  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8736571578709407\n",
      "    T:  324.8093714545725\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.15771439 -0.00762514  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.08443287531507e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 21/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.14818329 -0.02445019  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8722274938511158\n",
      "    T:  324.388745165533\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.14818329 -0.02445019  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844327915613625e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 22/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15883528 0.00270003 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8738252925907238\n",
      "    T:  325.067500718222\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15883528 0.00270003 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844327182537099e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 23/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.15641453 -0.01745031  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.873462178789022\n",
      "    T:  324.5637421472845\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.15641453 -0.01745031  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.084432671215035e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 24/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [ 0.16137812 -0.02317334  0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8742067186915355\n",
      "    T:  324.4206663811643\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [ 0.16137812 -0.02317334  0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844326453988243e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 25/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.15771572 0.01324904 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.873657358447608\n",
      "    T:  325.3312259445602\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: False\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.15771572 0.01324904 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844326327486897e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: False\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "\n",
      "Step 26/30:\n",
      " Normalized action: [0.5]\n",
      " Denormalized action: 298.5\n",
      "  New observation (normalized): [0.14973843 0.02757285 0.40000026]\n",
      "  New observation (real values):\n",
      "    Ca: 0.8724607643584308\n",
      "    T:  325.6893212342642\n",
      "    Cb: 0.8700000131130265\n",
      "  Reward: -0.0000\n",
      "  Terminated: True\n",
      "  Truncated: False\n",
      "  observation type: <class 'numpy.ndarray'>\n",
      "  observation shape: (3,)\n",
      "  observation values: [0.14973843 0.02757285 0.40000026]\n",
      " reward type: <class 'numpy.float64'>\n",
      " reward shape: ()\n",
      " reward values: -1.0844326271676641e-05\n",
      " terminated type: <class 'bool'>\n",
      " terminated values: True\n",
      " truncated type: <class 'bool'>\n",
      " truncated values: False\n",
      " info type: <class 'dict'>\n",
      " info values: {}\n",
      "  Episode ended at step 26\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN SIMULATION LOOP - nsteps STEPS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nStarting {nsteps}-step simulation...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for step in range(nsteps):\n",
    "    print(f\"\\nStep {step + 1}/{nsteps}:\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # ACTION SELECTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # For this demo, we'll use a simple strategy:\n",
    "    # - Moderate coolant temperature control\n",
    "    # - Stay within the safe operating range\n",
    "    \n",
    "    # Since actions are normalized, we need to provide values between 0 and 1\n",
    "    # These will be automatically scaled to the actual bounds defined in env_params\n",
    "    action = np.array([\n",
    "        0.5,    # Tc: Coolant temperature (normalized) - moderate value\n",
    "    ])\n",
    "    \n",
    "    # Denormalize: actual_value = low + (normalized_value * (high - low))\n",
    "    denorm_action = denormalize_actions(action, a_space)\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # ENVIRONMENT STEP\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Execute the action in the environment\n",
    "    # This advances the simulation by one time step\n",
    "    # Returns: new_observation, reward, terminated, truncated, info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Denormalize observation for better understanding\n",
    "    denorm_observation = denormalize_observations(observation, o_space)\n",
    "\n",
    "    print(f\" Normalized action: {action}\")\n",
    "    print(f\" Denormalized action: {denorm_action}\")\n",
    "    print(f\"  New observation (normalized): {observation}\")\n",
    "    print(f\"  New observation (real values):\")\n",
    "    print(f\"    Ca: {denorm_observation[0]}\")\n",
    "    print(f\"    T:  {denorm_observation[1]}\")\n",
    "    print(f\"    Cb: {denorm_observation[2]}\")\n",
    "    print(f\"  Reward: {reward:.4f}\")\n",
    "    print(f\"  Terminated: {terminated}\")\n",
    "    print(f\"  Truncated: {truncated}\")\n",
    "    print(f\"  observation type: {type(observation)}\")\n",
    "    print(f\"  observation shape: {observation.shape}\")\n",
    "    print(f\"  observation values: {observation}\")\n",
    "    print(f\" reward type: {type(reward)}\")\n",
    "    print(f\" reward shape: {reward.shape}\")\n",
    "    print(f\" reward values: {reward}\")\n",
    "    print(f\" terminated type: {type(terminated)}\")\n",
    "    print(f\" terminated values: {terminated}\")\n",
    "    print(f\" truncated type: {type(truncated)}\")\n",
    "    print(f\" truncated values: {truncated}\")\n",
    "    print(f\" info type: {type(info)}\")\n",
    "    print(f\" info values: {info}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DATA STORAGE\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Store the data for later analysis\n",
    "    observations.append(observation.copy())\n",
    "    actions.append(action.copy())\n",
    "    rewards.append(reward)\n",
    "    denorm_actions.append(denorm_action.copy())\n",
    "    denorm_observations.append(denorm_observation.copy())\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TERMINATION CHECK\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Check if the episode has ended\n",
    "    if terminated or truncated:\n",
    "        print(f\"  Episode ended at step {step + 1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SIMULATION COMPLETE - ANALYSIS\n",
      "============================================================\n",
      "Total steps completed: 30\n",
      "Average reward: -0.0001\n",
      "Total reward: -0.0034\n",
      "Denormalized actions: [298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5\n",
      " 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5 298.5\n",
      " 298.5 298.5 298.5 298.5 298.5]\n",
      "Denormalized observations: [[  0.82933053 326.2351389    0.85000001]\n",
      " [  0.85320202 325.72286714   0.85000001]\n",
      " [  0.86592612 325.32572191   0.85000001]\n",
      " [  0.87185123 324.85448049   0.90000002]\n",
      " [  0.87309998 325.18705228   0.90000002]\n",
      " [  0.87386542 324.91336884   0.90000002]\n",
      " [  0.87358614 325.23810699   0.87000001]\n",
      " [  0.87316031 324.80039452   0.87000001]\n",
      " [  0.87297796 324.62494774   0.87000001]\n",
      " [  0.87523312 324.88222768   0.87000001]\n",
      " [  0.87235698 324.78689364   0.87000001]\n",
      " [  0.87455333 324.56197494   0.87000001]\n",
      " [  0.87373117 325.03661682   0.87000001]\n",
      " [  0.87296204 324.88307285   0.87000001]\n",
      " [  0.87292444 324.7265825    0.87000001]\n",
      " [  0.87290013 325.05845825   0.87000001]\n",
      " [  0.87196825 324.82552552   0.87000001]\n",
      " [  0.87352289 324.8100827    0.87000001]\n",
      " [  0.87250822 324.98367991   0.87000001]\n",
      " [  0.87284456 324.92908418   0.87000001]\n",
      " [  0.87236077 324.34465704   0.87000001]\n",
      " [  0.87419206 325.10797904   0.87000001]\n",
      " [  0.87257192 324.40343854   0.87000001]\n",
      " [  0.87294387 325.15265117   0.87000001]\n",
      " [  0.87445345 325.34752685   0.87000001]\n",
      " [  0.87345255 325.80847343   0.87000001]\n",
      " [  0.87480388 324.7902709    0.87000001]\n",
      " [  0.8743065  325.27037422   0.87000001]\n",
      " [  0.87280251 325.11420609   0.87000001]]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SIMULATION COMPLETE - ANALYSIS AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SIMULATION COMPLETE - ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert lists to numpy arrays for easier analysis\n",
    "observations = np.array(observations)\n",
    "actions = np.array(actions)\n",
    "rewards = np.array(rewards)\n",
    "denorm_actions = np.array(denorm_actions)\n",
    "denorm_observations = np.array(denorm_observations)\n",
    "\n",
    "print(f\"Total steps completed: {len(observations)}\")\n",
    "print(f\"Average reward: {np.mean(rewards):.4f}\")\n",
    "print(f\"Total reward: {np.sum(rewards):.4f}\")\n",
    "print(f\"Denormalized actions: {denorm_actions}\")\n",
    "print(f\"Denormalized observations: {denorm_observations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all four plots\n",
    "variable_definitions = [\n",
    "    {\n",
    "        'name': 'Ca (Concentration A)',\n",
    "        'index': 0,\n",
    "        'color': 'blue',\n",
    "        'symbol': 'circle',\n",
    "        'yaxis_title': 'Concentration (mol/L)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'T (Temperature)',\n",
    "        'index': 1,\n",
    "        'color': 'red',\n",
    "        'symbol': 'square',\n",
    "        'yaxis_title': 'Temperature (K)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Cb (Concentration B)',\n",
    "        'index': 2,\n",
    "        'color': 'green',\n",
    "        'symbol': 'triangle-up',\n",
    "        'yaxis_title': 'Concentration (mol/L)'\n",
    "    }\n",
    "]\n",
    "\n",
    "plot_state_variables(denorm_observations, variable_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all four plots\n",
    "plot_control_actions(denorm_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_evolution(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/general_projects/rl_cstr_optimization/.venv/lib/python3.11/site-packages/gymnasium/spaces/box.py:236: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "\n",
      "/workspace/general_projects/rl_cstr_optimization/.venv/lib/python3.11/site-packages/gymnasium/spaces/box.py:306: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33333328,  0.2       , -1.00000024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r_init': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment to free resources\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
