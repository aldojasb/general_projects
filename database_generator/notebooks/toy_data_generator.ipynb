{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional, Literal, NewType\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from database_generator.logging_configuration import setup_logging_for_this_script\n",
    "setup_logging_for_this_script()\n",
    "# Get the logger for this module\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from database_generator.helpers import (\n",
    "    get_config_path,\n",
    "    load_and_process_params,\n",
    "    append_and_concatenate_dataframes\n",
    ")\n",
    "\n",
    "from database_generator.get_data import (\n",
    "    generate_stable_toy_data,\n",
    "    introduce_exponential_anomalies,\n",
    "    simulate_broken_sensor,\n",
    ")\n",
    "\n",
    "from database_generator.evaluate import (\n",
    "    overlaid_plots_with_plotly,\n",
    ")\n",
    "\n",
    "from database_generator.evaluate import (\n",
    "    overlaid_plots_with_plotly,\n",
    ")\n",
    "\n",
    "from database_generator.db_operations import(\n",
    "    create_sql_alchemy_engine,\n",
    "    get_last_timestamp,\n",
    "    query_data_by_datetime,\n",
    "    store_pandas_dataframe_into_postegre,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing and reading the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aldo/Repositories/general_projects/database_generator/config_file.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the path to the .json file from the environment\n",
    "\n",
    "path_for_the_json_file = get_config_path()\n",
    "path_for_the_json_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[database_generator.helpers] 2024-10-05 16:01:44 - INFO: number_of_rows_for_stable_toy_data = 10000\n",
      "[database_generator.helpers] 2024-10-05 16:01:44 - INFO: seed_for_the_stable_dataset = 300\n",
      "[database_generator.helpers] 2024-10-05 16:01:44 - INFO: start_date_for_the_toy_dataset = 2024-09-08 10:00:00+00:00\n",
      "[database_generator.helpers] 2024-10-05 16:01:44 - INFO: variable_of_interest = pH\n",
      "[database_generator.helpers] 2024-10-05 16:01:44 - INFO: path_to_save_the_outcomes = /home/aldo/Repositories/general_projects/database_generator/experiments\n",
      "[database_generator.helpers] 2024-10-05 16:01:44 - INFO: table_name_to_be_created_on_postgresql = raw_data_version_1\n"
     ]
    }
   ],
   "source": [
    "config_dict = load_and_process_params(path_for_the_json_file)\n",
    "\n",
    "seed_for_the_stable_dataset = config_dict['seed_for_the_stable_dataset']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the stable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "main_datetime_in_utc = pd.Timestamp.now(tz='UTC')\n",
    "start_datetime_in_utc = main_datetime_in_utc - timedelta(hours=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dataframes\n",
    "dfs_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[database_generator.helpers] 2024-10-05 16:02:01 - INFO: Datetime '2024-10-04 23:01:52.016261+00:00' is already in the correct format.\n",
      "[database_generator.helpers] 2024-10-05 16:02:01 - INFO: Datetime '2024-10-05 23:01:52.016261+00:00' is already in the correct format.\n"
     ]
    }
   ],
   "source": [
    "df_stable = generate_stable_toy_data(start_datetime=start_datetime_in_utc,\n",
    "                                     end_datetime=main_datetime_in_utc,\n",
    "                                     seed_for_random=seed_for_the_stable_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_stable_toy_data() missing 1 required positional argument: 'end_datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# When\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_stable_toy_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_datetime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# end_datetime=end_time,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed_for_random\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_stable_toy_data() missing 1 required positional argument: 'end_datetime'"
     ]
    }
   ],
   "source": [
    "start_time = pd.Timestamp(year=2024,\n",
    "                          month=10,\n",
    "                          day=5,\n",
    "                          hour=16,\n",
    "                          minute=0,\n",
    "                          second=0,\n",
    "                          microsecond=0,\n",
    "                          tz='utc')\n",
    "end_time = start_time + timedelta(minutes=1)\n",
    "frequency = '120s'\n",
    "random_state = 42\n",
    "# When\n",
    "df = generate_stable_toy_data(\n",
    "    start_datetime=start_time,\n",
    "    end_datetime=end_time,\n",
    "    frequency=frequency,\n",
    "    seed_for_random=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature_C</th>\n",
       "      <th>Pressure_MPa</th>\n",
       "      <th>Vibration_mm_s</th>\n",
       "      <th>Flow_Rate_l_min</th>\n",
       "      <th>Humidity_%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-05 16:00:00+00:00</th>\n",
       "      <td>75.496714</td>\n",
       "      <td>2.98812</td>\n",
       "      <td>3.086912</td>\n",
       "      <td>303.357246</td>\n",
       "      <td>38.829233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Temperature_C  Pressure_MPa  Vibration_mm_s  \\\n",
       "Timestamp                                                                \n",
       "2024-10-05 16:00:00+00:00      75.496714       2.98812        3.086912   \n",
       "\n",
       "                           Flow_Rate_l_min  Humidity_%  \n",
       "Timestamp                                               \n",
       "2024-10-05 16:00:00+00:00       303.357246   38.829233  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14405"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stable.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stable.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding df:\n",
    "dfs_list.append(df_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stable = overlaid_plots_with_plotly(df=df_stable,\n",
    "                           # scatter_variables=['Vibration_mm_s', 'Flow_Rate_l_min'],\n",
    "                           # variable_of_interest='Temperature_C',\n",
    "                           save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the two types of anomaly to evalaute it\n",
    "\n",
    "### Problem 1: Bearing Wear\n",
    "Description: Over time, the bearings in the pump might wear out, causing an increase in vibration levels.\n",
    "\n",
    "\n",
    "### Problem 5: Broken Temperature Sensor\n",
    "Description: The temperature sensor might malfunction or break, leading to inaccurate or stuck readings.\n",
    "\n",
    "- Stuck Readings: The sensor gets \"stuck\" at a constant value, providing the same reading for a period of time.\n",
    "\n",
    "- Sudden Jumps: The sensor might suddenly jump to an unusually high or low value, remaining there for some time.\n",
    "\n",
    "- Intermittent Spikes: The sensor occasionally produces spikes of incorrect readings, either very high or very low.\n",
    "\n",
    "- Dropouts: The sensor might stop reporting data altogether, which could be simulated as missing values (NaN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearing Wear Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining datetime\n",
    "start_time_anomaly_exponential = main_datetime_in_utc\n",
    "end_time_anomaly_exponential = main_datetime_in_utc + timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce bearing wear\n",
    "\n",
    "df_with_anomaly_exponential = introduce_exponential_anomalies(variable='Vibration_mm_s',\n",
    "                                                  start_datetime=start_time_anomaly_exponential,\n",
    "                                                  end_datetime=end_time_anomaly_exponential,\n",
    "                                                  increase_rate=0.01\n",
    "                                                  )\n",
    "\n",
    "dfs_list.append(df_with_anomaly_exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see the effect of the anomaly\n",
    "fig_anomaly_exponential = overlaid_plots_with_plotly(df_with_anomaly_exponential,\n",
    "                                                     # scatter_variables=['Vibration_mm_s'],\n",
    "                                                     variable_of_interest='Vibration_mm_s',\n",
    "                                                     save_plot=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_anomaly_exponential.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime_stuck_sensor = end_time_anomaly_exponential\n",
    "end_datetime_stuck_sensor = start_datetime_stuck_sensor + timedelta(hours=3)\n",
    "\n",
    "# Simulate a sensor stuck at a constant value\n",
    "df_with_sensor_issue_stuck = simulate_broken_sensor(variable='Temperature_C',\n",
    "                                              start_datetime=start_datetime_stuck_sensor,\n",
    "                                              end_datetime=end_datetime_stuck_sensor,\n",
    "                                              mode='stuck'\n",
    "                                              )\n",
    "\n",
    "dfs_list.append(df_with_sensor_issue_stuck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see the effect of the anomaly\n",
    "fig_anomaly_stuck = overlaid_plots_with_plotly(df_with_sensor_issue_stuck,\n",
    "                                               variable_of_interest='Temperature_C',\n",
    "                                               save_plot=False)\n",
    "fig_anomaly_stuck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sudden Jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime_jump_sensor = end_datetime_stuck_sensor\n",
    "end_datetime_jump_sensor = start_datetime_jump_sensor + timedelta(hours=3)\n",
    "\n",
    "# Simulate a sensor stuck at a constant value\n",
    "df_with_sensor_issue_jump = simulate_broken_sensor(variable='Temperature_C',\n",
    "                                              start_datetime=start_datetime_jump_sensor,\n",
    "                                              end_datetime=end_datetime_jump_sensor,\n",
    "                                              mode='jump'\n",
    "                                              )\n",
    "\n",
    "dfs_list.append(df_with_sensor_issue_jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see the effect of the anomaly\n",
    "fig_anomaly_stuck = overlaid_plots_with_plotly(df_with_sensor_issue_jump,\n",
    "                                               variable_of_interest='Temperature_C',\n",
    "                                               save_plot=False)\n",
    "fig_anomaly_stuck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermittent Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime_spike_sensor = end_datetime_jump_sensor\n",
    "end_datetime_spike_sensor = start_datetime_spike_sensor + timedelta(hours=3)\n",
    "\n",
    "# Simulate a sensor stuck at a constant value\n",
    "df_with_sensor_issue_spike = simulate_broken_sensor(variable='Temperature_C',\n",
    "                                              start_datetime=start_datetime_spike_sensor,\n",
    "                                              end_datetime=end_datetime_spike_sensor,\n",
    "                                              mode='spike'\n",
    "                                              )\n",
    "\n",
    "dfs_list.append(df_with_sensor_issue_spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see the effect of the anomaly\n",
    "fig_anomaly_stuck = overlaid_plots_with_plotly(df_with_sensor_issue_spike,\n",
    "                                               variable_of_interest='Temperature_C',\n",
    "                                               save_plot=False)\n",
    "fig_anomaly_stuck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime_dropout_sensor = end_datetime_spike_sensor\n",
    "end_datetime_dropout_sensor = start_datetime_dropout_sensor + timedelta(hours=3)\n",
    "\n",
    "# Simulate a sensor stuck at a constant value\n",
    "df_with_sensor_issue_dropout = simulate_broken_sensor(variable='Temperature_C',\n",
    "                                              start_datetime=start_datetime_dropout_sensor,\n",
    "                                              end_datetime=end_datetime_dropout_sensor,\n",
    "                                              mode='dropout'\n",
    "                                              )\n",
    "\n",
    "dfs_list.append(df_with_sensor_issue_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see the effect of the anomaly\n",
    "fig_anomaly_stuck = overlaid_plots_with_plotly(df_with_sensor_issue_dropout,\n",
    "                                               variable_of_interest='Temperature_C',\n",
    "                                               save_plot=False)\n",
    "fig_anomaly_stuck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames and handle duplicates by taking the mean of values for duplicate indices\n",
    "combined_df = append_and_concatenate_dataframes(dfs_list, method='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store pandas df into postgre\n",
    "store_pandas_dataframe_into_postegre(df=combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Architecture Overview\n",
    "\n",
    "\n",
    "## A. Components\n",
    "\n",
    "- Data Simulation Module\n",
    "Purpose: Generate synthetic datasets that represent normal operational conditions.\n",
    "Functions:\n",
    "generate_stable_toy_data: Generates the baseline dataset.\n",
    "introduce_exponential_anomalies: Simulates anomalies like bearing wear.\n",
    "simulate_broken_sensor: Introduces faults such as sensor failures.\n",
    "\n",
    "- Data Ingestion Service\n",
    "Purpose: Writes the generated/simulated data into a PostgreSQL database.\n",
    "Components:\n",
    "A script or service (e.g., using Python and psycopg2 or SQLAlchemy) to connect to the PostgreSQL database and insert the generated data.\n",
    "\n",
    "- PostgreSQL Database\n",
    "Purpose: Stores the simulated data. This data can be queried by other services/modules for anomaly detection and analysis.\n",
    "Schema Design:\n",
    "Table Structure: Design tables to hold time-series data with columns like timestamp, vibration_level, pressure, temperature, anomaly_flag, etc.\n",
    "Indexing: Ensure the timestamp field is indexed for efficient querying.\n",
    "\n",
    "- Anomaly Detection Module (External Service)\n",
    "Purpose: Consumes data from the PostgreSQL database, applies anomaly detection algorithms, and flags potential issues.\n",
    "Data Flow: Queries the database periodically or in real-time and applies models like autoencoders, statistical methods, or machine learning algorithms.\n",
    "\n",
    "- Monitoring and Logging\n",
    "Purpose: Monitors the service performance, logs errors, and ensures data integrity.\n",
    "Components: Tools like Prometheus for monitoring and Grafana for visualization. Logs can be stored locally or in a logging service.\n",
    "\n",
    "## B. Workflow\n",
    "Data Generation and Simulation:\n",
    "\n",
    "The service periodically or on-demand runs the generate_stable_toy_data function to create a stable dataset.\n",
    "Anomalies are introduced using introduce_exponential_anomalies and simulate_broken_sensor functions.\n",
    "Data Ingestion:\n",
    "\n",
    "The simulated data is sent to the Data Ingestion Service, which connects to the PostgreSQL database and inserts the data into the appropriate tables.\n",
    "Database Storage:\n",
    "\n",
    "The PostgreSQL database stores the time-series data along with any anomaly flags or metadata that might be useful for downstream analysis.\n",
    "Anomaly Detection:\n",
    "\n",
    "The Anomaly Detection Module queries the database, retrieves the data, and applies algorithms to detect anomalies. Detected anomalies are flagged and stored back in the database or sent to an alerting system.\n",
    "Monitoring:\n",
    "\n",
    "The entire process is monitored for performance and reliability. Logs are reviewed to ensure data integrity, and alerts are triggered for any unexpected behavior.\n",
    "\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    " - A. Setting Up the PostgreSQL Database\n",
    "Create the Database:\n",
    "Install PostgreSQL and create a new database for the service.\n",
    "Design the Schema:\n",
    "Define tables for storing the time-series data, ensuring that they are normalized and indexed appropriately.\n",
    "\n",
    "- B. Implement the Data Simulation Module\n",
    "Refactor Existing Functions:\n",
    "\n",
    "Refactor generate_stable_toy_data, introduce_exponential_anomalies, and simulate_broken_sensor to be callable by the service.\n",
    "Integrate with the Data Ingestion Service:\n",
    "\n",
    "Implement a Python script or service that runs these functions and writes the results to the PostgreSQL database.\n",
    "\n",
    "- C. Implement the Data Ingestion Service\n",
    "Database Connection:\n",
    "Use libraries like psycopg2 or SQLAlchemy to connect to the PostgreSQL database.\n",
    "Data Insertion Logic:\n",
    "Implement the logic to insert the generated data into the database, ensuring proper handling of timestamps and other relevant metadata.\n",
    "\n",
    "- D. Anomaly Detection Integration\n",
    "Develop or Integrate Anomaly Detection Algorithms:\n",
    "\n",
    "Implement or integrate existing anomaly detection algorithms that will consume the data from the PostgreSQL database.\n",
    "Store Results:\n",
    "\n",
    "Store the results of the anomaly detection in the same database or send them to a monitoring/alerting system.\n",
    "\n",
    "- E. Monitoring and Logging\n",
    "Set Up Monitoring Tools:\n",
    "\n",
    "Use Prometheus to monitor service metrics and Grafana to visualize them.\n",
    "Implement Logging:\n",
    "\n",
    "Ensure that all critical operations are logged, and set up error-handling mechanisms.\n",
    "\n",
    "## Tools and Technologies\n",
    "Python: Core language for scripting, data simulation, and ingestion.\n",
    "PostgreSQL: Database for storing and querying time-series data.\n",
    "SQLAlchemy/psycopg2: Libraries for database interaction.\n",
    "Prometheus/Grafana: Monitoring and visualization.\n",
    "Docker (Optional): For containerizing the service to ensure consistency across environments.\n",
    "\n",
    "## Future Considerations\n",
    "Scalability: Ensure the system can handle increasing volumes of data as the service expands.\n",
    "Real-Time Processing: Consider integrating real-time data processing pipelines if needed.\n",
    "Security: Implement proper security measures for database access and data handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the Services to Containerize\n",
    "Based on this architecture, the following components can be containerized:\n",
    "\n",
    "- Data Simulation and Ingestion Service\n",
    "- PostgreSQL Database\n",
    "- Anomaly Detection Service\n",
    "- Monitoring and Logging Tools (Prometheus and Grafana)\n",
    "\n",
    "\n",
    "### Putting It All Together with Docker Compose\n",
    "Use Docker Compose to orchestrate all the services:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested SQLAlchemy Methods to Build:\n",
    "\n",
    "- Setup: Connecting to the Database\n",
    "- Create a Table for Pump Data\n",
    "- Insert Data into the Table\n",
    "- Query Data from the Table\n",
    "- Update Data in the Table\n",
    "- Delete Data from the Table\n",
    "- Use SQLAlchemy ORM to Define Models and Perform CRUD Operations"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "database_generator",
   "language": "python",
   "name": "database_generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
