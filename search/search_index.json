{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my Data Science &amp; Software Engineering Portfolio","text":""},{"location":"#a-quick-background-about-me","title":"A Quick Background About Me","text":"<p>Hi, I\u2019m Aldo! </p> <p>I hold a Master\u2019s in Data Science from the University of British Columbia (UBC) and an undergraduate degree in Industrial/Production Engineering </p> <p>I started to work when I was 15. I opened my own business: a small locksmith shop in my hometown. I spent my days making key copies and helping people who locked themselves out of their homes or cars. That experience taught me the fondations of  customer service, business strategy, and how to adapt quickly to new technologies. Over time, I figured out a passion for math, computer science, and physics and I ended up taking a degree in production engineering. Since then, I\u2019ve built a career delivering data science solutions, managing projects and teams, and working across both large corporations (like Procter &amp; Gamble) and the fast-paced startups, where I currently work as a Data Scientist &amp; Manager.</p> <p>Outside of work, you\u2019ll find me skiing down the slopes,  reading in a cozy coffee shop, or riding motorcycles - always looking for the next adventure!</p>"},{"location":"#what-you-will-find-here","title":"What You Will Find Here","text":"<p>This portfolio brings together two key parts of my work:</p> <ul> <li>Educational content: I mentor aspiring data scientists, engineers, and product teams, covering topics like software architecture, MLOps, and real-world AI systems.</li> <li>Professional services: I offer as a Software &amp; AI Consultant, helping companies turn business problems into scalable AI solutions.</li> </ul> <p>Whether you\u2019re here to learn, collaborate, or explore how I can support your company, you\u2019ll find practical knowledge to building AI that works.</p>"},{"location":"#how-i-can-help-your-business","title":"How I Can Help Your Business","text":"<p>I build AI products that start with real business problems and end with a software that your clients will actually use. My services are divided into two complementary areas:</p>"},{"location":"#ai-systems-that-work","title":"AI Systems That Work","text":"<p>Build solutions that integrate, scale, and deliver real-world impact.</p> <ul> <li>Intuition to Prototype: Explore bold ML ideas and transform unstructured curiosity into structured insight</li> <li>Prototype to Production: Transform research code into scalable, deployable AI products</li> <li>Custom AI Integration: Build AI modules that plug into your existing architecture</li> <li>Digital Twins &amp; Simulation: Create safe environments for testing AI systems and business decisions</li> <li>Data Visualization: Design dashboards that unlock insights from your existing data</li> </ul>"},{"location":"#ai-foundations","title":"AI Foundations","text":"<p>Lay the groundwork to scale AI confidently and sustainably.</p> <ul> <li>AI Readiness Assessment: Evaluate your infrastructure, data, and team readiness before investing</li> <li>Innovation Lab Design: Create internal R&amp;D processes for systematic AI experimentation</li> <li>Technical Team Coaching: Mentor data scientists to build maintainable AI systems</li> <li>AI Culture Training: Help non-technical teams understand and collaborate on AI initiatives</li> </ul> <p>For more detail on each service, visit the Work With Me page.</p>"},{"location":"#lessons-i-share-with-mentees-and-teams","title":"Lessons I Share with Mentees and Teams","text":"<p>This section showcases my expertise in software architecture, data science, MLOps, and applied AI workflows. Whether you\u2019re diving into design patterns, refining your testing strategies, or exploring machine learning concepts, you\u2019ll find content to accelerate learning and apply best practices in real-world projects.</p> <ul> <li>Software Architecture: The (invisible) force driving business success - Principles and patterns for designing flexible, scalable, and maintainable systems (e.g. SOLID principles). </li> <li>AI Product Management: Practical ideas for managing uncertainty, making smart bets, and avoiding common pitfalls in real-world AI/ML product development. </li> <li>Data Science &amp; AI: Key concepts and learning projects in Artificial Intelligence and Machine Learning, covering topics like autoencoders, reinforcement learning, and more. </li> <li>Development &amp; MLOps: Best practices in pytest for testing, Dev Containers for stable environments, CI/CD integration, and more. </li> <li>Demos: A showcase of real-world applications that reinforce these concepts through hands-on implementation.</li> </ul>"},{"location":"#connect-with-me","title":"Connect With Me","text":"<p>Feel free to reach out for discussions, collaborations, or networking. </p> <ul> <li>LinkedIn: Aldo Saltao </li> <li>GitHub: aldojasb - General Projects Repository</li> </ul>"},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/","title":"** Framework That Embraces Uncertainty**","text":"<p>One of the biggest mistakes in AI adoption is treating uncertainty as something to be hidden or ignored. In traditional engineering projects, uncertainty is minimized through decades of standards, physical modeling, and deterministic planning. But in AI/ML, uncertainty is not only expected -- it's an essential part of how the technology learns and adapts.</p> <p>Instead of resisting that uncertainty, we can design around it.</p> <p>That's where this framework comes in. It doesn't promise perfection -- it's not a rigid waterfall plan. Instead, it provides scaffolding for responsible exploration: - Decision gates to evaluate progress - Simulation phases to test ideas safely - Operator involvement to ensure trust and adoption</p> <p>If we build the right foundation, we don't just make AI safer. We make it more enjoyable to work with, more trusted by stakeholders, and more likely to create lasting value.</p> <p>To make this concrete, let's walk through a realistic (but fictional) case study:</p> <p>Client: TerraMiner Corp </p> <p>Industry: Mining &amp; Metal Processing Goal: Reduce energy usage and increase throughput via AI -- including Reinforcement Learning for control optimization Context: An engineering-driven organization with no prior ML adoption. Strong hesitation around deploying \"black-box\" AI models into live operations due to concerns about safety, downtime, and regulatory impact.</p> <p>In the following sections, we'll use TerraMiner's journey to illustrate how a phased AI framework -- combined with simulation, human-in-the-loop validation, and deliberate decision points -- can turn uncertainty into a strength instead of a liability.</p>"},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#staged-mlrl-project","title":"Staged ML/RL Project","text":"<p>Set Milestones Around Usefulness, Not Perfection.</p> <p>To ensure the project remains valuable even in early stages, we break the ML initiative into staged, learning-focused milestones. Each phase produces concrete insights, regardless of whether the final automation is implemented.</p> <p>Note: Each phase is valuable even if the next phase never happens. That\u2019s the core principle.</p> <p></p> Phase Objective Deliverable \"Success\" Looks Like Decision (Go/No-Go Criteria &amp; Options) Phase 1 - Feasibility &amp; Data Audit Evaluate whether existing data is suitable for ML Data map, gaps, instrumentation recommendations TerraMiner understands what their data can/can't support Decision: Is data sufficient to proceed with modeling?  Go: Data quality meets minimum requirements for pilot.    No-Go: Recommend instrumentation upgrades before ML.    Adjust: Proceed with a simplified use case or synthetic data. Phase 2 - ML Hypothesis Exploration Train initial models on historical data Offline model results vs KPIs ML shows ability to predict or recommend better than baseline Decision: Does ML show promise over heuristics?    Go: Model outperforms baseline or provides valuable signal.    No-Go: Stop or reformulate use case.    Adjust: Explore alternative targets or hybrid models. Phase 3 - Simulation Create or adapt a process simulator to test ML/RL logic in a virtual environment Simulator linked to agent/environment logic, test results RL/ML system shows behavior in simulated environment without risk Decision: Is simulator behavior realistic and useful for policy evaluation?    Go: Simulator behaves as expected, agent shows safe behavior.    No-Go: Rebuild or postpone RL use case.    Adjust: Use simplified models or partial subsystems. Phase 4 - Operator Decision Support Use ML/RL in recommendation mode only (no automation), test against real ops Side-by-side recommendations dashboard Operators engage with system, trust begins to build Decision: Are operators engaging with and validating ML recommendations?   Go: Operators find suggestions useful and non-disruptive.    No-Go: Stop deployment and focus on explainability or training.    Adjust: Run additional simulations or redesign dashboard. Phase 5 - Limited Production Integration Deploy ML control to one subsystem or shift, with rollback capability Live metrics from test system, alerts, rollback logs Controlled success in production-like environment Decision: Does the model perform acceptably in live settings?    Go: Model improves key metrics with stability and safety.    No-Go: Revert to manual mode and review root causes.    Adjust: Tune model or expand simulation before retry. Phase 6 - Full Rollout Readiness Hand off tools, training, and process ownership to client team Documentation, automated pipelines, team onboarding Client can operate and improve ML/RL system independently Decision: Is the client ready to own and scale the solution?    Go: Team has been trained and infrastructure supports rollout.    No-Go: Delay scale-up, build internal capacity.    Adjust: Launch partial rollout or provide extended support."},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#phase-3-simulation-deep-dive","title":"** Phase 3: Simulation \u2013 Deep Dive**","text":""},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#why-its-critical","title":"Why It\u2019s Critical","text":"<ul> <li> <p>Provides a safe environment to test agent behavior.</p> </li> <li> <p>Allows clients to see consequences of AI decisions before going live.</p> </li> <li> <p>Builds confidence in \u201cwhat-if\u201d testing (e.g. \"What happens if we increase crusher RPM by 20%?\").</p> </li> </ul>"},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#types-of-simulators","title":"Types of Simulators","text":"<ul> <li> <p>Process Simulators (e.g. metallurgical flow, energy balances, hydraulics)</p> </li> <li> <p>Digital Twins (if available)</p> </li> <li> <p>Custom RL-compatible Environments (e.g., OpenAI Gym-style wrappers over legacy simulators)</p> </li> </ul>"},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#learning-outcomes-from-simulation","title":"Learning Outcomes from Simulation","text":"Test Learning ML/RL performance vs rule-based logic Quantify possible gains before risking live ops Sensitivity tests Understand how volatile the system is to small parameter changes Failure mode tests Identify risky agent behaviors before they happen in production Simulation-to-reality gap Document what\u2019s missing from simulator to make live integration trustworthy"},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#risk-management-table","title":"Risk Management Table","text":"<p>Instead of setting unrealistic expectations, we define success as learning, and set pre-agreed \u201cno-go\u201d points where we decide whether to pivot, pause, or proceed.</p> Phase Risk Detection Point Mitigation Framing the Outcome Phase 1 \u2013 Feasibility &amp; Data Audit Data is too sparse or noisy for modeling During exploratory data analysis Recommend instrumentation upgrades or data fusion techniques \"We now know where the data gaps are \u2014 this saves time before deeper investment.\" Data governance issues (access, silos, security) During data ingestion setup Align with IT/security early, request data access plan \"We've uncovered key organizational blockers and created a plan to resolve them.\" KPI misalignment between business and technical teams Project kickoff / initial scoping Run a KPI workshop to align ML goals with business goals \"We avoided building the wrong thing by syncing on goals upfront.\" Phase 2 \u2013 ML Hypothesis Exploration Model underperforms baseline heuristics Offline model evaluation Try alternate targets, features, or problem formulations \"We learned early that this path didn\u2019t work \u2014 a fast fail saves long-term cost.\" Historical data doesn\u2019t represent real system variability While splitting train/test datasets Inject variation scenarios, synthetic data, or scenario analysis \"This shows us how much production dynamics need to be captured \u2014 future-ready insight.\" Simulator scope underestimated or late to start Midway through Phase 2 planning Begin minimal sim definition early; leverage existing process models \"We adjusted course early to give the sim time to mature in parallel with modeling.\" Phase 3 \u2013 Simulation Integration Simulator doesn\u2019t match real process behavior During sim-agent validation runs Use hybrid testing (compare sim vs logs), introduce realism layer \"We\u2019ve defined the fidelity gap \u2014 it guides safe deployment boundaries.\" RL agent learns unsafe or brittle policies Policy evaluation, safety rule tests Add reward penalties, safety constraints, domain knowledge filters \"We surfaced risky behaviors in simulation, not reality \u2014 that\u2019s a win.\" Simulation takes too long to run or scale Early prototyping or agent training phase Simplify physics, parallelize, use fast approximators \"We made simulation feasible for iterative learning \u2014 a core ML enabler.\" Phase 4 \u2013 Operator Decision Support Operators reject or ignore ML recommendations Pilot usage feedback sessions Use co-design, add explainability, emphasize augmentation not replacement \"We gained adoption insight that will improve user trust and engagement.\" Recommendations conflict with existing SOPs Pilot observation / domain review Involve SME validation, tune model constraints to respect SOPs \"We learned how to align AI with existing practices instead of fighting them.\" Engineers doubt the sim-to-real transferability Side-by-side testing with real-world Document simulator assumptions and transfer validation results \"We gave engineers visibility and trust in the testing chain \u2014 crucial for buy-in.\" Phase 5 \u2013 Limited Production Integration ML decisions cause unexpected system behavior Online testing in shadow/rollback mode Roll out with fallback controls, alerting, and sandboxing \"We safely scoped the test to learn what happens live without major exposure.\" Infrastructure can\u2019t support real-time inference Integration testing with IT Use batch predictions or edge deployment \"We now know what infra to upgrade \u2014 this is a digital transformation enabler.\" Metrics don\u2019t improve during pilot Post-deployment analysis Analyze gap between sim, model, and ops context; iterate \"We captured valuable feedback and used it to refine model logic.\" Phase 6 \u2013 Full Rollout Readiness Internal team not ready to own/maintain system Readiness assessment, feedback interviews Run capability training and handoff documentation \"We identified where knowledge transfer was needed before scale-up.\" No retraining or monitoring pipeline in place During post-pilot audits Build MLOps tooling or retraining loop before scaling \"We baked in long-term sustainability and learning capacity.\" Organizational resistance to expanding AI use Stakeholder review post-pilot Share success stories, build champions, tie outcomes to business value \"We now have real-world results to guide buy-in and cultural adoption.\""},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#insight-even-if-ml-doesnt-scale-the-client-walks-away-with-data-visibility-process-audits-and-foundational-upgrades-all-real-business-value","title":"Insight: Even if ML doesn\u2019t scale, the client walks away with data visibility, process audits, and foundational upgrades \u2014 all real business value.","text":""},{"location":"guides/ai_product_development/framework_that_embraces_uncertainty/#_1","title":"Framework That Embraces Uncertainty","text":""},{"location":"guides/ai_product_development/overview/","title":"Building AI Is Not Like Building a Bridge","text":"<p>Artificial Intelligence and Machine Learning are powerful tools, but they\u2019re fundamentally different from traditional engineering projects. While civil or mechanical engineering relies on centuries of accumulated knowledge and deterministic principles, AI operates in a world of uncertainty, data variability, and adaptive logic.</p> <p>Treating AI like a solved field - something to rapidly scope, plan, and deliver like clockwork - is a recipe for disappointment, leading to mismatched expectations, slow adoption, and underwhelming results</p> <p>Here\u2019s how it compares:</p> Traditional Engineering (e.g., Building a Bridge) AI/ML Product Development Deterministic systems with well-defined inputs/outputs Probabilistic models based on data distributions Failure is often catastrophic (and unacceptable) \u201cFailure\u201d is a learning step; iteration is expected Methods validated over centuries Methods still emerging and rapidly evolving Primary risk: material/environmental conditions Primary risk: data quality, drift, user adoption <p>But here\u2019s the good news: with the right mindset and framework, AI can deliver massive value. The trick isn\u2019t to eliminate uncertainty - it\u2019s to design around it. </p> <p>AI is a process of discovery, not just implementation. When it is supported by a framework that embraces uncertainty rather than ignoring it, AI becomes a powerful enabler of real-world impact. In the next sections, I\u2019ll share some ideas I\u2019ve seen work throughout my career to properly address uncertainty in ML/AI projects: </p> <p>A Framework that Embraces Uncertainty</p> <p>The Skateboard Mindset for staged delivery and validation </p> <p>The MoSCoW Method for prioritizing what's essential now vs later</p>"},{"location":"guides/ai_product_development/skateboard_mindset/","title":"The Skateboard Mindset","text":""},{"location":"guides/ai_product_development/skateboard_mindset/#the-skateboard-mindset-a-practical-approach-to-mvps","title":"The Skateboard Mindset: A Practical Approach to MVPs","text":"<p>(Based on Henrik Kniberg\u2019s article on Making Sense of MVP)</p>"},{"location":"guides/ai_product_development/skateboard_mindset/#what-is-the-skateboard-mindset","title":"What is the Skateboard Mindset?","text":"<p>The Skateboard Mindset is an approach to building Minimum Viable Products (MVPs) with a focus on usability and value delivery rather than just isolated functionality.  </p> <p>Instead of delivering incomplete parts of a system, this mindset encourages teams to create small but functional iterations that work from day one.  </p>"},{"location":"guides/ai_product_development/skateboard_mindset/#how-it-works","title":"How It Works","text":"<p>Traditional MVP Mindset (Flawed Approach) A common mistake when building an MVP is treating it as a step-by-step assembly of the final product. Example:  </p> <ol> <li>Wheels \u2192  No usability yet.  </li> <li>Chassis \u2192  Still unusable.  </li> <li>Car Frame \u2192  Still not functional.  </li> <li>Final Car \u2192  Finally usable\u2014but only at the end.  </li> </ol> <p>The Skateboard Mindset (Better Approach) Instead, we start with a small, usable product and iterate towards a better version:  </p> <ol> <li>Skateboard \u2192  Basic but functional.  </li> <li>Scooter \u2192  More comfort &amp; usability.  </li> <li>Bicycle \u2192  Faster, scalable improvement.  </li> <li>Motorcycle \u2192  Improved efficiency.  </li> <li>Car \u2192  Fully featured, refined product.  </li> </ol> <p></p>"},{"location":"guides/ai_product_development/skateboard_mindset/#why-it-matters","title":"Why It Matters","text":"<p>\u2714 Delivers value from day one\u2014each iteration is a usable product. \u2714 Encourages user feedback early, reducing risk. \u2714 Prevents waste by validating real needs before overbuilding.  </p> <p>This mindset helps teams prioritize features that deliver real user value, ensuring software evolves in functional, testable stages rather than incomplete fragments.  </p>"},{"location":"guides/ai_product_development/the_moscow_method/","title":"The MoSCoW Method","text":""},{"location":"guides/ai_product_development/the_moscow_method/#the-moscow-method","title":"The MoSCoW Method","text":""},{"location":"guides/ai_product_development/the_moscow_method/#what-is-the-moscow-method","title":"What is the MoSCoW Method?","text":"<p>The MoSCoW method is a simple yet effective prioritization framework used in Agile development and project management. It helps teams decide which features or tasks should be prioritized first based on their importance and impact.  </p>"},{"location":"guides/ai_product_development/the_moscow_method/#how-it-works","title":"How It Works","text":"<p>Features, tasks, or requirements are categorized into four priority levels:  </p> <ol> <li>Must-Have \u2192 Critical features that are essential for the product to function. Without these, the project fails.  </li> <li> <p>Example: A login system for an authentication-based app.  </p> </li> <li> <p>Should-Have \u2192 Important features that significantly enhance the product but are not critical for immediate release.  </p> </li> <li> <p>Example: Dark mode for an app\u2014it improves user experience but isn't a blocker.  </p> </li> <li> <p>Could-Have \u2192 Nice-to-have features that can be added if time and resources allow but won't negatively impact the product if omitted.  </p> </li> <li> <p>Example: Custom user themes for personalization.  </p> </li> <li> <p>Won\u2019t-Have (for now) \u2192 Features that are out of scope for the current development cycle but could be revisited later.  </p> </li> <li>Example: Multi-language support when launching an MVP in a single market.  </li> </ol> <p></p>"},{"location":"guides/ai_product_development/the_moscow_method/#why-it-matters","title":"Why It Matters","text":"<p>Prevents scope creep by forcing teams to make trade-offs. Aligns stakeholders and developers on priorities. Helps deliver valuable software faster by focusing on essentials.  </p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/","title":"Clustering Algorithms","text":"<p>Clustering algorithms are unsupervised learning techniques used to automatically group data points based on similarity. Unlike supervised learning, clustering doesn\u2019t rely on predefined labels. Instead, it explores the structure of the data to discover meaningful groupings (clusters) based on feature patterns. Clustering is often used alongside dimensionality reduction techniques like PCA or t-SNE to visualize complex high-dimensional relationships in a lower-dimensional space.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#what-they-are-used-for","title":"What they are used for","text":"<p>Clustering algorithms are powerful tools for:</p> <ul> <li>Customer segmentation: Grouping users or stores by behavior or demographics (e.g., segmenting retail stores by size, sales, and geography)</li> <li>Anomaly detection: Identifying outliers by looking at how isolated they are from any cluster</li> <li>Market basket analysis: Finding purchasing patterns across similar groups of customers or products</li> </ul> <p>These applications are especially common when no labeled data is available, and the goal is to explore or categorize data in an interpretable way.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#types-of-algorithms","title":"Types of Algorithms","text":"<p>There are several families of clustering algorithms, each with different assumptions about the data structure:</p> <ul> <li>Partitioning-based (e.g., K-Means): Assigns data into a fixed number of clusters by minimizing variance.</li> <li>Hierarchical: Builds a tree of clusters by iteratively merging or splitting groups based on distance.</li> <li>Density-based (e.g., DBSCAN): Groups points that are closely packed together and labels sparse regions as outliers.</li> <li>Model-based (e.g., GMM): Assumes data is generated from a mixture of probabilistic distributions.</li> </ul> <p>Each algorithm comes with trade-offs depending on your dataset shape, dimensionality, and noise.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#k-means","title":"K-Means","text":"<p>A simple and popular partitioning method that:</p> <ul> <li>Groups data by minimizing the within-cluster sum of squares (WCSS)</li> <li>Assumes spherical clusters and equal variance</li> <li>Requires the number of clusters (K) to be defined in advance</li> <li>Works well on linearly separable and evenly sized clusters</li> </ul> <p>Internally, it uses Euclidean distance in high-dimensional space to assign points to the closest centroid and iteratively updates cluster centers.</p> <p>Common enhancements:</p> <ul> <li><code>k-means++</code> initialization (for smarter seeding)</li> <li>Elbow method or Silhouette score to choose K</li> </ul>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#lets-deep-dive-into-the-marvelous-linear-algebra-behind-k-means","title":"Let's deep dive into the marvelous linear algebra behind K-Means.","text":"<p>To truly understand how K-Means works, it\u2019s helpful to look at it from a linear algebra perspective. At its core, K-Means iteratively minimizes the Within-Cluster Sum of Squares (WCSS), a form of squared Euclidean distance that can be expressed using vector operations.</p> <p>The process can be interpreted as:</p> <ol> <li>Projecting points into a K-dimensional vector space, where each cluster center is a vector.</li> <li>Assigning each point to the cluster whose centroid vector has the minimum Euclidean distance to it.</li> <li>Recalculating centroids as the mean vector of all points currently assigned to each cluster.</li> </ol> <p>This repeats until convergence - i.e., assignments no longer change or the WCSS stops decreasing significantly.</p> <p>This mathematical framework allows K-Means to be both simple and fast, especially in high-dimensional data like store segmentation, where human visualization isn't possible but algebraic representations shine.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#context-store-clustering","title":"Context: Store Clustering","text":"<p>Imagine you have a dataset like this for each store, where each row is a high-dimensional vector representing the store\u2019s characteristics. These vectors form a cloud of points in a 4D space. K-Means acts like a gravitational system, pulling these points toward central locations (centroids) until each group stabilizes.</p> Store ID Monthly Purchase #Categories Store Size (m\u00b2) region_1 region_2 001 8,000 4 100 1 0 002 3,000 2 50 0 1 ... ... ... ... ... ... <p>Each of these values becomes a dimension in the clustering algorithm. Even though you can\u2019t visualize this 4D space, the algorithm processes it just like 2D or 3D \u2014 just with more features.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#task","title":"Task:","text":"<p>We want to group stores into clusters like:</p> <ul> <li>Cluster A: Big stores buying a lot of products across many categories.</li> <li>Cluster B: Small convenience stores buying little.</li> <li>Cluster C: Medium-size stores in suburban cities with average sales.</li> </ul> <p>To achieve that, K-Means looks for groupings with minimal internal variance. In practice, this gives rise to insights that help with operational planning, targeted marketing, and supply chain optimization. These clusters are interpretable, data-driven segments \u2014 not arbitrary groupings.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#actions","title":"Actions:","text":""},{"location":"guides/data_science_and_ai/clustering_algorithms/#1-standardize-the-data","title":"1. Standardize the Data","text":"<p>Why? Because features like purchase value and store size are on different scales.</p> <p>Standardization formula: <pre><code>z = (x - \u03bc)/\u03c3\n</code></pre> This centers the data (mean = 0, std = 1), preventing large features (e.g., purchase $) from dominating.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#2-initialize-k-centroids","title":"2. Initialize K Centroids","text":"<p>Let\u2019s say you pick K = 3.</p> <p>Think of this as placing 3 random \"magnets\" in the feature space.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#so-how-are-these-magnets-placed-under-the-hood","title":"So how are these magnets placed under the hood?","text":"<p>The default (naive) approach picks K random data points as initial centroids \u2014 but this can lead to poor or slow convergence if those points are too close or not representative.</p> <p><code>k-means++</code>: Smarter Initialization</p> <p><code>k-means++</code> improves this by spreading out initial centroids more intelligently:</p> <ol> <li>Randomly select the first centroid from the data.</li> <li>For each remaining point x, compute its distance squared from the nearest already chosen centroid.</li> <li>Choose the next centroid with probability proportional to that distance (further points are more likely to be chosen).</li> <li>Repeat until you have K centroids.</li> </ol>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#numerical-example","title":"Numerical Example","text":"<p>Suppose your stores (after standardization) are:</p> <ul> <li>A: [0.5, 0.2]</li> <li>B: [-0.6, -0.4]</li> <li>C: [0.9, 1.0]</li> <li>D: [0.3, -0.2]</li> <li>E: [-0.8, 0.6]</li> </ul> <p>Let\u2019s initialize K = 2:</p> <ul> <li>Randomly select A as the first centroid.</li> <li>Compute distances to A:</li> <li>B: d\u00b2 \u2248 1.17</li> <li>C: d\u00b2 \u2248 1.34</li> <li>D: d\u00b2 \u2248 0.20</li> <li>E: d\u00b2 \u2248 1.70</li> </ul> <p>Now we sample the second centroid from B, C, D, or E - but E is most likely since it\u2019s furthest from A.</p> <p>This leads to better-separated starting points and helps avoid poor local minima.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#3-assign-points-to-closest-centroid","title":"3. Assign Points to Closest Centroid","text":"<p>Each store is assigned to the cluster with the nearest centroid using Euclidean distance:</p> <p>Euclidean Distance formula between a store x\u1d62 and centroid \u03bc\u2096: <pre><code>d[x\u1d62, \u03bc\u2096] = Sqrt[(x\u1d62^(1) - \u03bc\u2096^(1))\u00b2 + (x\u1d62^(2) - \u03bc\u2096^(2))\u00b2 + ...]\n</code></pre></p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#under-the-hood-practical-example","title":"Under the Hood \u2014 Practical Example","text":"<p>Let\u2019s say:</p> <ul> <li> <p>Store A (standardized):    <pre><code>x = {0.5, 0.2}\n</code></pre></p> </li> <li> <p>Centroid 1:    <pre><code>\u03bc\u2081 = {0.3, 0.1}\n</code></pre></p> </li> <li> <p>Centroid 2:   <pre><code>\u03bc\u2082 = {-0.6, -0.4}\n</code></pre></p> </li> </ul> <p>Compute distances: <pre><code>d[x, \u03bc\u2081] = Sqrt[(0.5 - 0.3)\u00b2 + (0.2 - 0.1)\u00b2] = Sqrt[0.04 + 0.01] = Sqrt[0.05] \u2248 0.22\n</code></pre></p> <pre><code>d[x, \u03bc\u2082] = Sqrt[(0.5 + 0.6)\u00b2 + (0.2 + 0.4)\u00b2] = Sqrt[1.21 + 0.36] = Sqrt[1.57] \u2248 1.25\n</code></pre> <p>Store A is assigned to Centroid 1.</p> <p>This process repeats for every store in the dataset at every iteration of the algorithm.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#a-little-bit-of-math-what-is-euclidean-distance","title":"A little bit of math: What is Euclidean Distance?","text":"<p>Euclidean distance is just a generalization of the Pythagorean theorem into higher dimensions.</p> <p>Formula in n-dimensional space: <pre><code>d[x, y] = Sqrt[Sum[(x\u1d62 - y\u1d62)\u00b2, {i, 1, n}]]\n</code></pre></p> <p>Where:</p> <pre><code>x = {x\u2081, x\u2082, ..., x\u2099}\n</code></pre> <pre><code>y = {y\u2081, y\u2082, ..., y\u2099}\n</code></pre> <p>n is the number of features (dimensions)</p> <p>Even in a 4D, 7D, or 100D space, the math remains the same. You're just summing more squared differences.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#4-recalculate-the-centroids","title":"4. Recalculate the Centroids","text":"<p>Once every store has been assigned to the closest centroid, we need to recalculate the position of each centroid to reflect the new center of the stores that now belong to its cluster.</p> <p>Why? Because each centroid should represent the \"average\" position of the stores currently assigned to it. This step ensures that centroids gradually drift toward the true center of mass of their cluster \u2014 leading to better groupings in the next round.</p> <p>Centroid formula: <pre><code>\u03bc\u2096 = (1/N\u2096) * Sum[x\u1d62, {i, 1, N\u2096}]\n</code></pre> Where:</p> <ul> <li>N\u2096 is the number of stores in cluster k</li> <li>x\u1d62 are the data points in cluster k</li> </ul>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#numerical-example_1","title":"Numerical Example","text":"<p>Let\u2019s assume you\u2019re clustering stores in 2D space based on standardized features: purchase value and store size.</p> <p>After assigning points in Step 3, suppose:</p> <ul> <li>Cluster 1 has Stores A and C:</li> <li>A: [0.5, 0.2]</li> <li>C: [0.3, -0.1]</li> </ul> <p>Now recalculate the centroids:</p> <ul> <li>Centroid 1 (for Cluster 1):   <pre><code>\u03bc\u2081 = (1/2) * ({0.5, 0.2} + {0.3, -0.1}) = (1/2) * {0.8, 0.1} = {0.4, 0.05}\n</code></pre></li> </ul> <p>This shift in centroids better represents the current groupings. In the next iteration, these updated centroids will be used to reassign the stores again.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#5-repeat-steps-3-4-until-convergence","title":"5. Repeat Steps 3 &amp; 4 Until Convergence","text":"<p>The K-Means algorithm loops through Steps 3 and 4 \u2014 assignment and centroid update \u2014 until one of two stopping conditions is met:</p> <ul> <li>Convergence: Cluster assignments no longer change.</li> <li>Centroid shift is small: Movement of centroids is below a chosen threshold (e.g., tolerance = 0.0001).</li> </ul>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#understanding-the-cost-function","title":"Understanding the Cost Function","text":"<p>The algorithm is minimizing this cost function: <pre><code>J = Sum[Sum[Norm[x\u1d62 - \u03bc\u2096]\u00b2, {x\u1d62 \u2208 C\u2096}], {k, 1, K}]\n</code></pre> Where:</p> <ul> <li>x\u1d62: A store (data point)</li> <li>\u03bc\u2096: The centroid of cluster k</li> <li>\u2016x\u1d62 - \u03bc\u2096\u2016\u00b2: The squared Euclidean distance between a store and its cluster center</li> <li>C\u2096: The set of stores assigned to cluster k</li> </ul> <p>This function measures how tight the clusters are \u2014 the total sum of squared distances from each point to its centroid.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#numerical-example_2","title":"Numerical Example","text":"<p>Suppose Cluster 1 has:</p> <ul> <li>A: [0.5, 0.2]</li> <li>C: [0.3, -0.1]</li> <li>Centroid: [0.4, 0.05]</li> </ul> <p>Calculate WCSS (within-cluster sum of squares) for Cluster 1:</p> <ul> <li> <p>Distance A to centroid:   <pre><code>Norm[{0.5, 0.2} - {0.4, 0.05}]\u00b2 = (0.1)\u00b2 + (0.15)\u00b2 = 0.01 + 0.0225 = 0.0325\n</code></pre></p> </li> <li> <p>Distance C to centroid:   <pre><code>Norm[{0.3, -0.1} - {0.4, 0.05}]\u00b2 = (-0.1)\u00b2 + (-0.15)\u00b2 = 0.01 + 0.0225 = 0.0325\n</code></pre></p> </li> </ul> <p>Total cost for Cluster 1: <pre><code>J\u2081 = 0.0325 + 0.0325 = 0.065\n</code></pre> The algorithm will attempt to minimize the total J across all clusters, iterating until improvement stops.</p>"},{"location":"guides/data_science_and_ai/clustering_algorithms/#summary-of-the-math-all-in-one-view","title":"Summary of the Math (All in One View)","text":"Concept Math How It\u2019s Used in K-Means Distance d[x\u1d62, \u03bc\u2096] = Sqrt[Sum[(x\u1d62^(j) - \u03bc\u2096^(j))\u00b2, {j}]] Measures how close a store is to a centroid for cluster assignment Centroid Update \u03bc\u2096 = (1/N\u2096) * Sum[x\u1d62, {x\u1d62 \u2208 C\u2096}] Repositions the centroid to reflect the average of its assigned stores Cost Function J = Sum[Sum[Norm[x\u1d62 - \u03bc\u2096]\u00b2, {x\u1d62 \u2208 C\u2096}], {k}] The objective function minimized through iterative reassignments"},{"location":"guides/data_science_and_ai/clustering_algorithms/#limitations-to-keep-in-mind","title":"Limitations to Keep in Mind","text":"<ul> <li>K-Means assumes spherical clusters - not great if your stores form irregular shapes in the feature space.</li> <li>You must predefine K (although you can use methods like the Elbow Method to find it).</li> <li>It\u2019s sensitive to initialization (which is why <code>k-means++</code> was introduced).</li> </ul>"},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/","title":"Data Science Products as Python Packages","text":""},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#why-package-data-science-projects","title":"Why Package Data Science Projects?","text":"<p>Transforming data science projects into Python packages offers several advantages:  </p> <p>Reusability \u2013 Packages enable modular code that can be reused across multiple projects, avoiding redundant work. Collaboration \u2013 Easily share and distribute components with your team, ensuring consistency across projects. Simplified Deployment \u2013 Packaging makes it easier to integrate machine learning models, utilities, and pipelines into production environments. Better Testing &amp; Maintainability \u2013 Structured packages help in writing tests, managing dependencies, and scaling projects effectively.  </p> <p>By structuring data science workflows as Python packages, teams can improve efficiency, reduce development overhead, and enhance software reliability.  </p>"},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#how-to-package-a-python-project","title":"How to Package a Python Project","text":"<p>A Python package is a structured way to organize and distribute code. The official Python Packaging Authority (PyPA) recommends the following steps, as outlined in the official tutorial.  </p>"},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#basic-python-package-structure","title":"Basic Python Package Structure","text":"<p>A minimal Python package follows this structure:  </p> <pre><code>my_project/\n\u2502\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 my_package/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py  # Marks this as a package\n\u2502   \u2502   \u251c\u2500\u2500 module1.py   # Example module\n\u2502   \u2502   \u251c\u2500\u2500 module2.py   # Another module\n\u2502\u2500\u2500 tests/               # Test suite\n\u2502\u2500\u2500 pyproject.toml       # Project metadata &amp; dependencies\n\u2502\u2500\u2500 README.md            # Documentation\n\u2502\u2500\u2500 LICENSE              # License file\n</code></pre> <p>Key Components</p> <p>src/ \u2013 Contains the actual package code.</p> <p>tests/ \u2013 Unit tests for the package.</p> <p>pyproject.toml \u2013 Defines dependencies and build configuration.</p> <p>README.md \u2013 Provides documentation.</p>"},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#using-cookiecutter-for-project-templates","title":"Using Cookiecutter for Project Templates","text":""},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#what-is-cookiecutter","title":"What is Cookiecutter?","text":"<p>Cookiecutter is a powerful tool that allows developers to quickly scaffold project templates, ensuring best practices are followed from the start.</p>"},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#why-use-cookiecutter","title":"Why use Cookiecutter?","text":"<ul> <li> <p>Saves Time \u2013 Generates a consistent, ready-to-use project structure in seconds.</p> </li> <li> <p>Enforces Best Practices \u2013 Ensures your code follows proper package structures and industry standards.</p> </li> <li> <p>Improves Collaboration \u2013 Standardized templates make onboarding new developers easier.</p> </li> </ul>"},{"location":"guides/data_science_and_ai/ds_products_as_python_packages/#my-cookiecutter-implementation","title":"My Cookiecutter Implementation","text":"<p>I\u2019ve built a custom Cookiecutter template that fits my workflow and best practices. Check it out here:</p> <p>My Cookiecutter Template</p> <p>Cookiecutter in Action</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/","title":"Hands-on Reinforcement Learning Application: PPO for Chemical Process Control","text":"<p>This project illustrates the core concepts of Reinforcement Learning (RL) \u2014 such as policy optimization, value estimation, and safe exploration \u2014 through the implementation of a PPO agent managing an industrial Continuous Stirred Tank Reactor (CSTR). It demonstrates how RL can lead to significant operational and economic gains in real-world industrial scenarios.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#scenario-specialty-chemical-manufacturing-process-industry","title":"Scenario: Specialty Chemical Manufacturing [Process Industry]","text":"<p>Client: NovaSynauta Labs Inc.</p> <p>Facility: Modular continuous reactor line in British Columbia</p> <p>Technology: CSTR used in exothermic reaction for specialty intermediate synthesis</p> <p>Problem: Maintaining stable product quality and safety margins in an exothermic reaction is challenging. The process is sensitive to feed composition and temperature drift, often requiring manual adjustments by experienced operators.</p> <p>Current operations:</p> <ul> <li>Feed rates and concentrations are kept within conservative ranges.</li> <li>Manual monitoring and correction is frequent, especially during startup or process drift.</li> </ul> <p>Business implications:</p> <ul> <li>Conservative operations lead to lower throughput and efficiency.</li> <li>Overcorrecting feed composition to prevent overheating can cause reagent wastage.</li> </ul> <p>In continuous chemical operations, RL enables smart control that reacts to process dynamics in real time while respecting safety and quality constraints.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#the-environment-continuous-stirred-tank-reactor-cstr","title":"The Environment: Continuous Stirred Tank Reactor (CSTR)","text":"<p>We use the CSTR environment from PC-Gym, which models a continuous reactor where two input flows drive a temperature-sensitive, exothermic reaction.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#cstr-in-industrial-chemistry","title":"CSTR in Industrial Chemistry:","text":"<p>CSTRs are used across pharmaceuticals, petrochemicals, and specialty chemical production to carry out steady-state reactions. The main challenge is maintaining optimal reactor temperature and concentration to ensure product quality and safety.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#greenhouse-analogy-climate-control-for-fragile-plants","title":"Greenhouse Analogy: Climate Control for Fragile Plants","text":"<p>Imagine a greenhouse filled with delicate tropical plants. These plants grow best within a narrow temperature range \u2014 not too hot, not too cold.</p> <ul> <li>You can't control the sunlight or humidity directly.</li> <li>Your only lever is the cooling system (like the temperature-controlled vents or fans).</li> <li>On hot days, you lower the cooling system temperature to avoid plant stress.</li> <li>On cool days, you reduce cooling to keep conditions optimal.</li> </ul> <p>Just like in the CSTR environment, the RL agent adjusts only the cooling jacket temperature \u2014 similar to how a greenhouse operator tunes the cooling system. The goal is to keep the internal climate stable and productive, without wasting energy or risking overheating.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#the-goal-and-kpis","title":"The Goal and KPIs","text":"<p>Primary Goal: Maximize production efficiency (reaction conversion) while maintaining reactor temperature within safe bounds.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#key-kpis-for-cstr-optimization","title":"Key KPIs for CSTR Optimization:","text":"KPI Description Why it matters Cumulative Reward Encodes conversion rate vs. penalty for overheating Measures policy performance Temperature Violations Number of steps above safe temperature Indicates safety and operational risk Conversion Efficiency Output concentration as a function of input Reflects reaction success Reagent Usage Efficiency Product per unit reagent used Tracks economic and sustainability value Rule-Based Comparison RL vs. static control baseline Highlights learning performance gains"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#comparison-to-rule-based-operations","title":"Comparison to Rule-Based Operations","text":"<p>The rule-based baseline reflects a conservative logic:</p> <pre><code>if temperature &gt; 360:\n    reduce flow and concentration\nelse:\n    keep steady\n</code></pre> <p>This avoids runaway reactions, but sacrifices throughput and material efficiency.</p> <p>Our RL agent will be evaluated based on whether it can:</p> <ul> <li>Anticipate overheating and adjust proactively.</li> <li>Use more assertive settings safely when conditions allow.</li> <li>Increase output without wasting reagent.</li> <li>Support operators in managing drift, startup, and transient behaviors.</li> </ul> <p>Matching or outperforming the baseline proves RL's value as a partner in chemical process optimization.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#environment-variables","title":"Environment Variables","text":"<p>At every time step, the environment send a vector of observations that describe the internal condition of the reactor. These variables are essential for understanding the chemical reaction dynamics and guiding decisions about temperature control.</p> Observation Unit Description Ca mol/L Concentration of reactant A. A key input to the reaction. Lower Ca may signal progress or depletion. T K (Kelvin) Reactor temperature. Central to both safety and reaction rate. High T = faster reaction but riskier. Cb mol/L Concentration of product B. Indicates how much reactant has converted into product. Proxy for yield."},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#the-levers-the-agent-can-pull-in-this-environment","title":"The Levers the Agent Can Pull in this Environment","text":"<p>The RL agent in the CSTR environment controls a single continuous action: the temperature of the cooling jacket. This jacket indirectly regulates the reactor temperature and thus the reaction rate and safety.</p> Action Range Effect Jacket Temperature 290 \u2013 302 K Adjusts reactor cooling. Lower temperature = more cooling (slows reaction), higher = less cooling (faster reaction, but risk of overheating)."},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#industrial-interpretation","title":"Industrial Interpretation","text":"<ul> <li>The jacket acts like an external thermostat - controlling the environment surrounding the reactor.</li> <li>The agent\u2019s job is to manipulate this temperature smartly to keep the reaction steady and productive.</li> </ul> <p>In real-world operations, operators would adjust cooling rates based on experience or safety protocols. The RL agent learns to do this continuously and adaptively, helping maintain stability without overreacting to every fluctuation.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#smart-control-via-rl","title":"Smart Control via RL","text":"Time Step Jacket Temp (K) Reactor Temp (K) Interpretation 0 295.0 340.0 Stable production. Jacket provides cooling. 5 293.5 348.0 Anticipating heat buildup \u2014 more cooling. 10 292.0 356.5 Near thermal limit \u2014 aggressive cooling. 15 294.5 345.0 Temp stabilized \u2014 easing off cooling. 20 296.0 350.0 Maintaining optimal range with fine-tuning."},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#what-the-rl-agent-learns","title":"What the RL Agent Learns:","text":"<ul> <li>Proactively lowers jacket temperature as internal heat accumulates from reaction dynamics.</li> <li>Recovers from overshoot by adjusting cooling back up gradually.</li> <li>Maintains safety and throughput by learning when and how much to cool \u2014 instead of reacting too late or overcorrecting.</li> </ul> <p>This illustrates how an RL agent can act as a predictive control assistant, adjusting continuously and intelligently even with just one lever: temperature regulation.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#reward-function-how-the-agent-learns","title":"Reward Function: How the Agent Learns","text":"<p>The reward function is the core mechanism that teaches the RL agent how to control the CSTR effectively. Understanding how rewards are calculated is crucial for interpreting agent behavior and optimizing performance.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#reward-calculation-formula","title":"Reward Calculation Formula","text":"<p>The reward is calculated using a tracking control approach that penalizes deviations from target setpoints:</p> <pre><code>Reward = -\u03a3(r_scale[k] \u00d7 (state[k] - setpoint[k][t])\u00b2)\n</code></pre> <p>Where: - <code>state[k]</code> = Current sensor reading for variable k (e.g., Ca concentration) after the action is applied - <code>setpoint[k][t]</code> = Target value for variable k at the current timestep t - <code>r_scale[k]</code> = Reward scaling factor for variable k (makes penalties more significant) - The negative sign makes it a penalty (minimization problem)</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#variables-explained","title":"Variables Explained","text":"Variable Description Example <code>state[k]</code> Current sensor reading after action Ca = 0.816 mol/L <code>setpoint[k][t]</code> Target value for current timestep Target Ca = 0.90 mol/L <code>r_scale[k]</code> Reward scaling factor r_scale['Ca'] = 1000 <code>t</code> Current timestep t = 1"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#real-world-example","title":"Real-World Example","text":"<p>Scenario: Agent applies cooling action, resulting in: - Current Ca concentration: 0.816 mol/L - Target Ca concentration: 0.90 mol/L - Reward scaling: 1000</p> <p>Calculation: <pre><code>Error = 0.816 - 0.90 = -0.084\nSquared Error = (-0.084)\u00b2 = 0.007056\nReward = -1000 \u00d7 0.007056 = -7.06\n</code></pre></p> <p>Interpretation: The agent receives a negative reward (-7.06) because the current concentration (0.816) deviates from the target (0.90). The large magnitude is due to the scaling factor of 1000, which provides stronger learning signals.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#why-this-reward-design-works","title":"Why This Reward Design Works","text":"<ol> <li>Encourages Setpoint Tracking: The agent learns to keep sensor values close to targets</li> <li>Provides Strong Learning Signal: The scaling factor makes small deviations meaningful</li> <li>Supports Real-World Objectives: Tracks the desired operating trajectory</li> <li>Enables Safe Operation: Penalizes deviations that could lead to quality or safety issues</li> </ol>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#reward-scaling-importance","title":"Reward Scaling Importance","text":"<p>The <code>r_scale</code> parameter is crucial because: - Without scaling: Small deviations (0.0025) produce tiny rewards (-0.0025) - With scaling (1000): Same deviations produce meaningful rewards (-2.5) - Result: Agent receives stronger feedback for learning optimal control strategies</p> <p>The reward function transforms the tracking control problem into a reinforcement learning problem that the agent can solve through trial and error.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#setpoints-real-world-operating-targets","title":"Setpoints: Real-World Operating Targets","text":"<p>Setpoints represent the desired operating conditions that the CSTR should maintain throughout the production process. These are not arbitrary values but carefully designed trajectories based on real-world chemical engineering principles.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#what-are-setpoints","title":"What Are Setpoints?","text":"<p>Setpoints are target values for key process variables (like concentration) that change over time according to a predefined trajectory. They represent the optimal operating path that maximizes yield, quality, and safety.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#real-world-significance","title":"Real-World Significance","text":"<p>Who Defines Setpoints: - Process Engineers: Based on reaction kinetics and thermodynamics - Chemical Engineers: Ensuring safety constraints and optimal conditions - Production Managers: Balancing quality vs. quantity requirements - Quality Control: Meeting product specifications and regulatory compliance</p> <p>Why Setpoints Change Over Time: - Startup Phase: Safe catalyst activation and gradual temperature increase - Production Phase: Optimal conditions for maximum yield - Transition Phase: Process adjustments for different product requirements - Shutdown Phase: Controlled deactivation and safe cooling</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#configuration-example","title":"Configuration Example","text":"<p>In your <code>cstr_environment.yaml</code>:</p> <pre><code>setpoints:\n  ca_profile:\n    - value: 0.85         # Startup phase - safe startup conditions\n      duration: 3          # Apply for 3 timesteps\n    - value: 0.9          # Production phase - maximum yield\n      duration: 3          # Apply for 3 timesteps  \n    - value: 0.87         # Transition phase - process adjustment\n      duration: 4          # Apply for 4 timesteps (fills remaining steps)\n</code></pre> <p>Real-World Interpretation: - Timesteps 0-2: Startup phase with conservative concentration (0.85) - Timesteps 3-5: Production phase with optimal concentration (0.90) - Timesteps 6-9: Transition phase with adjusted concentration (0.87)</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#real-world-consequences-of-setpoint-deviations","title":"Real-World Consequences of Setpoint Deviations","text":"Deviation Type Real-World Impact Business Cost Product Quality Off-spec material requiring reprocessing \\(50K-\\)200K per batch Safety Issues Runaway reactions, equipment damage \\(500K-\\)2M in damages Yield Loss Reduced product output from same inputs 10-30% efficiency loss Environmental Waste generation, regulatory violations Fines + reputation damage"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#example-pharmaceutical-manufacturing","title":"Example: Pharmaceutical Manufacturing","text":"<p>Setpoint Trajectory: <code>[0.85 \u2192 0.90 \u2192 0.87]</code></p> <p>Real-World Scenario: 1. Startup (0.85): Safe catalyst activation, gradual temperature increase 2. Production (0.90): Optimal conditions for maximum API yield 3. Transition (0.87): Process adjustment for different product specifications</p> <p>Why This Matters: - Catalyst Safety: Gradual startup prevents thermal runaway - Yield Optimization: Production phase maximizes conversion - Quality Control: Transition phase ensures product consistency</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#industrial-control-systems","title":"Industrial Control Systems","text":"<p>In real CSTR operations, multiple control systems work together:</p> <ol> <li>PID Controllers: Traditional feedback control</li> <li>Model Predictive Control (MPC): Advanced control using process models  </li> <li>Reinforcement Learning: Adaptive control (like your CSTR environment)</li> </ol> <p>The setpoint trajectory represents the desired operating path that these control systems try to follow, ensuring safe, efficient, and profitable operation.</p> <p>Setpoints are both theoretical (based on chemical engineering principles) and practical (representing real-world CSTR control challenges that operators face daily).</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#ppo-implementation-deployment-guide","title":"PPO Implementation &amp; Deployment Guide","text":""},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#1-overview","title":"1. Overview","text":"<p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed for stability, simplicity, and effectiveness in complex environments. PPO iteratively improves an agent's policy through a clear three-step process:</p> <ul> <li>Experience Collection (Rollouts): Collect data by interacting with the environment.</li> <li>Advantage Estimation (GAE): Evaluate how actions perform relative to expectations.</li> <li>Policy and Value Update: Improve decisions based on collected experiences.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#2-ppo-concepts-intuition","title":"2. PPO Concepts &amp; Intuition","text":""},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#model-free-vs-model-based-rl","title":"Model-Free vs Model-Based RL","text":"<p>In reinforcement learning, there are two approaches:</p> <ul> <li>Model-based: Uses an internal model of environment dynamics.</li> </ul> <pre><code>Real or Simulated State \u2192 Model \u2192 Simulated Next State &amp; Reward \u2192 Policy Update\n</code></pre> <ul> <li>Model-free: Learns directly from interaction, without explicit environment modeling.</li> </ul> <pre><code>Real State \u2192 Action from Policy \u2192 Real Observation &amp; Reward \u2192 Policy Update\n</code></pre> <p>PPO is model-free because it learns from direct interaction and real observations without needing an explicit model of the environment.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#actor-critic-framework","title":"Actor-Critic Framework","text":"<p>PPO uses two networks working together:</p> <ul> <li>Actor (Policy Network): Selects actions given states.</li> <li>Critic (Value Network): Estimates how good the current state is.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#ppo-analogies","title":"PPO Analogies","text":"<ul> <li>Actor: A chef choosing recipes.</li> <li>Critic: A food critic rating the chef\u2019s dishes.</li> <li>Action Mean: Recipe choice.</li> <li>Standard Deviation: Level of improvisation in the recipe.</li> <li>Clipped Updates: Chef making incremental, cautious changes to recipes.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#how-ppo-handles-bellman-expectation-and-optimality","title":"How PPO Handles Bellman Expectation and Optimality:","text":"<p>Your intuition was correct! PPO indirectly solves both the Bellman Expectation (Policy Evaluation) and Bellman Optimality (Policy Optimization) equations.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#bellman-expectation-policy-evaluation","title":"Bellman Expectation (Policy Evaluation):","text":"<p>Bellman Expectation calculates the expected returns given a policy:</p> <pre><code>V_\u03c0(s) = \u03a3_a \u03c0(a|s)[R(s,a) + \u03b3 \u03a3_s' P(s'|s,a)V_\u03c0(s')]\n</code></pre> <p>PPO addresses this through its Critic network, estimating V_\u03c0(s). The critic continuously evaluates the policy by minimizing the difference between its predictions and observed returns.</p> <p>Example:</p> <p>In the CSTR scenario, the critic estimates how valuable a given reactor state (temperature and concentrations) is when following your current policy. If the critic sees the reactor overheating after certain decisions, it will assign a lower value to the states/actions that caused overheating.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#bellman-optimality-policy-optimization","title":"Bellman Optimality (Policy Optimization):","text":"<p>Bellman Optimality finds the optimal policy, maximizing expected rewards:</p> <pre><code>V*(s) = max_a [R(s,a) + \u03b3 \u03a3_s' P(s'|s,a)V*(s')]\n</code></pre> <p>PPO's Actor network implicitly solves Bellman Optimality by improving the policy with respect to the critic's feedback (the advantage \u00c2_t).</p> <p>Example:</p> <p>When controlling the CSTR, PPO\u2019s actor learns the best cooling jacket temperatures to maximize outcomes and avoid dangerous temperatures. Initially, it tries random adjustments. The critic evaluates these and gives feedback (advantages), guiding the actor towards optimal control decisions.</p> <p>Summary:</p> Component Role Bellman Equation Actor Chooses action, improving policy gradually - Chef improvising (makes decisions) Optimality (Finds optimal policy) Critic Evaluates how good the chosen action/state is -  food critic providing feedback (evaluates decisions). Expectation (Evaluates given policy) PPO Clip Ensures stable, incremental policy improvements Stabilizes policy optimization"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#3-practical-workflow-for-real-scenarios","title":"3. Practical Workflow for Real Scenarios","text":"<p>Deploying PPO in real scenarios involves clearly defined stages:</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#training","title":"Training","text":"<ul> <li>Conducted in a simulated environment.</li> <li>Goal: Train the agent safely and efficiently.</li> <li>Ensure your simulation is high-fidelity, accurately mimicking real plant dynamics.</li> <li>Collect extensive logs to verify agent performance thoroughly.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#validation","title":"Validation","text":"<ul> <li>Perform extensive tests in your simulator under:</li> <li>Normal conditions</li> <li>Edge conditions (extreme or boundary scenarios)</li> <li>Disturbances (unexpected scenarios)</li> <li>Consider safety constraints (e.g., reactor temperature limits).</li> <li>Check your PPO policy outputs: verify no dangerous/unstable action recommendations.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#serving-inference","title":"Serving (Inference)","text":"<ul> <li>Deploy the trained policy in a real-world environment as inference-only.</li> <li>Fast and robust inference is critical.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#monitoring-retraining","title":"Monitoring &amp; Retraining","text":"<ul> <li>Continuous performance monitoring.</li> <li>Log states, actions, and critical KPIs (key performance indicators).</li> <li>Set up alerts for abnormal states or unsafe actions.</li> <li>Periodic retraining with updated data for sustained effectiveness</li> <li>Real plants often change over time due to equipment aging, sensor drifts, etc.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#4-ppo-training-loop","title":"4. PPO Training Loop","text":"<p>PPO training iteratively loops through three core phases:</p> <ol> <li>Rollouts: Collect data from the environment.</li> <li>GAE: Compute advantages for actions.</li> <li>Update: Adjust policy and value networks.</li> </ol> <p>Visual representation:</p> <pre><code>Environment\n     |\n     v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    PPO TRAINING CYCLE                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   STEP 1    \u2502    \u2502   STEP 2    \u2502    \u2502   STEP 3    \u2502       \u2502\n\u2502  \u2502  Rollouts   \u2502\u2500\u2500\u2500\u25b6\u2502    GAE      \u2502\u2500\u2500\u2500\u25b6\u2502   Update    \u2502       \u2502\n\u2502  \u2502             \u2502    \u2502             \u2502    \u2502             \u2502       \u2502\n\u2502  \u2502 \u2022 Test      \u2502    \u2502 \u2022 Compute   \u2502    \u2502 \u2022 Improve   \u2502       \u2502\n\u2502  \u2502   policy    \u2502    \u2502   advantages\u2502    \u2502   policy    \u2502       \u2502\n\u2502  \u2502 \u2022 Collect   \u2502    \u2502 \u2022 Normalize \u2502    \u2502 \u2022 Update    \u2502       \u2502 \n\u2502  \u2502   data      \u2502    \u2502 \u2022 Calculate \u2502    \u2502   critic    \u2502       \u2502\n\u2502  \u2502 \u2022 Record    \u2502    \u2502   returns   \u2502    \u2502 \u2022 Multiple  \u2502       \u2502\n\u2502  \u2502   probs     \u2502    \u2502             \u2502    \u2502   epochs    \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#cstr-example","title":"CSTR Example","text":"<ul> <li>Rollouts: Test reactor temperature adjustments.</li> <li>GAE: Evaluate reactor performance relative to expectations.</li> <li>Update: Refine temperature control strategy based on feedback.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#5-ppo-detailed-implementation","title":"5. PPO Detailed Implementation","text":""},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#experience-collection-rollouts","title":"Experience Collection (Rollouts)","text":"<p>Rollouts test the current policy in the environment and gather experience data:</p> <pre><code>states, actions, rewards, dones, values, log_probs_old = collect_trajectories(model, env)\n</code></pre> <p>Analogy: Pilot-testing a new restaurant menu.</p> <p>Why is it Essential for PPO?</p> <ul> <li> <p>Policy Evaluation: See how well the current policy performs</p> </li> <li> <p>Data Collection: Gather experience for training</p> </li> <li> <p>Value Estimation: Get critic's value estimates for advantage computation</p> </li> <li> <p>Importance Sampling: Record action probabilities for PPO's ratio calculation</p> </li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#advantage-estimation-gae","title":"Advantage Estimation (GAE)","text":"<p>Generalized Advantage Estimation (GAE) calculates how much better or worse actions performed compared to expectations.</p> <pre><code>advantages, returns = compute_gae(rewards, dones, values)\n</code></pre> <p>GAE Formula:   </p> <pre><code>GAE(\u03b3,\u03bb) = \u03a3(\u03b3\u03bb)^l * \u03b4_{t+l}\n</code></pre> <p>where </p> <pre><code>\u03b4_t = r_t + \u03b3V(s_{t+1}) - V(s_t)\n</code></pre> <p>Key Parameters:</p> <p>\u200b    - gamma (\u03b3): Discount factor - how much we value future vs immediate rewards. e.g. \"How much do we care about long-term reactor performance vs immediate temperature control?\"</p> <p>\u200b    - lambda (\u03bb): GAE parameter - balances bias vs variance in advantage estimation. e.g. \"How much do we trust our value estimates vs actual rewards?\"</p> <p>\u200b    - l: Time step index - represents how far into the future we look when computing advantages. e.g. \"How many future temperature adjustments do we consider when evaluating current performance?\"</p> <p>\u200b    - Delta (\u03b4): The immediate difference between actual reward and expected value .e.g. \"Did this temperature adjustment give us better results than expected?\"</p> <p>\u200b    - Advantages: How much better/worse an action was compared to expectations. e.g. \"How much better was this temperature adjustment than expected?\"</p> <p>\u200b    - Dones: Episode termination flags</p> <p>Understanding GAE and the Advantage Function:</p> <p>The Advantage Function measures whether an action is better or worse than the policy's default behavior:</p> <pre><code>A(s,a) = Q(s,a) - V(s)\n</code></pre> <p>Where: - Q(s,a): How good was this specific action in this state? - V(s): How good did we expect this state to be (on average)? - A(s,a): How much better/worse was this action than expected?</p> <p>GAE makes this calculation more stable by considering future rewards and smoothing the advantage estimate.</p> <p>Analogy: Restaurant critic expectations vs. actual dining experience.</p> <p>Think of GAE like a restaurant review system:</p> <ul> <li> <p>Expected Rating (Value Function): Critics predict how good a restaurant will be (e.g., 7/10)</p> </li> <li> <p>Actual Experience (Reward): You actually visit and rate it (e.g., 9/10)</p> </li> <li> <p>Advantage: The difference between expectation and reality (9 - 7 = +2)</p> </li> </ul> <p>If the advantage is positive, the action was better than expected \u2192 encourage similar actions. If negative, the action was worse \u2192 discourage similar actions.</p> <p>Exploring GAE Properties with Intuitive Analogies</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#understanding-gamma-the-discount-factor","title":"Understanding \u03b3 (Gamma) - The Discount Factor","text":"<p>Why This Matters for CSTR Control: - High \u03b3 (0.99): \"Future reactor efficiency matters almost as much as current efficiency\" - Low \u03b3 (0.5): \"Only immediate reactor performance really matters\"</p> <p>Visual Representation of \u03b3 Values <pre><code>\u03b3 = 0.99: Future reward = 0.99 \u00d7 0.99 \u00d7 0.99 \u00d7 ... (slow decay)\n\u03b3 = 0.5:  Future reward = 0.5 \u00d7 0.5 \u00d7 0.5 \u00d7 ... (fast decay)\n</code></pre></p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#understanding-lambda-the-bias-variance-trade-off","title":"Understanding \u03bb (Lambda) - The Bias-Variance Trade-off","text":"<p>\u03bb = 0.0 (Pure TD) - \"One-Step Lookahead\" - Pros: Quick to compute, low variance - Cons: May miss long-term patterns - CSTR Context: \"Judge temperature adjustment by immediate reward + critic's prediction of next state\"</p> <p>\u03bb = 1.0 (Pure Monte Carlo) - \"Complete Experience\" - Pros: Uses all available information, unbiased - Cons: High variance, requires complete episodes - CSTR Context: \"Judge temperature adjustment by actual reactor performance until episode ends\"</p> <p>\u03bb = 0.95 (Standard GAE) - \"Balanced Assessment\"\" - Pros: Best of both worlds - low bias, manageable variance - Cons: More complex to compute - CSTR Context: \"Judge temperature adjustment by immediate performance + expected future performance, weighted by confidence\"</p> <p>Visual Representation of \u03bb Values</p> <pre><code>\u03bb = 0.0 (TD):     [Current] \u2192 [Next Prediction]\n                   Immediate + One-step lookahead\n\n\u03bb = 0.5:          [Current] \u2192 [Next] \u2192 [Next+1] \u2192 [Next+2] \u2192 ...\n                   Weighted combination of multiple steps\n\n\u03bb = 0.95:         [Current] \u2192 [Next] \u2192 [Next+1] \u2192 [Next+2] \u2192 [Next+3] \u2192 ...\n                   Long-term weighted combination (standard)\n\n\u03bb = 1.0 (MC):     [Current] \u2192 [Next] \u2192 [Next+1] \u2192 [Next+2] \u2192 ... \u2192 [End]\n                   Complete episode experience\n</code></pre>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#cstr-specific-interpretations","title":"CSTR-Specific Interpretations","text":"<p>\u03b3 (Gamma) in CSTR Context: - \u03b3 = 0.99: \"Temperature adjustments today affect reactor performance for many future timesteps\" - \u03b3 = 0.9: \"Temperature adjustments have moderate long-term effects\" - \u03b3 = 0.5: \"Only immediate temperature control matters, future effects decay quickly\"</p> <p>\u03bb (Lambda) in CSTR Context: - \u03bb = 0.0: \"Judge temperature adjustment by immediate efficiency + critic's prediction\" - \u03bb = 0.95: \"Judge temperature adjustment by weighted combination of immediate and future performance\" - \u03bb = 1.0: \"Judge temperature adjustment by complete reactor performance until episode ends\"</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#practical-guidelines","title":"Practical Guidelines","text":"<p>When to Use Different \u03b3 Values: - \u03b3 = 0.99: Long-term planning (default for most RL) - \u03b3 = 0.9: Medium-term planning - \u03b3 = 0.5: Short-term/immediate rewards only</p> <p>When to Use Different \u03bb Values: - \u03bb = 0.95: Standard choice (best balance) - \u03bb = 0.0: When you need fast computation or have limited data - \u03bb = 1.0: When you have complete episodes and want unbiased estimates</p> <p>CSTR Optimization Recommendations: - \u03b3 = 0.99: Reactor control has long-term effects - \u03bb = 0.95: Standard GAE for stable learning - Combination: Balances immediate temperature control with long-term reactor efficiency</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#ppo-policy-and-value-updates","title":"PPO Policy and Value Updates","text":""},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#ppos-core-innovation-clipped-surrogate-objective","title":"PPO's Core Innovation: Clipped Surrogate Objective","text":"<p>PPO's main contribution is preventing the policy from changing too drastically in a single update. This is achieved through the clipped surrogate objective:</p> <pre><code>L^CLIP(\u03b8) = E[min(r_t(\u03b8)A_t, clip(r_t(\u03b8), 1-\u03b5, 1+\u03b5)A_t)]\n</code></pre> <p>where  <pre><code>`r_t(\u03b8) = \u03c0_\u03b8(a_t|s_t) / \u03c0_\u03b8_old(a_t|s_t)`\n</code></pre></p> <p>The Problem PPO Solves: Standard policy gradient methods can make large policy changes that lead to performance collapse. PPO prevents this by clipping the objective function to limit how much the policy can change.</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#value-function-clipping-optional-enhancement","title":"Value Function Clipping (Optional Enhancement):","text":"<p>Many modern PPO implementations also clip the value function to prevent the critic from making too large updates, which can destabilize training.</p> <p>Why This Matters: 1. Stability: Prevents performance collapse from aggressive updates 2. Conservative Learning: Allows for more aggressive learning rates 3. Sample Efficiency: Multiple epochs of updates on the same data 4. Value Stability: Prevents critic from making extreme changes</p> <p>For CSTR Context: - Actor Update: Improves temperature control strategy conservatively - Critic Update: Improves reactor state value estimation conservatively - Clipping: Prevents drastic changes to both policy and value function</p> <p>Analogy: Fine-Tuning a Master Chef - Before: Chef has certain cooking techniques (policy) - During: Chef tries new techniques based on feedback (advantages) - Clipping: Chef doesn't change too drastically (stays within 20% of original) - Multiple Epochs: Chef practices same recipes multiple times - After: Chef has refined techniques based on what worked well</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#how-clipping-works","title":"How Clipping Works:","text":"<ol> <li><code>r_t(\u03b8)A_t</code>: Standard policy gradient objective</li> <li><code>clip(r_t(\u03b8), 1-\u03b5, 1+\u03b5)A_t</code>: Clipped version that limits ratio to <code>[1-\u03b5, 1+\u03b5]</code></li> <li><code>min(...)</code>: Take the minimum to ensure we don't make changes that are too large</li> <li>For <code>\u03b5=0.2</code>: ratios are clipped to <code>[0.8, 1.2]</code> (20% max change)</li> </ol> <p>Why This Works: - When ratio \u2248 1: No clipping, standard policy gradient - When ratio &gt; 1+\u03b5: Clipped to prevent too much increase - When ratio &lt; 1-\u03b5: Clipped to prevent too much decrease - The minimum ensures we don't make changes that would hurt performance</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#separate-optimization-strategy","title":"Separate Optimization Strategy:","text":"<p>PPO uses separate optimizers for actor and critic networks: - Actor Loss: Policy improvement (main objective) - Critic Loss: Value function improvement (weighted) - Entropy Bonus: Encourage exploration (small penalty) - Total Loss: <code>actor_loss + 0.5 * critic_loss - 0.01 * entropy</code></p> <p>Why Separate Optimization? 1. Different Learning Rates: Actor and critic often need different learning rates 2. Different Objectives: Actor learns policy, critic learns value function 3. Stability: Prevents one network from interfering with the other 4. Control: Can apply different regularization to each network</p> <p>Value Function Clipping (Optional): - Standard MSE Loss: <code>F.mse_loss(values.squeeze(), returns)</code> - Clipped Values: <code>values_old + clip(values - values_old, -clip, clip)</code> - Clipped MSE Loss: <code>F.mse_loss(values_clipped.squeeze(), returns)</code> - Final Loss: <code>max(standard_loss, clipped_loss)</code> (opposite of policy clipping)</p> <p>Multiple Epochs for Sample Efficiency: - Run multiple epochs (default: 10) to make efficient use of collected experience - Each epoch: forward pass \u2192 compute losses \u2192 update networks - For CSTR: Practice the same temperature control decisions multiple times</p> <p>Gradient Clipping for Stability: - <code>torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)</code> - Prevents exploding gradients that could destabilize training - For CSTR: Prevent drastic changes to temperature control parameters</p> <p>Memory Management: - Clear gradients after each epoch to prevent memory leaks - <code>param.grad.zero_()</code> and <code>param.grad = None</code> - Ensures clean gradients for each epoch (standard PPO practice)</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#6-detailed-pytorch-actorcriticnet-explanation","title":"6. Detailed PyTorch ActorCriticNet Explanation","text":"<pre><code>class ActorCriticNet(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        # Actor network\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, 64), nn.Tanh(),\n            nn.Linear(64, 64), nn.Tanh(),\n        )\n        self.mean_head = nn.Linear(64, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim)) # trainable log std\n                # Critic network\n            self.critic = nn.Sequential(\n              nn.Linear(state_dim, 64), nn.Tanh(),\n                nn.Linear(64, 64), nn.Tanh(),\n              nn.Linear(64, 1)\n            )\n\ndef forward(self, state):\n    actor_features = self.actor(state)\n    action_mean = self.mean_head(actor_features)\n    action_std = self.log_std.exp()\n\n    state_value = self.critic(state)\n    return action_mean, action_std, state_value\n</code></pre>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#actor-network","title":"Actor Network","text":"<p>Structure:</p> <ul> <li>Linear \u2192 Tanh \u2192 Linear \u2192 Tanh</li> </ul> <p>Purpose:</p> <ul> <li>Predicts mean and standard deviation for actions.</li> <li>Allows the agent to balance exploitation (low std) and exploration (high std).</li> </ul> <p>Why this sequence?</p> <ul> <li>Linear layers (nn.Linear):</li> <li>They project inputs into feature spaces. A linear layer simply performs a weighted sum (matrix multiplication + bias).</li> <li>Two linear layers let the network learn more complex, nonlinear mappings.</li> <li>Non-linear activations (nn.Tanh):</li> <li>Add complexity to network outputs by introducing nonlinearities.</li> <li>Tanh activation is bounded between -1 and 1, keeping internal activations stable, which is very beneficial in reinforcement learning.</li> </ul> <p>Why 64 units?</p> <ul> <li>The number 64 is a practical choice, balancing computational cost and representational capacity. Typically, values like 32, 64, 128 are standard defaults in RL literature.</li> <li>You can easily tune this number: bigger = more expressive, smaller = simpler but less flexible.</li> </ul> <p>After the shared layers, the actor network predicts parameters for the action distribution. For continuous actions, we usually assume a Gaussian (normal) distribution:</p> <ul> <li>Gaussian has two parameters:</li> <li>Mean (mean_head)</li> <li>Standard deviation (log_std \u2192 exponentiated to std).</li> </ul> <pre><code>self.mean_head = nn.Linear(64, action_dim)\nself.log_std = nn.Parameter(torch.zeros(action_dim)) \n</code></pre> <p>What these mean theoretically:</p> <ul> <li>mean_head:</li> <li>Predicts the center (\u201caverage\u201d) of the distribution for the actions given the state.</li> <li>Think of this as \u201cthe actor\u2019s best guess for the optimal action.\u201d</li> <li>log_std:</li> <li>Standard deviation (std) describes the uncertainty or exploration level. A higher std means more exploration around the predicted mean.</li> <li>Why use log_std?<ul> <li>To ensure the standard deviation is always positive, we parameterize it as the exponential of log_std (std = exp(log_std)).</li> <li>This ensures the network learns more stably.</li> </ul> </li> </ul> <p>Analogy: Imagine you\u2019re throwing darts at a target:</p> <ul> <li>The mean_head is where you aim\u2014the center of your throw.</li> <li>The log_std (converted to standard deviation) describes how precise your aim is:</li> <li>Small std = very precise throws (little variation).</li> <li>Large std = lots of uncertainty, wider range of where darts may land.</li> </ul> <p>Initially, you start uncertain (high std), then gradually become more confident (low std).</p>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#critic-network","title":"Critic Network","text":"<p>Structure:</p> <ul> <li>Linear \u2192 Tanh \u2192 Linear \u2192 Tanh \u2192 Linear</li> </ul> <p>Purpose:</p> <ul> <li>Outputs a single value estimating state quality.</li> </ul> <p>Why this sequence?</p> <ul> <li>Same reasoning as the actor: linear layers plus nonlinear activations allow the critic to represent complex, nonlinear value functions.</li> <li>The critic network often has similar complexity as the actor to reliably estimate values.</li> </ul> <p>Why final layer has 1 neuron?</p> <ul> <li>The critic outputs a single number: the estimated value of the given state, V(s).</li> <li>This is the expected total reward starting from that state, following the current policy.</li> <li>This single scalar number represents the critic\u2019s evaluation of \u201chow good\u201d the current state is.</li> </ul> <p>Analogy: Imagine a real estate expert evaluating homes (states):</p> <ul> <li>After considering various home features (inputs: rooms, location, etc.), the expert provides a single dollar estimate of the home\u2019s worth.</li> <li>Similarly, the critic gives one numeric evaluation of the state\u2019s worth in terms of future rewards.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#forward-method","title":"Forward Method","text":"<p>The forward method processes inputs through both actor and critic:</p> <ul> <li>Actor pathway: Suggests actions.</li> <li>Critic pathway: Evaluates state quality.</li> </ul> <p>This method executes the full prediction process, clearly split into:</p> <ul> <li>Actor pathway:</li> <li>Input state passes through actor layers \u2192 features.</li> <li>Features produce mean and standard deviation for action selection.</li> <li>Critic pathway:</li> <li>Input state independently passes through critic layers.</li> <li>Critic produces the scalar value of the current state.</li> </ul> <p>Note on clamping:</p> <ul> <li>The line self.log_std.clamp(-20, 2) prevents extreme variance values, improving training stability.</li> <li>(exp(-20) ~ very close to zero std \u2192 precise; exp(2) ~ 7.4 \u2192 large std \u2192 exploratory.</li> </ul> <p>Analogy: Think of the forward method as a restaurant:</p> <ul> <li>Input (State): Ingredients arrive at your kitchen.</li> <li>Actor (Chef):</li> <li>Looks at ingredients, decides recipe (\u201cmean\u201d).</li> <li>Decides how much to improvise (\u201cstd\u201d). Sometimes chefs strictly follow recipes (low std); sometimes they experiment (high std).</li> <li>Critic (Food Critic):</li> <li>Separately evaluates these ingredients and predicts how delicious the meal could be (state value).</li> </ul> <p>At the end:</p> <ul> <li>Chef (Actor) serves a suggested dish with defined flexibility (mean, std).</li> <li>Food Critic (Critic) provides an independent evaluation of the ingredients\u2019 potential (state value).</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#7-pitfalls-implementation-tips","title":"7. Pitfalls &amp; Implementation Tips","text":"<p>Common pitfalls and prevention:</p> Pitfall How to Avoid Instability &amp; Exploding gradients Clip gradients (torch.nn.utils.clip_grad_norm_) High variance in rewards Normalize rewards, or use reward shaping Incorrect advantage calculation Carefully debug advantage calculation step-by-step Action distribution collapse Include sufficient entropy bonus Slow or no learning Adjust learning rates, clip parameter, or verify observations/actions normalization <p>Special tips:</p> <ul> <li>Normalize observations and advantages.</li> <li>Regularly monitor KL-divergence.</li> <li>Validate PPO setup on simpler environments first.</li> </ul>"},{"location":"guides/data_science_and_ai/handson_rl_application_ppo/#8-recommended-resources","title":"8. Recommended Resources","text":"<ul> <li>Original PPO Paper</li> <li>Spinning Up PPO Guide</li> <li>Stable-Baselines3 PPO Implementation</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/","title":"Common Techniques for K-Means Visualization","text":"<p>K-Means operates in high-dimensional space (e.g., 4 or more features), which makes direct visualization impossible. That\u2019s why we often use dimensionality reduction techniques to project these high-dimensional clusters into 2D or 3D space for plotting \u2014 making the clusters easier to interpret and present.</p> <p>Two widely used techniques for this are Principal Component Analysis (PCA) and t-SNE (t-distributed Stochastic Neighbor Embedding). Both reduce dimensions, but they serve different purposes and operate under different assumptions.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":""},{"location":"guides/data_science_and_ai/k-means_visualization/#general-mechanics","title":"General Mechanics","text":"<p>PCA is a linear technique that transforms the data into a new coordinate system where:</p> <ul> <li>The first axis (PC1) captures the most variance (spread) in the data.</li> <li>The second axis (PC2) captures the second-most variance, orthogonal to PC1.</li> </ul> <p>The transformation is based on eigenvectors and eigenvalues of the covariance matrix of the data. These new axes (principal components) are combinations of your original features.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-analogy","title":"Store Analogy","text":"<p>Imagine you're analyzing stores based on monthly purchase, store size, number of categories, and region/city code. These features form a 4D space. PCA finds the \"best possible plane\" that captures the most variation in how stores differ \u2014 like angling your camera to get the best view of a crowd.</p> <p>In practice:</p> <ul> <li>PCA might reveal that most of the variance is explained by purchase and size, so it compresses the 4D data into 2D using those axes.</li> <li>You can then plot the stores in 2D while still capturing 90\u201395% of the original structure.</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#best-use-case","title":"Best Use Case","text":"<ul> <li>When you want to preserve global structure</li> <li>When clusters are roughly linearly separable</li> <li>When interpretability of axes matters (e.g., \"PC1 is mostly purchase size\")</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#t-sne-t-distributed-stochastic-neighbor-embedding","title":"t-SNE (t-distributed Stochastic Neighbor Embedding)","text":""},{"location":"guides/data_science_and_ai/k-means_visualization/#general-mechanics_1","title":"General Mechanics","text":"<p>t-SNE is a non-linear technique designed to preserve local neighborhoods rather than global structure. It works by:</p> <ol> <li>Computing pairwise similarities between all points in high-dimensional space based on proximity.</li> <li>Mapping those similarities to a lower-dimensional space (e.g., 2D) using a Student-t distribution.</li> <li>Minimizing the mismatch (using KL divergence) between high-D and low-D similarity structures.</li> </ol>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-analogy_1","title":"Store Analogy","text":"<p>Let\u2019s say you have:</p> <ul> <li>Stores A, B, and C in big cities with high sales and full category coverage.</li> <li>Stores D and E in small towns with minimal sales and fewer products.</li> </ul> <p>In 4D space, these two groups are far apart. t-SNE unfolds the high-D space into 2D in such a way that:</p> <ul> <li>A\u2013B\u2013C appear as a tight group in 2D.</li> <li>D\u2013E appear as a separate, tight group.</li> <li>The exact distance between clusters might be distorted, but cluster shapes are preserved.</li> </ul> <p>It\u2019s like turning a crumpled map of a city into a flattened version that shows neighborhoods clearly, even if scale isn't perfect.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#best-use-case_1","title":"Best Use Case","text":"<ul> <li>When your goal is to visually highlight clusters</li> <li>When you're dealing with non-linear structures or overlapping distributions</li> <li>When you don't care about precise global distances but want tight grouping to show up clearly</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#summary-table","title":"Summary Table","text":"Method Type What It Preserves When to Use PCA Linear Global variance and structure You want interpretable axes, or to understand dominant trends t-SNE Non-linear Local relationships (clusters) You want clear visual separation of clusters for storytelling or inspection"},{"location":"guides/data_science_and_ai/k-means_visualization/#lets-deep-dive-into-the-marvelous-linear-algebra-behind-principal-component-analysis-pca","title":"Let's Deep Dive into the Marvelous Linear Algebra Behind Principal Component Analysis (PCA)","text":"<p>Principal Component Analysis (PCA) is a linear technique that transforms your high-dimensional data into a lower-dimensional space, ideally capturing most of its structure (variance) in fewer dimensions. Let\u2019s walk through the full PCA process using a numerical example with four features:</p> <ul> <li>Purchase (k$)</li> <li>Categories</li> <li>Size (m\u00b2)</li> <li>City_1 \u2192 one-hot encoded (1 = store is in City 1; 0 = otherwise)</li> </ul> <p>Our goal: reduce this 4D space into 2  or 1 dimensions using PCA.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-1-standardize-the-data","title":"Step 1: Standardize the Data","text":""},{"location":"guides/data_science_and_ai/k-means_visualization/#raw-data","title":"Raw Data","text":"Store Purchase Categories Size City_1 A 8.0 4 120 1 B 3.0 2 60 0 C 4.0 1 90 0"},{"location":"guides/data_science_and_ai/k-means_visualization/#standardize-each-feature","title":"Standardize Each Feature","text":"<p>We calculate the mean and standard deviation (std) for each feature:</p> Feature Mean Std Dev Purchase 5.0 2.65 Categories 2.33 1.53 Size 90.0 30.0 City_1 0.33 0.58 <p>Now apply: <pre><code>z = (x - \u03bc)/\u03c3\n</code></pre></p> Store Std Purchase Std Categories Std Size Std City_1 A \\(\\frac{8 - 5}{2.65} =\\) 1.13 \\(\\frac{4 - 2.33}{1.53} =\\) 1.09 \\(\\frac{120 - 90}{30} =\\) 1.00 \\(\\frac{1 - 0.33}{0.58} =\\) 1.15 B -0.75 -0.22 -1.00 -0.57 C -0.38 -0.87 0.00 -0.57 <p>Let\u2019s label the rows as vectors:</p> <ul> <li>x\u2081: Store A</li> <li>x\u2082: Store B</li> <li>x\u2083: Store C</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-2-create-the-covariance-matrix","title":"Step 2: Create the Covariance Matrix","text":"<p>The covariance matrix shows how features vary with respect to each other.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#mathematical-formula","title":"Mathematical Formula","text":"<p>Given a standardized matrix X, the covariance matrix is: <pre><code>\u03a3 = (1/(n - 1)) * X^T * X\n</code></pre> Where:</p> <ul> <li>X^T is the transpose of the standardized matrix</li> <li>n is the number of samples (3 stores here)</li> </ul> <p>Let\u2019s compute one full row \u2014 the covariance of Purchase with every feature.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-by-step-covariance-calculations","title":"Step-by-Step Covariance Calculations","text":"<p>We use: <pre><code>Cov[x, y] = (1/(n - 1)) * Sum[x\u1d62 * y\u1d62, {i}]\n</code></pre> Since data is standardized (mean = 0), we can use this simplified version.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#covpurchase-purchase","title":"Cov(Purchase, Purchase)","text":"<pre><code>= (1/2) * (1.13\u00b2 + (-0.75)\u00b2 + (-0.38)\u00b2) = 1.9838/2 = 0.992\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#covpurchase-categories","title":"Cov(Purchase, Categories)","text":"<pre><code>= (1/2) * (1.13*1.09 + (-0.75)*(-0.22) + (-0.38)*(-0.87))\n= (1/2) * (1.2317 + 0.165 + 0.3306) = 1.7273/2 = 0.864\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#covpurchase-size","title":"Cov(Purchase, Size)","text":"<pre><code>= (1/2) * (1.13*1.00 + (-0.75)*(-1.00) + (-0.38)*(0.00)) = 1.88/2 = 0.94\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#covpurchase-city_1","title":"Cov(Purchase, City_1)","text":"<pre><code>= (1/2) * (1.13*1.15 + (-0.75)*(-0.57) + (-0.38)*(-0.57))\n= (1/2) * (1.2995 + 0.4275 + 0.2166) = 1.9436/2 = 0.972\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#first-row-of-covariance-matrix","title":"First Row of Covariance Matrix","text":"<pre><code>[Purchase] = {0.992, 0.864, 0.940, 0.972}\n</code></pre> <p>You would repeat the same process to fill the other rows.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#full-covariance-matrix-symmetric-44-matrix","title":"Full Covariance Matrix (Symmetric 4\u00d74 Matrix)","text":"Purchase Categories Size City_1 Purchase 0.992 0.864 0.940 0.972 Categories 0.864 ... ... ... Size 0.940 ... ... ... City_1 0.972 ... ... ... <p>This matrix is the foundation for the next step: eigenvalue decomposition \u2014 which finds the principal components that define our new axes.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-3-find-eigenvalues-and-eigenvectors-principal-components","title":"Step 3: Find Eigenvalues and Eigenvectors (Principal Components)","text":"<p>In PCA, eigenvectors of the covariance matrix define the directions (axes) of maximum variance \u2014 these are called principal components. The corresponding eigenvalues tell us how much variance is explained in each direction.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#goal","title":"Goal","text":"<ul> <li>Eigenvectors \u2192 the new axes we project data onto</li> <li>Eigenvalues \u2192 how much spread/variance each axis captures</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#the-covariance-matrix-from-step-2","title":"The Covariance Matrix from Step 2","text":"<p>We previously calculated one full row; now here\u2019s the full symmetric matrix with values filled in (rounded):</p> Purchase Categories Size City_1 Purchase 0.992 0.864 0.940 0.972 Categories 0.864 0.794 0.883 0.872 Size 0.940 0.883 1.000 0.899 City_1 0.972 0.872 0.899 1.000 <p>Let\u2019s denote this matrix as \\(\\beta\\).</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-31-finding-eigenvalues","title":"Step 3.1: Finding Eigenvalues","text":""},{"location":"guides/data_science_and_ai/k-means_visualization/#characteristic-equation","title":"Characteristic Equation","text":"<p>To find eigenvalues \u03bb, solve: <pre><code>Det[\u03b2 - \u03bb * IdentityMatrix[4]] == 0\n</code></pre> Where:</p> <ul> <li>\u03b2 is the 4\u00d74 covariance matrix</li> <li>I is the 4\u00d74 identity matrix</li> <li>\u03bb are the scalar eigenvalues</li> </ul> <p>This results in a 4th-degree polynomial, which is best solved numerically (e.g., using NumPy or symbolic solvers). But we can illustrate this on a smaller scale and show how it's computed.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#in-python-for-example","title":"In Python (for example):","text":"<pre><code>import numpy as np\n\ncov_matrix = np.array([\n    [0.992, 0.864, 0.940, 0.972],\n    [0.864, 0.794, 0.883, 0.872],\n    [0.940, 0.883, 1.000, 0.899],\n    [0.972, 0.872, 0.899, 1.000]\n])\n\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#sample-output-approximate","title":"Sample Output (approximate):","text":"<ul> <li>Eigenvalues:   <pre><code>\u03bb\u2081 \u2248 3.76, \u03bb\u2082 \u2248 0.02, \u03bb\u2083 \u2248 0.002, \u03bb\u2084 \u2248 0.0004\n</code></pre></li> </ul> <p>These tell us how much variance each principal component captures.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#variance-explained","title":"Variance Explained:","text":"<p>To find the proportion of variance explained by each component: <pre><code>ExplainedRatio\u1d62 = \u03bb\u1d62 / Sum[\u03bb]\n</code></pre> In this example:</p> <ul> <li>PC1 explains 3.76/(3.76 + 0.02 + 0.002 + 0.0004) \u2248 99.4%</li> <li>Remaining PCs together explain less than 1%</li> </ul> <p>That\u2019s why we can safely reduce from 4D to 2D, or even 1D.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-32-finding-eigenvectors","title":"Step 3.2: Finding Eigenvectors","text":"<p>For each eigenvalue \u03bb, solve: <pre><code>(\u03a3 - \u03bb * I) * v == 0\n</code></pre> Where:</p> <ul> <li>v is the eigenvector associated with \u03bb</li> <li>This is a system of linear equations</li> </ul> <p>We solve this using linear algebra techniques (e.g., Gaussian elimination), but again, it's most practical with software.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#example-eigenvector-for-1-376","title":"Example (Eigenvector for \u03bb\u2081 \u2248 3.76):","text":"<p>Let\u2019s say the result is: <pre><code>v\u2081 = {0.51, 0.50, 0.51, 0.48}\n</code></pre></p> <p>This vector tells us how to combine the original features to form the first principal component (PC1).</p> <p>So: <pre><code>PC1 = 0.51 * Purchase + 0.50 * Categories + 0.51 * Size + 0.48 * City_1\n</code></pre> This direction captures almost all of the variance \u2014 it's your \"best lens\" for viewing the data.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#what-do-the-other-eigenvectors-mean","title":"What Do the Other Eigenvectors Mean?","text":"<p>Each eigenvector represents a new axis in our transformed feature space \u2014 a direction along which your data varies. In PCA:</p> <ul> <li>The first eigenvector (PC1) captures the direction of maximum variance.</li> <li>The second eigenvector (PC2) captures the next most variance, orthogonal (at a right angle) to PC1.</li> <li>The third (PC3) and fourth (PC4) follow in decreasing order of variance captured, and are also orthogonal to all previous ones.</li> </ul> <p>This orthogonality ensures the new axes are uncorrelated, which is a key reason PCA works so well. In our case:</p> <ul> <li>\u03bb\u2081 \u2248 3.76 \u2192 PC1</li> <li>\u03bb\u2082 \u2248 0.02 \u2192 PC2</li> <li>\u03bb\u2083 \u2248 0.002 \u2192 PC3</li> <li>\u03bb\u2084 \u2248 0.0004 \u2192 PC4</li> </ul> <p>Each eigenvalue tells us how important its corresponding component is. Let\u2019s interpret each:</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#pc1-v1-dominant-direction","title":"PC1 (v\u2081) \u2013 Dominant Direction","text":"<p>Example: <pre><code>v\u2081 = {0.51, 0.50, 0.51, 0.48}\n</code></pre> This tells us that most of the variability in the store dataset comes from a roughly equal mix of purchase, category, and size, and even city membership (City_1) contributes significantly. This component might reflect a general \"store scale\" or \"activity\" metric.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#pc2-v2-subtle-orthogonal-contrast","title":"PC2 (v\u2082) \u2013 Subtle Orthogonal Contrast","text":"<p>Example: <pre><code>v\u2082 = {0.60, -0.70, 0.30, -0.10}\n</code></pre> This component captures the next strongest contrast, but it's orthogonal to PC1 \u2014 it shows patterns not explained by PC1.</p> <p>Interpretation:</p> <ul> <li>High positive loading on Purchase</li> <li>Strong negative on Categories</li> </ul> <p>This might represent stores that buy a lot but in fewer categories, versus those that spread purchases across more categories. Think of this as separating focused high-volume buyers from broad but lower-volume stores.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#pc3-v3-weak-niche-variation","title":"PC3 (v\u2083) \u2013 Weak, Niche Variation","text":"<p>Example: <pre><code>v\u2083 = {-0.40, -0.30, 0.20, 0.85}\n</code></pre> PC3 captures a very small amount of variance. It may highlight subtle behavior related to geography (City_1) \u2014 perhaps some city-specific trend not present in the other components.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#pc4-v4-noise-or-redundant-dimension","title":"PC4 (v\u2084) \u2013 Noise or Redundant Dimension","text":"<p>Example: <pre><code>v\u2084 = {0.01, 0.02, -0.80, 0.59}\n</code></pre> This might reflect tiny fluctuations or noise that are not meaningful for clustering or interpretation. Components like this are typically discarded when reducing dimensionality.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#why-only-keep-pc1-and-pc2","title":"Why Only Keep PC1 and PC2?","text":"<p>In our case:</p> <ul> <li>PC1 explains ~99.4% of variance</li> <li>PC2 adds ~0.5%</li> <li>PC3 and PC4 contribute almost nothing</li> </ul> <p>Keeping PC1 + PC2 preserves ~99.9% of the structure \u2014 and simplifies your data from 4D to 2D with almost no information loss.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#summary-of-what-we-found","title":"Summary of What We Found","text":"Step Result Eigenvalues Tell us how much variance each principal component captures Eigenvectors Tell us the directions (feature combinations) of each component Top PCs Let us reduce data to fewer dimensions while preserving its structure"},{"location":"guides/data_science_and_ai/k-means_visualization/#step-5-project-data","title":"Step 5: Project Data","text":""},{"location":"guides/data_science_and_ai/k-means_visualization/#the-goal","title":"The Goal","text":"<p>We now want to transform the original 4D standardized data into a new space defined by:</p> <ul> <li>1 component (PC1): one axis, max variance</li> <li>2 components (PC1 + PC2): plane capturing almost all structure</li> </ul> <p>This allows us to visualize or cluster data with minimal loss of information.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#projection-formula","title":"Projection Formula","text":"<p>Let:</p> <ul> <li>\\(X\\) be your standardized data matrix (3 samples \u00d7 4 features)</li> <li>\\(W\\) be the matrix of eigenvectors (4 features \u00d7 k components)</li> </ul> <p>Then the projected data is: <pre><code>Z = X * W\n</code></pre> Where:</p> <ul> <li>Z = transformed data (3 samples \u00d7 k components)</li> <li>Each row in Z represents a store in the new PC space</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-data-standardized","title":"Store Data (Standardized)","text":"Store Purchase Categories Size City_1 A 1.13 1.09 1.00 1.15 B -0.75 -0.22 -1.00 -0.57 C -0.38 -0.87 0.00 -0.57"},{"location":"guides/data_science_and_ai/k-means_visualization/#scenario-1-project-into-1d-pc1-only","title":"Scenario 1: Project into 1D (PC1 only)","text":"<p>Let\u2019s say the first eigenvector (PC1) is: <pre><code>v\u2081 = {0.51, 0.50, 0.51, 0.48}\n</code></pre> Then for each store, we compute: <pre><code>z = x * v\u2081 = dot product of row vector and PC1\n</code></pre></p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-a","title":"Store A:","text":"<pre><code>z = 1.13*0.51 + 1.09*0.50 + 1.00*0.51 + 1.15*0.48\n= 0.5763 + 0.545 + 0.51 + 0.552 \u2248 2.18\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-b","title":"Store B:","text":"<pre><code>z = -0.75*0.51 + (-0.22)*0.50 + (-1.00)*0.51 + (-0.57)*0.48\n= -0.3825 - 0.11 - 0.51 - 0.2736 \u2248 -1.28\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-c","title":"Store C:","text":"<pre><code>z = -0.38*0.51 + (-0.87)*0.50 + 0.00*0.51 + (-0.57)*0.48\n= -0.1938 - 0.435 + 0 - 0.2736 \u2248 -0.90\n</code></pre>"},{"location":"guides/data_science_and_ai/k-means_visualization/#interpretation-1d-projection-pc1-only","title":"Interpretation: 1D Projection (PC1 only)","text":"Store PC1 Value A 2.18 B -1.28 C -0.90 <ul> <li>PC1 acts like a composite index measuring \"overall store activity\"</li> <li>Store A stands out as high-performing</li> <li>Stores B and C are low performers, with B being the furthest</li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#scenario-2-project-into-2d-pc1-pc2","title":"Scenario 2: Project into 2D (PC1 + PC2)","text":"<p>Let\u2019s assume:</p> <ul> <li>v\u2081 = {0.51, 0.50, 0.51, 0.48}</li> <li>v\u2082 = {0.60, -0.70, 0.30, -0.10}</li> </ul> <p>Now we calculate two projections per store: one onto PC1 and one onto PC2.</p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-a_1","title":"Store A:","text":"<ul> <li> <p>PC1: as before \u2192 2.18</p> </li> <li> <p>PC2:   <pre><code>1.13*0.60 + 1.09*(-0.70) + 1.00*0.30 + 1.15*(-0.10)\n= 0.678 - 0.763 + 0.3 - 0.115 \u2248 0.10\n</code></pre></p> </li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-b_1","title":"Store B:","text":"<ul> <li> <p>PC1: -1.28</p> </li> <li> <p>PC2:   <pre><code>-0.75*0.60 + (-0.22)*(-0.70) + (-1.00)*0.30 + (-0.57)*(-0.10)\n= -0.45 + 0.154 - 0.30 + 0.057 \u2248 -0.54\n</code></pre></p> </li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#store-c_1","title":"Store C:","text":"<ul> <li> <p>PC1: -0.90</p> </li> <li> <p>PC2:   <pre><code>-0.38*0.60 + (-0.87)*(-0.70) + 0.00*0.30 + (-0.57)*(-0.10)\n= -0.228 + 0.609 + 0 + 0.057 \u2248 0.44\n</code></pre></p> </li> </ul>"},{"location":"guides/data_science_and_ai/k-means_visualization/#interpretation-2d-projection-pc1-pc2","title":"Interpretation: 2D Projection (PC1 + PC2)","text":"Store PC1 PC2 A 2.18 0.10 B -1.28 -0.54 C -0.90 0.44 <p>Plotting this in 2D shows clusters or spread more clearly:</p> <ul> <li>PC1 separates stores by general performance.</li> <li>PC2 might separate stores that buy more in fewer categories (B) vs. diverse but smaller buyers (C).</li> </ul> <p></p>"},{"location":"guides/data_science_and_ai/k-means_visualization/#summary","title":"Summary","text":"Projection Use Case Insight 4D \u2192 1D (PC1) Create a composite score or ranking Simple comparison of store intensity 4D \u2192 2D (PC1 + PC2) Visualize cluster patterns Richer analysis and exploratory clustering"},{"location":"guides/data_science_and_ai/overview/","title":"My Journey into Data Science &amp; AI","text":""},{"location":"guides/data_science_and_ai/overview/#from-curiosity-to-real-world-impact","title":"From Curiosity to Real-World Impact","text":"<p>I\u2019ve always been fascinated by how data can drive decisions, automate tasks, and even mimic human intelligence. What started as a curiosity led me to dive deep into machine learning, deep learning, and AI applications.  </p> <p>In this section, I share key data science concepts alongside practical pocket projects, showcasing how I apply these techniques in real-world scenarios.  </p>"},{"location":"guides/data_science_and_ai/overview/#topics-covered","title":"Topics Covered","text":"<p>Reinforcement Learning \u2013 Teaching models through rewards and decision-making.</p> <p>Hands-on RL application: PPO for Chemical Process Control - This project illustrates the core concepts of RL through the implementation of a PPO agent managing an industrial Continuous Stirred Tank Reactor (CSTR).</p> <p>The Neural Network Structure \u2013 Learn the core fundamentals of the Neural Network by using a visual analogy: Neura\u2019s Painting Studio</p> <p>Clustering algorithms \u2013 Unsupervised learning techniques used to automatically group data points based on similarity. For example, K-Means and its visualization techniques like PCA</p> <p>Data Science Products as Python Packages \u2013 Turning data science projects into Python packages to enhance reusability, collaboration, and deployment. </p> <p>Deep Learning &amp; Autoencoders \u2013 Exploring neural networks for pattern recognition.  </p> <p>Each topic connects theory to hands-on projects, showing how AI is shaping the future - and how we, as Data Scientist and Developers, can contribute to it. </p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/","title":"Reinforcement Learning (RL)","text":"<p>Reinforcement Learning is a branch of Machine Learning where an agent learns by interacting with an environment to achieve a goal. It\u2019s less about feeding the model labeled data (as in supervised learning) and more about trial, error, and feedback.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#rl-vs-traditional-machine-learning","title":"RL vs Traditional Machine Learning","text":"<p>There are two key differences between Reinforcement Learning (RL) and typical Machine Learning (ML):</p> <ol> <li>Agent's Action     In RL, the agent\u2019s actions affect the data it receives next from the environment. In contrast, ML usually assumes the data is static and independent of it's outputs (predictions).</li> <li>Reward Signal     RL uses a scalar reward to guide learning-  what actions lead to good outcomes through trial and error.</li> </ol>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#rewards","title":"Rewards","text":"<p>A reward (denoted as <code>R\u209c</code>) is a scalar signal received after taking an action. It tells the agent how well it\u2019s doing at time step t.</p> <p>The goal of the agent is to maximize cumulative reward over time, not just perform well in the moment.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#the-reward-hypothesis","title":"The Reward Hypothesis","text":"<p>RL is built on the Reward Hypothesis:</p> <p>\u201cAll goals can be described as the maximization of expected cumulative rewards.\u201d</p> <p>Example (Power Plant Control):</p> <ul> <li>+1 for producing more power efficiently</li> <li>-10 for exceeding safety thresholds</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#agents-actions","title":"Agent's Actions","text":"<p>The agent must choose a sequence of actions to maximize future rewards.</p> <p>Caveats:</p> <ul> <li>Immediate rewards might not be the optimal.</li> <li>Sometimes, sacrificing short-term gain leads to better long-term outcomes.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#history-vs-state","title":"History vs. State","text":"<p>One of the most common sources of confusion in reinforcement learning is the difference between history and state.</p> <p>It might seem like the state should just be \u201ceverything that happened before\u201d (i.e. the history), but in practice, state is a concise summary of that history\u2014not the entire thing.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#what-is-the-history","title":"What is the History?","text":"<p>The history at time step <code>t</code> is the full record of everything the agent has experienced up to that point:</p> <pre><code>H\u209c = {O\u2081, A\u2081, R\u2081, O\u2082, A\u2082, R\u2082, ..., O\u209c, A\u209c, R\u209c}\n</code></pre> <p>This contains:</p> <ul> <li>All past observations (<code>O</code>)</li> <li>All taken actions (<code>A</code>)</li> <li>All received rewards (<code>R</code>)</li> </ul> <p>It\u2019s incredibly detailed\u2014but often too bulky or unnecessary for real-time decision-making.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#what-is-the-state","title":"What is the State?","text":"<p>A state <code>S\u209c</code> is a compressed representation of the history that contains all the information the agent needs to decide what to do next.</p> <pre><code>S\u209c = f(H\u209c)\n</code></pre> <p>This function <code>f</code> summarizes the useful parts of history and discards the rest.</p> <p>A good state captures just enough about the past to make optimal decisions\u2014nothing more, nothing less.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#why-this-matters","title":"Why This Matters","text":"<ul> <li>If you mistakenly treat the history as the state, your system becomes unnecessarily large and computationally expensive.</li> <li>Worse, the agent may get distracted by irrelevant past details\u2014leading to poor learning performance.</li> </ul> <p>This brings us to the Markov property, which tells us how to define a state that is truly useful for decision-making.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#the-markov-property","title":"The Markov Property","text":"<p>A state is Markov if it captures everything needed to determine the next step:</p> <pre><code>P[S\u209c\u208a\u2081 | S\u209c] = P[S\u209c\u208a\u2081 | S\u2081, S\u2082, ..., S\u209c]\n</code></pre> <p>\u201cGiven the present, the future is independent of the past.\u201d</p> <p>This means:</p> <ul> <li>The agent doesn\u2019t need full history.</li> <li>A well-designed state simplifies learning and planning.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#example-rat-and-reward-puzzle","title":"Example: Rat and Reward Puzzle","text":"<p>Let\u2019s say a rat agent presses levers and hears bells and sees lights before receiving cheese (reward). Depending on how we define the state:</p> State Design What the Agent Sees Pros / Cons Last 3 items in sequence \"Bell \u2192 Lever \u2192 Light\" Small input, fast decisions, might miss context Count of bells/lights/levers so far \"3 bells, 4 lights, 5 levers\" Captures trends, not exact order Full sequence (entire history) \"Bell, Bell, Light, Lever, Bell, Lever, Light...\" Accurate but computationally heavy <p>Insight: How you define the state greatly affects learning efficiency.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#the-one-step-agentenvironment-loop-based-on-sutton-barto","title":"The One-Step Agent\u2013Environment Loop (based on Sutton &amp; Barto)","text":"<p>At the heart of every Reinforcement Learning system lies a continuous interaction loop between the agent and the environment, formalized as a Markov Decision Process (MDP).</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#sequence-of-interactions","title":"Sequence of Interactions:","text":"<p>At each discrete time step <code>t</code>, the following happens:</p> <ol> <li>The agent observes the current state <code>S\u209c</code>.</li> <li>Based on <code>S\u209c</code>, the agent chooses and executes an action <code>A\u209c</code>.</li> <li>The environment receives the action <code>A\u209c</code>, responds with:</li> <li>A new state <code>S\u209c\u208a\u2081</code></li> <li>A reward <code>R\u209c\u208a\u2081</code></li> <li>The agent receives <code>R\u209c\u208a\u2081</code> and <code>S\u209c\u208a\u2081</code>, then repeats the cycle at <code>t+1</code>.</li> </ol> <p>This interaction yields a trajectory like:</p> <pre><code>S\u2080, A\u2080, R\u2081, S\u2081, A\u2081, R\u2082, S\u2082, A\u2082, ...\n</code></pre> <p>This diagram, from Sutton &amp; Barto\u2019s book, formalizes the MDP loop:</p> <ul> <li>The agent produces an action <code>A\u209c</code>.</li> <li>The environment receives <code>A\u209c</code>, and in response:</li> <li>Emits the next state <code>S\u209c\u208a\u2081</code></li> <li>Emits a reward <code>R\u209c\u208a\u2081</code></li> </ul> <p></p> <p>Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed., in progress). http://incompleteideas.net/book/the-book-2nd.html</p> <p>Each new state influences future actions, forming a feedback loop. Importantly, this formulation obeys the Markov property\u2014meaning future outcomes depend only on the current state and action, not on the full history.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#major-components-of-a-reinforcement-learning-agent","title":"Major Components of a Reinforcement Learning Agent","text":"<p>An RL agent can be thought of as a system that makes decisions by learning from interaction. It may be composed of three core components:</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#1-policy-what-should-i-do","title":"1. Policy \u2013 \u201cWhat should I do?\u201d","text":"<p>A policy defines the agent\u2019s behavior: how it chooses actions based on the current state.</p> <ul> <li>Deterministic Policy: always chooses the same action for a given state.</li> </ul> <pre><code>a = \u03c0(s)\n</code></pre> <ul> <li>Stochastic Policy: chooses actions according to a probability distribution.</li> </ul> <pre><code>\u03c0(a|s) = P[ A\u209c = a | S\u209c = s ]\n</code></pre> <p>Used when exploration is important or the environment is noisy.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#2-value-function-how-good-is-this-state","title":"2. Value Function \u2013 \u201cHow good is this state?\u201d","text":"<p>A value function estimates how much future reward the agent can expect from a given state (or state-action pair).</p> <ul> <li>Helps the agent choose better states over time.</li> <li>Doesn\u2019t directly pick actions, but it influences decisions when paired with a policy.</li> </ul> <pre><code>V(s) = expected cumulative reward from state s onward\n</code></pre>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#3-model-what-will-happen-next","title":"3. Model \u2013 \u201cWhat will happen next?\u201d","text":"<p>A model is the agent\u2019s internal understanding of how the environment behaves.</p> <ul> <li>It predicts:</li> <li>Next state: <code>S\u209c\u208a\u2081</code></li> <li>Reward: <code>R\u209c\u208a\u2081</code></li> <li>Allows the agent to simulate outcomes before acting.</li> <li>Not all RL agents use a model.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#categorizing-rl-agents","title":"Categorizing RL Agents","text":"<p>Let\u2019s now organize RL agents by how they combine the components above, and clarify pros, cons, and examples for each category.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#1-value-based-agents","title":"1. Value-Based Agents","text":"<ul> <li>Uses: Value Function</li> <li>Doesn\u2019t explicitly use a Policy (it\u2019s implicit\u2014derived from the value function)</li> <li>Doesn\u2019t use a Model</li> </ul> <p>Example:</p> <ul> <li>Q-Learning estimates the value of state-action pairs (<code>Q(s,a)</code>) and acts greedily.</li> <li>SARSA (State-Action-Reward-State-Action): Like Q-learning, but updates values using the actual next action taken.</li> </ul> <p>Pros:</p> <ul> <li>Simple and widely used.</li> <li>Good for discrete action spaces.</li> </ul> <p>Cons:</p> <ul> <li>Struggles with continuous or high-dimensional actions.</li> <li>Doesn\u2019t directly represent policy (requires argmax tricks).</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#2-policy-based-agents","title":"2. Policy-Based Agents","text":"<ul> <li>Uses: Policy</li> <li>Doesn\u2019t use a Value Function</li> <li>Doesn\u2019t use a Model</li> </ul> <p>Example: </p> <ul> <li>REINFORCE: A basic policy gradient method that updates the policy to maximize expected reward.</li> </ul> <p>Pros:</p> <ul> <li>Naturally handles continuous action spaces.</li> <li>Can learn stochastic or deterministic policies.</li> </ul> <p>Cons:</p> <ul> <li>Higher variance in updates.</li> <li>Often less sample-efficient than value-based methods.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#3-actor-critic-agents","title":"3. Actor-Critic Agents","text":"<ul> <li>Uses: Policy</li> <li>Uses: Value Function</li> <li>Doesn\u2019t use a Model</li> </ul> <p>Example:</p> <ul> <li> <p>PPO \u2013 Proximal Policy Optimization: Balances learning progress and stability using a clipped objective.</p> </li> <li> <p>A2C \u2013 Advantage Actor-Critic: Computes advantage estimates to reduce variance and improve stability.</p> </li> <li> <p>A3C \u2013 Asynchronous Advantage Actor-Critic: Runs multiple agents in parallel with independent environments.</p> </li> <li> <p>DDPG \u2013 Deep Deterministic Policy Gradient: For continuous action spaces. Actor-critic with deterministic policies.</p> </li> <li> <p>SAC \u2013 Soft Actor-Critic: Adds entropy regularization to encourage exploration.</p> </li> </ul> <p>Pros:</p> <ul> <li>Combines low variance from value-based with direct optimization of policy.</li> <li>Very popular for complex environments.</li> </ul> <p>Cons:</p> <ul> <li>More complex architecture.</li> <li>Balancing value and policy updates can be tricky.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#4-model-free-agents","title":"4. Model-Free Agents","text":"<ul> <li>Uses: Policy and/or Value Function</li> <li>Doesn\u2019t use a Model</li> </ul> <p>Examples:</p> <ul> <li>Q-learning, PPO, DQN, A3C, REINFORCE</li> </ul> <p>Pros:</p> <ul> <li>Easier to implement and train.</li> <li>No need to learn or assume environment dynamics.</li> </ul> <p>Cons:</p> <ul> <li>Can be sample inefficient\u2014needs lots of interactions.</li> <li>Less suitable for planning or simulations.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#5-model-based-agents","title":"5. Model-Based Agents","text":"<ul> <li>Uses: Model</li> <li>May also use: Policy and/or Value Function</li> </ul> <p>Pros:</p> <ul> <li>Sample efficient\u2014can plan and simulate.</li> <li>Useful when real-world data is costly.</li> </ul> <p>Cons:</p> <ul> <li>Requires learning or designing an accurate model.</li> <li>Model errors can lead to bad decisions (model bias).</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#two-main-approaches-to-develop-reinforcement-learning-agents","title":"Two Main Approaches to Develop Reinforcement Learning Agents","text":"<p>In reinforcement learning, agents can learn through two broad approaches, depending on how much they know about the environment at the start:</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#1-planning-model-based-learning","title":"1. Planning (Model-Based Learning)","text":"<p>The agent has a model of the environment.</p> <ul> <li>In this approach, the agent knows in advance how the environment works (i.e., it has a model that describes the dynamics: what happens when it takes an action).</li> <li>Instead of learning through direct interaction, the agent simulates outcomes internally using the model.</li> </ul> <pre><code>State \u2192 Model \u2192 Simulated Next State &amp; Reward \u2192 Policy Update\n</code></pre> <p>Great for low-risk, fast iteration Not applicable if the environment is unknown or too complex to model accurately</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#2-reinforcement-learning-model-free-learning","title":"2. Reinforcement Learning (Model-Free Learning)","text":"<p>The agent learns everything through direct interaction with the environment.</p> <ul> <li>The agent doesn't know the rules of the world it lives in.</li> <li>It must explore, collect experience, and learn from trial and error.</li> <li>This is the most common setup in real-world problems where dynamics are unknown or too complex to define upfront.</li> </ul> <pre><code>State \u2192 Real Action \u2192 Observation &amp; Reward \u2192 Policy Update\n</code></pre> <p>More flexible and general Typically requires a lot more data (sample inefficient)</p> <p>Example 1: Learning to Play Tic-Tac-Toe</p> Planning Approach Reinforcement Learning Approach The agent is given all game rules, and uses search algorithms (like Minimax) to simulate future moves and choose the best one. The agent plays thousands of games, learns from wins/losses, and gradually discovers winning strategies via trial and error. Efficient learning with a known model No prior knowledge required <p>Example 2: A Robot Learning to Navigate a Warehouse</p> Planning Approach Reinforcement Learning Approach The robot is given a map of the warehouse, and simulates paths to find the most efficient one using A* or Dijkstra\u2019s algorithm. The robot starts without a map, explores randomly, and learns optimal paths based on feedback (e.g., delivery success, collision penalties). Works well in static and known settings Adapts to real-world changes like obstacles or delays"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>A core challenge in RL is choosing between:</p> <ul> <li>Exploitation: Use what you already know to get high reward now</li> <li>Exploration: Try new actions to potentially discover better rewards later</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#balance-is-key","title":"Balance is key","text":"<p>If the agent only exploits, it may miss better strategies.  If it only explores, it may never get good at anything.</p> <p>Real-World Analogies:</p> Example Exploitation Exploration Restaurant Choice Go to your favorite Italian place Try a new sushi bar Online Ads Show top-performing ad Show a new variation Oil Drilling Drill where oil was found before Explore untested land Game Playing Play the best-known chess move Try an unconventional strategy <p>These examples show how short-term gain vs. long-term learning is a universal tension.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#prediction-vs-control-problems","title":"Prediction vs. Control Problems","text":"<p>Reinforcement Learning problems fall into two broad types:</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#1-prediction-problem","title":"1. Prediction Problem:","text":"<p>Evaluate how good a policy is.</p> <p>Task:</p> <p>Given a fixed policy <code>\u03c0</code>, estimate the value function:</p> <pre><code>V(s) = Expected future reward when starting from state s and following \u03c0\n</code></pre> <p>Example Use Case:</p> <ul> <li>You\u2019ve built a robot's policy and want to estimate how good it is.</li> <li>You use methods like:</li> <li>Monte Carlo Estimation</li> <li>Temporal Difference (TD) Learning</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#2-control-problem","title":"2. Control Problem:","text":"<p>Find the best policy</p> <p>Task:</p> <p>Optimize the policy <code>\u03c0</code> to maximize reward.</p> <p>This is harder because:</p> <ul> <li>You\u2019re both evaluating and improving the policy.</li> <li>The agent must explore, learn value estimates, and update the policy iteratively.</li> </ul> <p>Example Algorithms:</p> Method Solves Approach Q-learning Control Value-based: improve policy via Q-values SARSA Control Value-based with on-policy updates REINFORCE Control Policy-based: improve directly via gradients PPO Control Actor-critic: uses both policy + value function Dyna-Q Control (with model) Uses planning + learning"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#dynamic-programming-dp-principles-and-rl","title":"Dynamic Programming (DP) Principles and RL","text":"<p>Dynamic Programming (DP) is a powerful method for solving sequential decision problems. It forms the theoretical backbone of many reinforcement learning algorithms.</p> <p>DP is applicable when the problem has two key properties:</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#1-optimal-substructure","title":"1. Optimal Substructure","text":"<p>The solution to the overall problem can be built by combining solutions to its subproblems.</p> <p>This is the classic \"divide and conquer\" idea.</p> <p>Example 1: Shortest Path in a Maze</p> <ul> <li>To find the shortest path from A to C, you can split the problem:</li> <li>First solve from A to B</li> <li>Then from B to C</li> <li>Combine the results to find the total shortest path from A to C.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#2-overlapping-subproblems","title":"2. Overlapping Subproblems","text":"<p>Subproblems recur often, so instead of solving them over and over, we cache (store) their solutions for reuse.</p> <p>Example: Robot Vacuum Cleaning a House</p> <p>Imagine a robot vacuum learning the shortest cleaning path through a house.</p> <ul> <li>It must figure out how to go from Room A \u2192 Room D.</li> <li>But to do that, it repeatedly needs to solve:</li> <li>What's the best way from Room A \u2192 Room B?</li> <li>And from Room B \u2192 Room C?</li> <li>And so on\u2026</li> </ul> <p>Since these room-to-room paths are shared across many larger routes, the same subpaths appear over and over.</p> <p>Once the robot has figured out how to go from Room A to B efficiently, it can store that result and reuse it whenever any larger route requires it.</p> <p>This makes the learning process faster and more efficient, just like memoization in classic dynamic programming.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#why-dp-is-relevant-to-rl","title":"Why DP is Relevant to RL?","text":"<p>Markov Decision Processes (MDPs) - the foundation of RL - satisfy both:</p> <ul> <li>Optimal Substructure </li> <li>Overlapping Subproblems </li> </ul> <p>Thus, we can use DP to solve RL problems (when the model is known).</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#bellman-equations-the-heart-of-rl","title":"Bellman Equations \u2013 The Heart of RL","text":"<p>These equations give RL its recursive structure. They define how value functions can be broken down into immediate reward plus future value.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#bellman-expectation-equation-for-a-given-policy","title":"Bellman Expectation Equation (for a given policy \u03c0)","text":"<pre><code>V\u03c0(s) = E[ R\u209c\u208a\u2081 + \u03b3 * V\u03c0(S\u209c\u208a\u2081) | S\u209c = s ]\n</code></pre> <p>Interpretation:</p> <p>The value of being in state <code>s</code> (under policy \u03c0) = the expected reward right now plus the discounted value of where you\u2019ll land next, if you keep following \u03c0.</p> <p>It answers:</p> <p>\u201cIf I follow this policy, how much reward can I expect to accumulate starting from this state?\u201d</p> <p>Real-World Analogy:</p> <p>You're using Google Maps to follow a predefined route to a destination.</p> <ul> <li><code>R\u209c\u208a\u2081</code> = how enjoyable or efficient your next road segment is (traffic, views, fuel efficiency)</li> <li><code>V\u03c0(S\u209c\u208a\u2081)</code> = how promising the rest of the trip looks from the next junction</li> <li><code>\u03b3</code> = how far you care about future road conditions (e.g., close trip vs. cross-country)</li> </ul> <p>So the value of your current location = how good the next step is + how good things look after that.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#bellman-optimality-equation","title":"Bellman Optimality Equation","text":"<pre><code>V*(s) = max\u2090 E[ R\u209c\u208a\u2081 + \u03b3 * V*(S\u209c\u208a\u2081) | S\u209c = s, A\u209c = a ]\n</code></pre> <p>Interpretation:</p> <p>The optimal value of a state is the maximum expected return achievable by choosing the best action now, assuming we act optimally from that point on.</p> <p>It tells us how to compute V*(s) by evaluating all possible actions and choosing the best.</p> <p>Real-World Analogy:</p> <p>You're now choosing your own route on Google Maps, not following a fixed one.</p> <ul> <li>You consider all exits from the roundabout.</li> <li>For each one, you calculate:</li> <li>\u201cWhat\u2019s the reward from taking this exit?\u201d</li> <li>\u201cHow good are things from there on?\u201d</li> </ul> <p>Then, you choose the best path.</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#summary","title":"Summary","text":"Equation Purpose Intuition Bellman Expectation Evaluate a given policy \u03c0 \u201cWhat happens if I follow the instructions?\u201d Bellman Optimality Find the best policy \u201cWhat\u2019s the smartest action I can take now?\u201d"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#planning-approach-by-using-dynamic-programming","title":"Planning approach by using Dynamic Programming","text":"<p>Because DP assumes full knowledge of the MDP  - i.e., transition probabilities <code>P</code>, rewards <code>R</code>, and state/action sets - it\u2019s used primarily for planning (Model-Based Learning).</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#dp-is-used-for","title":"DP is used for:","text":""},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#prediction-policy-evaluation","title":"Prediction (Policy Evaluation):","text":"<ul> <li>Input: MDP \u27e8S, A, P, R, \u03b3\u27e9 and policy \u03c0</li> <li>Output: Value function <code>V\u03c0(s)</code></li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#control-policy-optimization","title":"Control (Policy Optimization):","text":"<ul> <li>Input: MDP \u27e8S, A, P, R, \u03b3\u27e9</li> <li>Output: Optimal value function <code>V*</code>, and optimal policy <code>\u03c0*</code></li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#which-rl-agent-categories-use-dp","title":"Which RL Agent Categories Use DP?","text":"<p>Here\u2019s how DP relates to the RL agent categories you explored earlier:</p> Agent Type Uses DP? How? Value-Based Yes Uses Bellman backups to compute value functions (e.g., via Q-learning) Policy-Based No Optimizes policies directly without relying on value recursion Actor-Critic Partially Critic uses value estimates based on Bellman updates (TD, advantage) Model-Free Indirectly Value updates follow Bellman logic, but with sampled experience only Model-Based Fully Can use DP-style backups by simulating transitions from the model"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#policy-based-algorithms","title":"Policy-Based Algorithms","text":"<p>Policy-based methods:</p> <ul> <li>Don\u2019t use value functions explicitly (though they sometimes do, like in actor-critic).</li> <li>Don\u2019t perform backups or rely on recursive decomposition via the Bellman equation.</li> <li>Instead, they treat the policy itself as the thing to optimize.</li> </ul>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#why-you-cant-use-dp-with-pure-policy-based-algorithms","title":"Why You Can't Use DP with Pure Policy-Based Algorithms","text":"<p>Dynamic Programming works by recursively estimating values of states or actions. But in policy gradient methods, the policy is parameterized (e.g., with a neural network), and we directly adjust the parameters to maximize expected return\u2014not via value recursion.</p> <p>So:</p> DP-based methods Policy-based methods What is updated? <code>V(s)</code> or <code>Q(s,a)</code> using Bellman equations <code>\u03c0(a)</code> What is needed? Full model (<code>P</code>, <code>R</code>) No model needed (can use sampled rewards) Algorithm type Planning Optimization via stochastic gradient ascent Backup type Recursive Sample-based Monte Carlo or TD estimates"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#so-what-do-we-use-instead-of-dp","title":"So What Do We Use Instead of DP?","text":"<p>Policy-based algorithms use gradient-based optimization, not Bellman backups.</p> <p>There are two major categories:</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#1-monte-carlo-policy-gradient","title":"1. Monte Carlo Policy Gradient","text":"<ul> <li>Estimate total return from sampled episodes.</li> <li>Use this to compute the gradient of the expected return w.r.t. the policy parameters \u03b8.</li> </ul> <p>Classic example: REINFORCE</p> <pre><code>\u03b8 \u2190 \u03b8 + \u03b1 * \u2207\u03b8 log \u03c0\u03b8(a|s) * G\u209c\n</code></pre> <p>Where:</p> <ul> <li><code>G\u209c</code> is the return from time <code>t</code> onward.</li> <li><code>\u2207\u03b8 log \u03c0\u03b8(a|s)</code> is the gradient of the log-policy.</li> </ul> <p>Doesn't require value function or model High variance in gradient estimates</p>"},{"location":"guides/data_science_and_ai/reinforcement_learning_basics/#2-actor-critic-methods","title":"2. Actor-Critic Methods","text":"<p>Hybrid methods where:</p> <ul> <li>The actor is the policy being optimized.</li> <li>The critic estimates a value function (usually via TD learning) to reduce variance.</li> </ul> <p>Examples:</p> <ul> <li>A2C (Advantage Actor-Critic)</li> <li>PPO (Proximal Policy Optimization)</li> <li>DDPG, SAC</li> </ul> <p>Here, the critic may use Bellman-style updates, but the policy update is still gradient-based, not DP.</p>"},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/","title":"\ud83d\ude80 Simulators for Unlocking Innovation","text":""},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#why-simulators-drive-ai-innovation-and-business-growth","title":"Why Simulators Drive AI, Innovation, and Business Growth","text":"<p>Simulators provide a controlled environment for exploring decisions without the risks and costs associated with real-world experimentation. In AI workflows, they act as a stuntman, allowing teams to safely test strategies, train intelligent agents, and optimize processes before deployment.</p> <p>From a business perspective, simulators reduce operational costs, accelerate innovation, and increase confidence in the implementation of AI solutions. They play a key role in bridging the gap between theoretical models and real-world outcomes - especially in complex or regulated industries.</p>"},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#what-this-simulator-emulates","title":"What This Simulator Emulates","text":""},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#modeling-industrial-processes-with-finite-state-machines","title":"Modeling Industrial Processes with Finite State Machines","text":"<p>This simulator models the behavior of physical or digital systems using Finite State Machines (FSMs). Each \u201cstate\u201d reflects a system condition, while transitions are governed by business rules, sensor data, or external triggers.</p> <p>This approach is ideal for:</p> <ul> <li>Semi-batch or continuous processes</li> <li>Emulating time-based or condition-driven system flows</li> <li>Integrating control logic for decision-making under constraints</li> </ul>"},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#scalable-architecture-built-on-solid-foundations","title":"\ud83e\uddf1 Scalable Architecture, Built on Solid Foundations","text":""},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#designed-for-modularity-extensibility-and-client-reusability","title":"Designed for Modularity, Extensibility, and Client Reusability","text":"<ul> <li>Modular Architecture based on SOLID principles, ensuring maintainability and testability</li> <li>Client-Specific Configurations: Easily spin up new simulations with domain-specific rules and data</li> <li>Transparent I/O Interfaces: Standardized contracts for ingesting sensor data and outputting forecasts or alerts</li> <li>Plug-and-Play Modifiers: Inject dynamic behaviors (e.g., equipment failures, operator interventions) via time-stamped modifiers</li> <li>Separation of Concerns: Decouples simulation core from domain logic and presentation layers</li> </ul>"},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#real-world-business-impact","title":"\ud83d\udcca Real-World Business Impact","text":""},{"location":"guides/data_science_and_ai/simulators_for_unlocking_innovation/#accelerating-decision-making-and-ai-integration","title":"Accelerating Decision-Making and AI Integration","text":"<ul> <li>Enabled rapid prototyping and testing of AI-driven optimization strategies</li> <li>Facilitated explainable AI through simulation-based counterfactuals</li> <li>Reduced the time-to-deployment for intelligent agents by providing pre-training environments</li> <li>Supported stakeholder communication with visual demos and what-if analysis</li> <li>Enabled cross-functional alignment between Data Science, Engineering, and Business teams by acting as a common sandbox</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/","title":"The Neural Network","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#step-1-the-neural-network-structure","title":"Step 1: The Neural Network Structure","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-neuras-painting-studio","title":"Analogy: Neura\u2019s Painting Studio","text":"<p>Imagine you\u2019re training a robot artist named Neura. She creates paintings based on three key inputs:</p> <ul> <li>Color mix ratio</li> <li>Brush pressure</li> <li>Tilt angle</li> </ul> <p>Each painting starts from a sketch made of these 3 values.</p> <p>But Neura doesn\u2019t just throw those values directly onto the canvas \u2014 she sends them through a sequence of creative stations (layers), where she processes, transforms, and combines the inputs using adjustable brushes (weights and biases) and creative filters (activation functions).</p> <p></p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-does-the-network-look-like","title":"What Does the Network Look Like?","text":"<p>We\u2019re using:</p> <ul> <li>3 inputs (color, pressure, tilt)</li> <li>1 hidden layer with 4 neurons</li> <li>1 output (a score or prediction)</li> </ul> <p><pre><code>import torch\nimport torch.nn as nn\n\nclass NeuralPainter(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(3, 4)  # 3 inputs \u2192 4 hidden neurons\n        self.layer2 = nn.Linear(4, 1)  # 4 \u2192 1 output\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))  # Neura applies her creative filter\n        return self.layer2(x)\n\nmodel = NeuralPainter()\n</code></pre> </p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#example-input-neura-paints-for-two-sketches","title":"Example Input: Neura Paints for Two Sketches","text":"<pre><code>inputs = torch.tensor([\n    [0.5, 0.7, 0.3],  # Sketch 1: a gentle stroke with a warm tone\n    [0.9, 0.2, 0.6]   # Sketch 2: a bold stroke with a tilted brush\n])\nprediction = model(inputs)\n</code></pre> <p>The input shape is <code>[2, 3]</code>:</p> <ul> <li>2 samples (batch size)</li> <li>3 features each (color, pressure, tilt)</li> </ul> <p>Neura paints both sketches at once using the same model!</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#how-modelinputs-works-internally","title":"How <code>model(inputs)</code> Works Internally","text":"<p>When you run <code>model(inputs)</code>, PyTorch is doing this under the hood:</p> <pre><code>model.__call__(inputs)  # Magic from nn.Module\n\u2192 model.forward(inputs) # This is your blueprint in action\n</code></pre>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-neuras-painting-process","title":"Analogy: Neura\u2019s Painting Process","text":"<p>Neura has multiple stations with different brushes. The <code>forward()</code> method is the blueprint that tells her:</p> <p>\u201cHere\u2019s how to go from the sketch to the final painting.\u201d</p> <p>So in practice:</p> <ul> <li>She sends inputs through <code>layer1</code></li> <li>Applies a filter (ReLU) to remove weak/unhelpful strokes</li> <li>Passes the result to <code>layer2</code> to finalize the artwork</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#activation-function-why-relu","title":"Activation Function: Why ReLU?","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-relu-as-neuras-creative-filter","title":"Analogy: ReLU as Neura\u2019s \"Creative Filter\"","text":"<p>After the first station (layer), Neura stops and reflects:</p> <p>\u201cIf a stroke had a positive effect, I\u2019ll keep it.  If it hurt the painting (negative), I\u2019ll just erase it.\u201d</p> <p>That\u2019s what ReLU (Rectified Linear Unit) does.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#in-math","title":"In math:","text":"<p><pre><code>ReLU[x] = Max[0, x]\n</code></pre> It turns all negative values into 0 and leaves positive values unchanged.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#why-is-it-important","title":"Why is it important?","text":"<p>Without ReLU (or another activation function), Neura\u2019s network is just a giant linear equation. No matter how many layers she adds, she\u2019s still only able to draw straight lines \u2014 no curves, corners, shadows, or depth.</p> <p>ReLU adds non-linearity, giving her the power to paint abstract shapes, fluid transitions, and creative patterns.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#heads-up-the-vanishing-gradient-problem","title":"Heads-Up: The Vanishing Gradient Problem","text":"<p>In deeper networks, we train Neura by passing feedback backward through the layers \u2014 telling her how to improve.</p> <p>With older activations like sigmoid or tanh, the feedback (gradient) gets squashed at each layer, until it\u2019s too tiny to matter.</p> <p>That means:</p> <ul> <li>Neura\u2019s early brushes don\u2019t get meaningful corrections</li> <li>Those layers stop learning</li> <li>Training slows down or fails</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#relu-fixes-that","title":"ReLU Fixes That","text":"<p>ReLU gradients are:</p> <ul> <li>1 (if the output was positive)</li> <li>0 (if the output was zeroed)</li> </ul> <p>This helps gradients survive the trip back \u2014 so every brush in Neura\u2019s chain gets useful feedback.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#visual-flow-of-data","title":"Visual Flow of Data","text":"<pre><code>[Color, Pressure, Tilt]   \u2190 sketch features\n      \u2193\n  [Layer 1: Linear \u2192 ReLU] \u2190 brush stroke + filter\n      \u2193\n  [Layer 2: Linear]        \u2190 combine and score\n      \u2193\n  [Final Prediction]       \u2190 painting score for each sample\n</code></pre>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#summary-table","title":"Summary Table","text":"Concept PyTorch Neura Analogy Inputs <code>[batch_size, 3]</code> Sketches: color, pressure, tilt <code>__init__()</code> Layer definitions Neura sets up painting stations <code>forward()</code> Flow of data Neura\u2019s blueprint for painting <code>model(inputs)</code> Calls <code>forward()</code> You hand Neura 2 sketches and ask her to paint them Activation Function <code>ReLU</code> Neura filters out weak/negative strokes Vanishing Gradients Solved by <code>ReLU</code> Neura\u2019s early brushes still get good feedback"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#step-2-input-and-prediction-the-forward-pass","title":"Step 2: Input and Prediction (The Forward Pass)","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-neura-receives-sketches","title":"Analogy: Neura Receives Sketches","text":"<p>You hand Neura two sketch requests:</p> <ol> <li>A warm, firm stroke with a light tilt</li> <li>A bold color, gentle pressure, and medium tilt</li> </ol> <p>These sketches are represented as feature vectors (3 values each).</p> <p>Neura uses her current brush settings (weights and biases \u2014 still random at first) to attempt her first paintings.</p> <p>She follows the forward pass blueprint defined earlier, applying her brush strokes layer by layer.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#code-example-batch-of-2-sketches","title":"Code Example (Batch of 2 sketches)","text":"<pre><code>import torch\n\n# Each row is a sketch with 3 features: [color, pressure, tilt]\ninputs = torch.tensor([\n    [0.5, 0.7, 0.3],  # Sample 1\n    [0.9, 0.2, 0.6]   # Sample 2\n])\n\n# Neura tries painting using her current brush settings\nprediction = model(inputs)\nprint(prediction)\n</code></pre> <ul> <li>The input shape is <code>[2, 3]</code> \u2014 2 sketches, 3 features each.</li> <li>The output shape is <code>[2, 1]</code> \u2014 Neura returns a prediction score for each painting.</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-actually-happens-under-the-hood","title":"What Actually Happens (Under the Hood)","text":"<ul> <li>Inputs go through <code>layer1</code>: Neura combines the sketch features using her hidden brushes.</li> <li>ReLU removes unhelpful strokes (negative outputs).</li> <li>Outputs go through <code>layer2</code>: Neura blends the remaining strokes into a final score.</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy","title":"Analogy","text":"<p>\u201cNeura uses her internal logic and current brushes to paint what she thinks each sketch should look like. It\u2019s her best guess \u2014 for now.\u201d</p> <p>At this point, her painting is likely far from perfect \u2014 she hasn\u2019t received any feedback yet!</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#step-3-loss-function-how-bad-was-the-painting","title":"Step 3: Loss Function \u2013 How Bad Was the Painting?","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-neura-gets-judged","title":"Analogy: Neura Gets Judged","text":"<p>After Neura finishes her two paintings, you compare each one to a reference painting and say:</p> <p>\u201cHere's what you were actually supposed to paint.\u201d</p> <p>This comparison generates a loss value \u2014 a number representing how far off her guesses were.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#the-loss-function","title":"The Loss Function","text":"<p>We\u2019ll use Mean Squared Error (MSE) \u2014 a common choice when outputs are continuous (like price, score, etc.).</p> <pre><code>import torch.nn as nn\n\n# Target values for each painting (the ideal outputs)\ntarget = torch.tensor([[0.8], [0.3]])  # Ground truth for both samples\n\n# Define the loss function\nloss_fn = nn.MSELoss()\n\n# Calculate how far Neura was from the real painting\nloss = loss_fn(prediction, target)\nprint(loss)\n</code></pre>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-is-mse-doing","title":"What is MSE doing?","text":"<p>It calculates: <pre><code>Loss = (1/n) * Sum[(\u0177\u1d62 - y\u1d62)\u00b2, {i}]\n</code></pre> In words:</p> <p>\u201cFor each painting, square the difference between Neura's version and the correct one, then average those errors.\u201d</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-continued","title":"Analogy (continued)","text":"<p>You, the mentor, hold up her two paintings next to the real ones and say:</p> <p>\u201cPainting #1 was too light. Painting #2 missed the tone.\u201d  \u201cLet\u2019s figure out how wrong each one was.\u201d</p> <p>This numeric feedback becomes the signal Neura will use to learn.</p> <p>But we\u2019re not done yet \u2014 she still needs to figure out which brushes to adjust, and how much. That\u2019s what happens next with backpropagation.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#summary-of-steps-2-and-3","title":"Summary of Steps 2 and 3","text":"Step Code Neura Analogy Prepare Inputs <code>inputs = torch.tensor(...)</code> You hand her sketches to paint Make Prediction <code>prediction = model(inputs)</code> Neura paints using her current skills Define Targets <code>target = torch.tensor(...)</code> You show her what the real paintings should look like Calculate Loss <code>loss = loss_fn(pred, target)</code> You give her a number showing how far off she was"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#step-4-backpropagation-compute-the-feedback","title":"Step 4: Backpropagation \u2013 Compute the Feedback","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-is-backpropagation","title":"What Is Backpropagation?","text":"<p>Backpropagation is the process of computing how much each weight and bias contributed to the final error, using calculus (specifically the chain rule). It\u2019s the feedback mechanism that lets Neura improve her painting technique.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#neura-analogy-backprop-as-a-feedback-chain","title":"Neura Analogy: Backprop as a Feedback Chain","text":"<p>Imagine Neura just finished painting two sketches. The results were off \u2014 maybe too pale, too harsh, or missing contrast.</p> <p>You don\u2019t just say:</p> <p>\u201cDo better next time.\u201d</p> <p>Instead, you help her trace each error back to the exact brush and stroke that caused it. That\u2019s backpropagation:</p> <ul> <li>\u201cYour final stroke added too much red.\u201d</li> <li>\u201cThat red came from Layer 2\u2019s pressure.\u201d</li> <li>\u201cLayer 1's brush angle influenced that stroke.\u201d</li> </ul> <p>Each brush station gets a personalized correction note.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-happens-in-lossbackward","title":"What Happens in <code>loss.backward()</code>?","text":"<p>Once you've computed the loss:</p> <pre><code>loss = loss_fn(prediction, target)\nloss.backward()\n</code></pre> <p>PyTorch:</p> <ol> <li> <p>Traverses the computation graph backward</p> </li> <li> <p>Uses the chain rule to compute:    \u2202Loss/\u2202[Each Weight/Bias]</p> </li> <li> <p>Stores those gradients in each parameter\u2019s <code>.grad</code> field</p> </li> </ol>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#code-insight","title":"Code Insight","text":"<pre><code>model.layer1.weight.grad\nmodel.layer2.bias.grad\n</code></pre> <p>These give the gradient values \u2014 the feedback Neura needs.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#neuras-intuition","title":"Neura\u2019s Intuition","text":"<p>Before backprop:</p> <p>\u201cHmm\u2026 my painting missed the mark.\u201d</p> <p>After backprop:</p> <p>\u201cAh! Brush #1 was too heavy, and brush #3 was too soft. I\u2019ll fix those.\u201d</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#how-the-chain-rule-guides-neura","title":"How the Chain Rule Guides Neura","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#real-math-example-1-layer","title":"Real Math Example (1-layer)","text":"<p>Let\u2019s say:</p> <ul> <li>Input: x = 0.5</li> <li>Weight: w</li> <li>Output: \u0177 = w * x</li> <li>Target: y = 1.0</li> <li>Loss: L = (\u0177 - y)\u00b2</li> </ul> <p>Apply chain rule: <pre><code>dL/dw = (dL/d\u0177) * (d\u0177/dw) = 2(\u0177 - y) * x\n</code></pre> If:</p> <ul> <li>\u0177 = 0.8</li> <li>Error = \u0177 - y = -0.2</li> <li>x = 0.5</li> <li>dL/dw = 2(-0.2) * 0.5 = -0.2</li> </ul> <p>This is the gradient \u2014 the tweak Neura needs to apply.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#step-5-optimizer-apply-the-corrections","title":"Step 5: Optimizer \u2013 Apply the Corrections","text":"<p>Now that Neura has her feedback (gradients), she needs to act on it.</p> <p>This is where the optimizer comes in. It decides how much to adjust each brush (weight/bias) based on the gradient.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#the-sgd-formula","title":"The SGD Formula","text":"<pre><code>w = w - \u03b7 * gradient\n</code></pre> <p>Where:</p> <ul> <li>\u03b7 = learning rate</li> <li>gradient = dL/dw</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#code-example","title":"Code Example","text":"<pre><code>optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n# Apply the update (after loss.backward())\noptimizer.step()\n\n# Reset gradients before next loop\noptimizer.zero_grad()\n</code></pre>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#neura-analogy-different-optimizers","title":"Neura Analogy: Different Optimizers","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#sgd-stochastic-gradient-descent","title":"SGD (Stochastic Gradient Descent)","text":"<p>Neura takes your feedback literally and immediately:</p> <p>\u201cBrush #2 was off by 0.1? Got it \u2014 let me fix it by exactly that much!\u201d</p> <p>Simple and fast Can be noisy or unstable in tricky paintings</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Neura now keeps a journal:</p> <ul> <li>Tracks feedback over time (momentum)</li> <li>Adjusts learning speed per brush</li> <li>Slows down where things are volatile, speeds up where it\u2019s stable</li> </ul> <pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n</code></pre> <p>More stable Works well on complex or noisy sketches Great for Neura when she\u2019s dealing with many styles</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#comparison-table","title":"Comparison Table","text":"Concept SGD Adam Neura Analogy Uses only latest gradient yes uses history too Neura adjusts vs. Neura with memory Adapts learning rate Same for all Learns per-parameter Smart pacing for each brush Stability Can overshoot Very stable Careful &amp; strategic learning"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#summary-the-feedback-fix-loop","title":"Summary: The Feedback &amp; Fix Loop","text":"Step Code Neura Analogy Compute gradients <code>loss.backward()</code> \u201cHow much did each brush cause the mistake?\u201d Read the gradients <code>param.grad</code> Feedback note taped to each brush Apply corrections <code>optimizer.step()</code> Neura adjusts brush pressure/angle/color Clear gradients <code>optimizer.zero_grad()</code> Neura erases old notes before the next painting"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#step-6-training-loop-repetition-makes-perfect","title":"Step 6: Training Loop \u2013 Repetition Makes Perfect","text":""},{"location":"guides/data_science_and_ai/the_neural_network_structure/#analogy-practice-makes-the-painter","title":"Analogy: Practice Makes the Painter","text":"<p>At this point, Neura has:</p> <p>A painting plan (<code>forward()</code>) A way to assess her results (<code>loss_fn</code>) A feedback system (<code>loss.backward()</code>) A strategy to apply changes (<code>optimizer.step()</code>)</p> <p>But doing this just once won\u2019t turn her into an expert.  She needs to practice over and over \u2014 painting, reviewing, adjusting.</p> <p>That\u2019s where the training loop comes in.</p>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-happens-in-each-epoch","title":"What Happens in Each Epoch?","text":"<p>Each time Neura goes through the loop (i.e., an epoch), she:</p> <ol> <li>Receives a batch of sketch inputs</li> <li>Paints predictions using her current brushes</li> <li>Compares predictions to the reference paintings</li> <li>Computes how far off she was (loss)</li> <li>Traces the mistakes back to specific brushes (gradients)</li> <li>Updates each brush using the optimizer</li> <li>Clears feedback for the next round</li> </ol>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#code-logging-training-info-in-pandas","title":"Code \u2013 Logging Training Info in Pandas","text":"<p>We\u2019ll train on 2 sketches using our existing <code>NeuralPainter</code> model.</p> <pre><code>import torch\nimport torch.nn as nn\nimport pandas as pd\n\n# Define the model\nclass NeuralPainter(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(3, 2)  # smaller for clarity\n        self.layer2 = nn.Linear(2, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        return self.layer2(x)\n\n# Initialize\nmodel = NeuralPainter()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nloss_fn = nn.MSELoss()\n\n# Input sketches (batch of 2) and targets\ninputs = torch.tensor([\n    [0.5, 0.7, 0.3],\n    [0.9, 0.2, 0.6]\n])\ntargets = torch.tensor([[0.8], [0.3]])\n\n# Log training info\nlog = []\n\n# Training loop\nfor epoch in range(10):\n    predictions = model(inputs)\n    loss = loss_fn(predictions, targets)\n\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Extract data for logging (focusing on first neuron of layer1)\n    log.append({\n        'Epoch': epoch,\n        'W1[0][0]': model.layer1.weight[0][0].item(),\n        'W1[0][1]': model.layer1.weight[0][1].item(),\n        'W1[0][2]': model.layer1.weight[0][2].item(),\n        'B1[0]': model.layer1.bias[0].item(),\n        'Prediction[0]': predictions[0].item(),\n        'Prediction[1]': predictions[1].item(),\n        'Target[0]': targets[0].item(),\n        'Target[1]': targets[1].item(),\n        'Loss': loss.item(),\n        'Grad_W1[0][0]': model.layer1.weight.grad[0][0].item(),\n        'Grad_B1[0]': model.layer1.bias.grad[0].item(),\n    })\n\n    optimizer.step()\n\n# Convert to DataFrame\ndf = pd.DataFrame(log)\n</code></pre>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#how-to-interpret-the-table","title":"How to Interpret the Table","text":"<p>This table is like Neura\u2019s training diary. Each row = one epoch.</p> <p></p> <p>Here\u2019s what the columns mean:</p> Column Meaning <code>Epoch</code> Iteration count (one painting session) <code>W1[0][0]</code> Weight from input 0 \u2192 neuron 0 in layer 1 <code>W1[0][1]</code>, <code>W1[0][2]</code> Other weights for same neuron <code>B1[0]</code> Bias for that same neuron <code>Prediction[0]</code> Neura\u2019s guess for Sketch #1 <code>Target[0]</code> Ground truth for Sketch #1 <code>Loss</code> Average squared error across both samples <code>Grad_W1[0][0]</code> Gradient for <code>W1[0][0]</code> \u2014 how much Neura should adjust it <code>Grad_B1[0]</code> Bias adjustment signal"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#what-to-look-for","title":"What to Look For:","text":"<ul> <li>Predictions should get closer to targets</li> <li>Loss should decrease over time</li> <li>Weights and bias will change with each epoch (watch them learn!)</li> <li>Gradients should shrink \u2192 feedback becomes gentler as Neura improves</li> </ul>"},{"location":"guides/data_science_and_ai/the_neural_network_structure/#summary","title":"Summary","text":"Step Action Taken Neura Analogy Loop begins New epoch Another painting session <code>model(inputs)</code> Forward pass Neura paints using current brushes <code>loss_fn(...)</code> Compute error You judge the result <code>loss.backward()</code> Backpropagation Neura gets specific feedback <code>optimizer.step()</code> Apply update Neura tweaks her brush settings <code>optimizer.zero_grad()</code> Clear old gradients She starts fresh for the next sketch"},{"location":"guides/demos/overview/","title":"Projects: Real-World Applications in Software &amp; Data Science","text":"<p>This section highlights key projects that bring Data Science, Software Engineering, and DevOps concepts to life. Each project is designed to solve real-world challenges and improve efficiency through hands-on implementation.  </p>"},{"location":"guides/demos/overview/#dev-container","title":"Dev Container","text":"<p>View on GitHub </p> <p>A pre-configured development environment that ensures consistency and isolation across development setups.  </p> <p>Why it Matters? Development environments can vary across machines, leading to inconsistencies and \"works on my machine\" issues. This dev container provides an isolated environment with all necessary tools and dependencies, ensuring that the development setup is uniform across all team members.  </p> <p>Key Features: - Isolated development environment to prevent conflicts with host systems. - Pre-installed tools and libraries, such as Python, pyenv, and Poetry. - Reproducible setups ensuring consistency across different machines. - Simplifies onboarding for new developers by providing a ready-to-use environment.  </p>"},{"location":"guides/demos/overview/#cookiecutter-template","title":"Cookiecutter Template","text":"<p>View on GitHub </p> <p>A project template generator designed to standardize and accelerate the setup of new projects.  </p> <p>Why it Matters? Starting a new project often involves repetitive setup tasks. This template ensures that all new projects adhere to a consistent structure and include necessary configurations, reducing setup time and potential errors.  </p> <p>Key Features: - Pre-defined directory structures for various project types. - Inclusion of essential configuration files and dependencies. - Customization prompts to tailor the template to specific project needs. - Enhances collaboration by maintaining uniformity across projects.  </p>"},{"location":"guides/demos/overview/#database-toolkit","title":"Database Toolkit","text":"<p>View on GitHub </p> <p>A powerful, scalable solution for generating synthetic datasets tailored to industrial applications.  </p> <p>Why it Matters? Businesses relying on machine learning models need high-quality training data. This toolkit automates data generation, anomaly simulation, and dataset merging, making it easier to test and deploy AI models.  </p> <p>Key Capabilities: - Automated generation of structured industrial pump data. - Custom anomaly injection for model testing. - Flexible factory design pattern to streamline dataset creation. - Built using SOLID principles for maintainability.  </p> <p>These projects exemplify practical applications of software engineering and data science principles, aiming to enhance productivity and maintainability in real-world scenarios.</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/","title":"Introduction: Why Use a Dev Container?","text":"<p>Modern Data Science and software teams require consistency, reproducibility, and fast on-boarding. A Dev Container (via VS Code\u2019s Dev Containers or GitHub Codespaces) packages your code, dependencies, and tooling into a single, versioned environment. This approach brings benefits on three levels:</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#benefits-for-data-science-development-teams","title":"Benefits for Data Science &amp; Development Teams","text":"<ul> <li> <p>Consistency Across Machines    Every team member runs in the exact same container image, so \u201cit works on my machine\u201d becomes a thing of the past.</p> </li> <li> <p>Low Setup Friction    New hires or contributors simply open the repo in VS Code and the container spins up with all runtimes, libraries, and tools pre-installed.</p> </li> <li> <p>Isolation &amp; Safety    You keep project dependencies separate from host-OS packages\u2014no risk of polluting your laptop or server with conflicting libraries.</p> </li> <li> <p>Easy Experimentation    Spin up multiple containers side-by-side (e.g. one per project or feature) and tear them down cleanly when you\u2019re done.</p> </li> </ul>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#benefits-for-the-business","title":"Benefits for the Business","text":"<ul> <li> <p>Faster Time to Market    Developers and analysts spend less time wrestling with environment issues and more time delivering features and insights.</p> </li> <li> <p>Lower Support Overhead    Fewer \u201cenvironment setup\u201d tickets for IT or DevOps, and predictable deployments in staging and production.</p> </li> <li> <p>Reproducible Pipelines    Models, reports, and demos run identically in local, CI, and cloud, reducing bugs and data-drift surprises.</p> </li> <li> <p>Stronger Security &amp; Compliance    Containers can be locked down (fixed base images, vulnerability scanning) and audited, aligning with enterprise governance.</p> </li> <li> <p>Scalable Collaboration    Cross-functional teams (Data Science, Engineering, QA, Product) share a single source of truth\u2014no more \u201cwhich Python version?\u201d debates.</p> </li> </ul>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#considerations-trade-offs","title":"Considerations &amp; Trade-Offs","text":"<p>While Dev Containers offer consistency and reproducibility, they also introduce some overhead and complexity. It\u2019s important to understand these downsides so you can make an informed choice:</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#technical-considerations","title":"Technical Considerations","text":"<ul> <li> <p>Container Build &amp; Startup Time    Each time you rebuild or open the container, VS Code must spin up Docker layers, install dependencies, and initialize services. For large images or frequent rebuilds, this can add minutes to your workflow.</p> </li> <li> <p>Resource Consumption    Running Docker containers\u2014especially multiple in parallel\u2014consumes CPU, memory, and disk I/O. On resource\u00ad-constrained machines, this may slow down not only your containers but also other local applications.</p> </li> <li> <p>Debugging Container Issues    When something goes wrong (e.g. networking, volume mounting, permission errors), you need Docker-specific troubleshooting skills, which can steepen the learning curve for data scientists or analysts unfamiliar with container tooling.</p> </li> <li> <p>Dependency on Docker &amp; VS Code     Your team must install and maintain compatible versions of Docker Desktop (or Docker Engine) and the VS Code \u201cDev Containers\u201d extension. Contributors using other editors will miss out or need an alternative workflow.</p> </li> </ul>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Docker Desktop for Mac     Download and install Docker Desktop for Mac (Intel or Apple Silicon) following the official guide Docker Documentation.</p> </li> <li> <p>Make sure you meet the system requirements (macOS version, \u22654 GB RAM).</p> </li> <li> <p>Visual Studio Code     Install VS Code from https://code.visualstudio.com/.</p> </li> <li> <p>Dev Containers extension     In VS Code, install Dev Containers (formerly \u201cRemote \u2013 Containers\u201d) from the Extensions marketplace:</p> </li> </ol> <pre><code>ms-vscode-remote.remote-containers\n</code></pre> <ol> <li>Git     Ensure <code>git</code> is available (macOS Command Line Tools or Homebrew).</li> </ol> <pre><code>git --version\n</code></pre>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#quickstart-clone-run-the-dev-container","title":"Quickstart: Clone &amp; Run the Dev Container","text":"<ol> <li>Clone the repository</li> </ol> <pre><code>clone https://github.com/aldojasb/my_devcontainer.git\ncd my_devcontainer\n</code></pre> <ol> <li>Open in VS Code</li> </ol> <pre><code>code .\n</code></pre> <p>VS Code will detect the <code>devcontainer.json</code> in your workspace.</p> <ol> <li> <p>Rebuild (or Reopen) the Container</p> </li> <li> <p>When prompted in the lower right, click \u201cReopen in Container\u201d.</p> </li> <li> <p>Or, open the Command Palette, type Dev Containers: Rebuild Container, and press Enter.</p> </li> <li> <p>Wait for the build to complete     VS Code will pull your base image, install dependencies (Python, Zsh, Oh My Zsh, Poetry, pyenv, etc.), and mount your workspace.</p> </li> <li> <p>Verify</p> </li> <li> <p>Open a new integrated terminal (Terminal \u2192 New Terminal).</p> </li> <li> <p>Ensure you\u2019re in Zsh with Oh My Zsh loaded.</p> </li> <li> <p>Check that you can run your project\u2019s Poetry venv:</p> <pre><code>general_projects/simulators_fsm\npoetry run python --version\n</code></pre> </li> <li> <p>You should see <code>Python 3.11.10</code> (or your configured version) coming from <code>.venv</code>.</p> </li> </ol>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#helpful-links","title":"Helpful Links","text":"<ul> <li>Docker Desktop for Mac install guide: https://docs.docker.com/desktop/install/mac-install/ Docker Documentation</li> <li>VS Code download: https://code.visualstudio.com/</li> <li>Dev Containers extension: https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers</li> </ul> <p>That\u2019s it! Your team can now clone the repo, rebuild the container, and have a consistent, ready-to-code environment in minutes.</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#get-my-portfolio-locally","title":"Get My Portfolio Locally","text":"<p>Follow these steps to clone and run the general_projects portfolio on your machine.</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#1-clone-the-repo","title":"1. Clone the repo","text":"<pre><code>git clone https://github.com/aldojasb/general_projects.git\ncd general_projects\n</code></pre>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#2-install-all-dependencies","title":"2. Install all dependencies","text":"<pre><code>poetry install\n</code></pre>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#how-to-use-create-a-new-project-by-using-cookiecutter-template","title":"How to Use Create a New Project by using Cookiecutter Template","text":"<p>To create a new project using the Cookiecutter template, follow these steps:</p> <ol> <li> <p>Navigate to the Repository Root: <pre><code>cd /path/to/general_projects\n</code></pre></p> </li> <li> <p>Run the Cookiecutter Command: <pre><code>cookiecutter ./cookiecutter_template\n</code></pre></p> </li> <li> <p>Provide Input:</p> </li> <li> <p>When prompted, enter values such as <code>project_name</code>. For example:      <pre><code>[1/1] project_name (default: project_test): MyNewProject\n</code></pre></p> </li> <li> <p>Project Generation:</p> </li> <li>A new folder will be created in the <code>general_projects</code> directory with the name you specified (e.g., <code>MyNewProject</code>).</li> <li> <p>The folder will include pre-configured files and a standardized structure.</p> </li> <li> <p>Start Development:</p> </li> <li>Navigate to the new project folder:      <pre><code>cd MyNewProject\n</code></pre></li> <li>Install dependencies with Poetry:      <pre><code>poetry install\n</code></pre></li> <li>Activate the virtual environment:      <pre><code>poetry shell\n</code></pre></li> <li>Start coding!</li> </ol>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#troubleshooting-jupyter-kernel-in-vs-code-dev-container","title":"Troubleshooting: Jupyter Kernel in VS Code Dev Container","text":"<p>If your notebook isn\u2019t picking up the in-project virtual environment, follow these steps to verify and register the kernel.</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#1-list-all-registered-kernels","title":"1. List all registered kernels","text":"<p>If your notebook isn\u2019t picking up the in-project virtual environment, follow these steps to verify and register the kernel.</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#1-list-all-registered-kernels_1","title":"1. List all registered kernels","text":"<p>Ensure Jupyter can see existing kernels:</p> <pre><code>poetry run jupyter kernelspec list\n</code></pre> <p>What it does: Uses Poetry\u2019s venv to run <code>jupyter kernelspec list</code>, which outputs all kernel names and paths.</p> <p>Example output:</p> <pre><code>Available kernels:\npython3            /home/vscode/.local/share/jupyter/kernels/python3\nMyNewProject     /home/vscode/.local/share/jupyter/kernels/MyNewProject\n</code></pre>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#2-register-your-simulators-virtualenv-as-a-new-kernel","title":"2. Register your simulator\u2019s virtualenv as a new kernel","text":"<p>From your project root (e.g. <code>MyNewProject/</code>), run:</p> <pre><code>poetry run python -m ipykernel install --user --name MyNewProject --display-name \"MyNewProject_python_venv\"\n</code></pre> <p><code>poetry run</code>: Executes the following command inside the project\u2019s <code>.venv</code>.</p> <p><code>python -m ipykernel install</code>: Installs a new Jupyter kernel spec.</p> <p>Flags:</p> <ul> <li><code>--user</code> \u2192 install under your home directory (no root needed).</li> <li><code>--name MyNewProject</code> \u2192 the internal ID (folder name under <code>kernels/</code>).</li> <li><code>--display-name \"MyNewProject_python_venv\"</code> \u2192 the name shown in VS Code\u2019s kernel picker</li> </ul>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#3-reload-vs-code","title":"3. Reload VS Code","text":"<p>After registering:</p> <ol> <li>Open the Command Palette (<code>Ctrl+Shift+P</code> / <code>\u2318+Shift+P</code>).</li> <li>Type Developer: Reload Window and hit Enter.</li> </ol> <p>This forces VS Code to refresh its kernel list.</p>"},{"location":"guides/development_and_devops/introduction_to_dev_container/#4-select-your-new-kernel","title":"4. Select your new kernel","text":"<ol> <li>Open your <code>.ipynb</code> file.</li> <li>In the top-right corner, click the current kernel name.</li> <li>Choose MyNewProject_python_venv.</li> </ol> <p>Now your notebook will run on the exact Python interpreter inside <code>MyNewProject/.venv</code>.</p> <p>Tip: To confirm, run in a code cell:</p> <pre><code>import sys\nprint(sys.executable)\n</code></pre> <p>It should point to:</p> <pre><code>/workspace/general_projects/MyNewProject/.venv/bin/python\n</code></pre>"},{"location":"guides/development_and_devops/logging_manager_package/","title":"<code>logging_manager</code> Package Documentation","text":""},{"location":"guides/development_and_devops/logging_manager_package/#overview","title":"Overview","text":"<p>logging_manager is a lightweight, reusable Python package designed to standardize and simplify logging setup across multiple projects.  It provides a consistent way to initialize loggers with both console and file handlers, ensuring that every script or application follows a professional logging strategy with minimal effort.</p> <p>Instead of configuring logging manually in every project, developers can simply import and use <code>logging_manager</code> to instantly enable robust logging behaviors.</p>"},{"location":"guides/development_and_devops/logging_manager_package/#core-functions","title":"Core Functions","text":""},{"location":"guides/development_and_devops/logging_manager_package/#setup_logginglog_file_pathnone-log_levellogginginfo","title":"<code>setup_logging(log_file_path=None, log_level=logging.INFO)</code>","text":"<p>Sets up logging configuration with customizable options.</p> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>log_file_path</code> (str, optional): Path to the log file. If <code>None</code>, logs will be shown only in the console.</p> </li> <li> <p><code>log_level</code> (int, optional): Standard Python logging level (e.g., <code>logging.INFO</code>, <code>logging.DEBUG</code>).</p> </li> <li> <p>Behavior:</p> </li> <li> <p>If a file path is provided, logs will be saved both to the file and printed to the console.</p> </li> <li> <p>If no file path is provided, logs are only printed to the console.</p> </li> <li> <p>Logs follow a standardized format, including:</p> <pre><code>[LoggerName]:LineNumber - Timestamp - LogLevel: Message\n</code></pre> </li> </ul>"},{"location":"guides/development_and_devops/logging_manager_package/#setup_logging_for_this_scriptlog_levellogginginfo","title":"<code>setup_logging_for_this_script(log_level=logging.INFO)</code>","text":"<p>Specialized helper function to automatically configure logging based on an environment variable (<code>PATH_TO_SAVE_THE_LOGS</code>).</p> <ul> <li>Parameters:</li> <li><code>log_level</code> (int, optional): Logging verbosity level (default: <code>logging.INFO</code>).</li> <li>Behavior:</li> <li>Retrieves the base directory for saving logs from the environment variable <code>PATH_TO_SAVE_THE_LOGS</code>.</li> <li>Validates that the path exists and is writable.</li> <li>Creates a default subdirectory (<code>tmp/</code>) if necessary.</li> <li>Sets up logging to a <code>logs.log</code> file inside the prepared directory.</li> <li>Also streams logs to the console simultaneously.</li> <li>Exceptions:</li> <li>Raises a clear <code>EnvironmentError</code> if the environment variable is missing.</li> <li>Raises a <code>FileNotFoundError</code> if the directory does not exist.</li> <li>Raises a <code>PermissionError</code> if the directory is not writable.</li> </ul>"},{"location":"guides/development_and_devops/logging_manager_package/#why-use-a-centralized-logging-manager","title":"Why Use a Centralized Logging Manager?","text":"<ul> <li>Consistency: Avoid reinventing logging configuration for each project or script.</li> <li>Portability: Projects can move across machines/environments while maintaining clean, reliable logging.</li> <li>Separation of concerns: Application logic stays clean, with logging concerns handled by a dedicated, reusable component.</li> <li>Error Handling: Clear errors when the environment is misconfigured, helping catch deployment issues early.</li> <li>Scalability: Easy to extend the logging configuration later (e.g., adding rotating file handlers, cloud log streams).</li> </ul>"},{"location":"guides/development_and_devops/logging_manager_package/#best-practices-followed","title":"Best Practices Followed","text":"<ul> <li>Logging configuration is isolated from application code.</li> <li>Environment-driven configuration improves portability across dev/staging/production.</li> <li>Graceful error handling when environment setup is incorrect.</li> <li>Flexible logging outputs: console-only for local dev, file+console for production.</li> <li>Standardized log format for easier parsing and debugging.</li> </ul>"},{"location":"guides/development_and_devops/logging_manager_package/#example-usage","title":"Example Usage","text":"<pre><code>from logging.manager.logging_config import setup_logging_for_this_script\nimport logging\n\n# Initialize logging\nsetup_logging_for_this_script(log_level=logging.DEBUG)\n\n# Use logger\nlogger = logging.getLogger(__name__)\n\nlogger.info(\"Application started.\")\nlogger.debug(\"This is a debug message.\")\nlogger.warning(\"This is a warning message.\")\n</code></pre>"},{"location":"guides/development_and_devops/logging_manager_package/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>Environment variable <code>PATH_TO_SAVE_THE_LOGS</code> must be defined if using <code>setup_logging_for_this_script</code>.</li> </ul>"},{"location":"guides/development_and_devops/mkdocs_tutorial/","title":"MkDocs: Installing, Configuring, and Automating it with GitHub Pages","text":"<p>This tutorial provides a step-by-step walkthrough to install, configure, and deploy MkDocs manually and set up an automated deployment pipeline using GitHub Actions.</p>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#step-1-install-mkdocs-with-poetry","title":"Step 1: Install MkDocs with Poetry","text":"<p>I use Poetry as a package manager to keep dependencies organized.</p>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#1-navigate-to-your-project-folder","title":"1. Navigate to your project folder:","text":"<pre><code>cd /path/to/general_projects\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#2-initialize-poetry-if-not-already-set-up","title":"2. Initialize Poetry (if not already set up):","text":"<pre><code>poetry init\n</code></pre> <p>(You can leave dependencies empty for now.)</p>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#3-install-mkdocs-and-the-material-theme","title":"3. Install MkDocs and the Material theme:","text":"<pre><code>poetry add --group dev mkdocs mkdocs-material\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#4-verify-the-installation","title":"4. Verify the installation:","text":"<pre><code>poetry run mkdocs --version\n</code></pre> <p>Expected output:</p> <pre><code>mkdocs, version X.X.X from ...\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#step-2-create-mkdocs-file-structure","title":"Step 2: Create MkDocs File Structure","text":"<p>MkDocs expects content inside a <code>docs/</code> directory.</p>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#1-initialize-mkdocs","title":"1. Initialize MkDocs","text":"<pre><code>mkdocs new .\n</code></pre> <p>This creates:</p> <pre><code>.\n\u251c\u2500\u2500 docs\n\u2502   \u2514\u2500\u2500 index.md  # Homepage content\n\u251c\u2500\u2500 mkdocs.yml  # Configuration file\n</code></pre> <p>We can add extra folders following the same template:</p> <pre><code>\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 guides\n\u2502   \u2502   \u251c\u2500\u2500 deep_learning\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 overview.md\n\u2502   \u2502   \u251c\u2500\u2500 projects\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 overview.md\n\u2502   \u2502   \u251c\u2500\u2500 software_architecture\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 SOLID_principles.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 overview.md\n\u2502   \u2502   \u2514\u2500\u2500 unit_testing\n\u2502   \u2502       \u2514\u2500\u2500 overview.md\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs.yml\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#2-modify-mkdocsyml-to-use-the-material-theme-and-ensure-proper-navigation","title":"2. Modify <code>**mkdocs.yml**</code> to use the Material theme and ensure proper navigation:","text":"<pre><code>site_name: My Software &amp; Data Science Portfolio\nsite_url: https://yourusername.github.io/repository-name/\n\ntheme:\n  name: material\n  features:\n    - navigation.instant\n    - navigation.tabs\n    - navigation.top\n    - search.suggest\n    - search.highlight\n  palette:\n    scheme: default\n  icon:\n    logo: material/book\n\nnav:\n  - Home: index.md\n  - Software Architecture:\n      - Overview: guides/software_architecture/overview.md\n      - SOLID Principles: guides/software_architecture/SOLID_principles.md\n  - Unit Testing:\n      - Overview: guides/unit_testing/overview.md\n  - Deep Learning:\n      - Overview: guides/deep_learning/overview.md\n  - Projects:\n      - Overview: guides/projects/overview.md\n\nextra_css:\n  - assets/stylesheets/extra.css\n\nextra_javascript:\n  - assets/javascripts/extra.js\n\nmarkdown_extensions:\n  - admonition\n  - pymdownx.highlight\n  - pymdownx.superfences\n  - toc:\n      permalink: true\n\nplugins:\n  - search\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#3-create-the-necessary-folders-and-files","title":"3. Create the necessary folders and files:","text":"<pre><code>mkdir -p docs/guides/software_architecture docs/guides/unit_testing docs/guides/deep_learning docs/guides/projects\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#step-3-run-mkdocs-locally","title":"Step 3: Run MkDocs Locally","text":"<p>To preview the documentation before deploying:</p> <pre><code>poetry run mkdocs build --clean\npoetry run mkdocs serve --dev-addr=0.0.0.0:8080\n</code></pre> <p>Open http://0.0.0.0:8080/ in your browser to verify.</p>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#step-4-manual-deployment-to-github-pages","title":"Step 4: Manual Deployment to GitHub Pages","text":""},{"location":"guides/development_and_devops/mkdocs_tutorial/#1-ensure-github-pages-is-enabled","title":"1. Ensure GitHub Pages is enabled:","text":"<ul> <li>Go to Settings &gt; Pages in your repo.</li> <li>Set Branch to <code>gh-pages</code>, and select **/ (root)<code>(NOT/docs</code> !!!).</li> </ul>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#2-disable-jekyll-to-prevent-conflicts","title":"2. Disable Jekyll to prevent conflicts:","text":"<pre><code>touch docs/.nojekyll\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#3-commit-and-push-all-changes","title":"3. Commit and push all changes:","text":"<pre><code>git add .\ngit commit -m \"Initial MkDocs setup\"\ngit push origin main\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#4-deploy-manually","title":"4. Deploy manually:","text":"<pre><code>poetry run mkdocs gh-deploy --force\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#5-verify-deployment-at","title":"5. Verify deployment at:","text":"<pre><code>https://yourusername.github.io/repository-name/\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#step-5-set-up-github-actions-for-automatic-deployment","title":"Step 5: Set Up GitHub Actions for Automatic Deployment","text":"<p>Instead of running <code>mkdocs gh-deploy</code> manually, automate deployment when merging into <code>main</code>.</p>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#1-create-the-workflow-file","title":"1. Create the workflow file:","text":"<pre><code>mkdir -p .github/workflows\nnano .github/workflows/deploy-mkdocs.yml\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#2-add-the-following-configuration","title":"2. Add the following configuration:","text":"<pre><code>name: Deploy MkDocs to GitHub Pages\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install Poetry\n        run: pip install poetry\n\n      - name: Install dependencies\n        run: poetry install --no-root\n\n      - name: Deploy MkDocs\n        run: poetry run mkdocs gh-deploy --force\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#3-commit-and-push-the-workflow","title":"3. Commit and push the workflow:","text":"<pre><code>git add .github/workflows/deploy-mkdocs.yml\ngit commit -m \"Add GitHub Actions for MkDocs deployment\"\ngit push origin main\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#4-test-automatic-deployment","title":"4. Test Automatic Deployment:","text":"<ul> <li>Go to GitHub &gt; Actions tab.</li> <li>Ensure the workflow runs successfully after merging a branch into <code>**main**</code>.</li> <li>Once finished, visit your site to check the updates.</li> </ul>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#common-errors-and-fixes","title":"Common Errors and Fixes","text":""},{"location":"guides/development_and_devops/mkdocs_tutorial/#error-oserror-errno-98-address-already-in-use","title":"** Error: OSError: [Errno 98] Address already in use**","text":"<p>Fix: If MkDocs fails to start due to a port conflict, find and kill the process occupying the port.</p> <pre><code>lsof -i :8080\nkill -9 &lt;PID&gt;\n</code></pre> <p>Then restart MkDocs:</p> <pre><code>poetry run mkdocs serve --dev-addr=0.0.0.0:8080\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#summary-quick-commands","title":"Summary: Quick Commands","text":""},{"location":"guides/development_and_devops/mkdocs_tutorial/#run-mkdocs-locally","title":"Run MkDocs Locally","text":"<pre><code>poetry run mkdocs serve\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#deploy-manually","title":"Deploy Manually","text":"<pre><code>poetry run mkdocs gh-deploy --force\n</code></pre>"},{"location":"guides/development_and_devops/mkdocs_tutorial/#enable-github-actions-for-automatic-deployment","title":"Enable GitHub Actions for Automatic Deployment","text":"<pre><code>git push origin main\n</code></pre> <p>Now you have a fully automated MkDocs site deployed on GitHub Pages! </p>"},{"location":"guides/development_and_devops/overview/","title":"Development &amp; DevOps Overview","text":""},{"location":"guides/development_and_devops/overview/#why-development-devops-matter","title":"Why Development &amp; DevOps Matter","text":"<p>Modern Data Scientists don\u2019t stop at writing Jupyter Notebooks. Training models or creating predictive services also involves maintaining reliable infrastructure, automating deployments, and ensuring smooth collaboration between other Data Scientists, developers and operations teams. By following the guides in this section, you'll gain practical knowledge to:  </p> <p>Improve developer productivity through streamlined workflows. Reduce manual effort by automating repetitive tasks. Maintain clear, well-documented projects for better collaboration. Ensure consistent and scalable deployments.  </p> <p>Stay tuned as I will add more hands-on guides and best practices to this section! </p>"},{"location":"guides/development_and_devops/overview/#what-this-section-covers","title":"What This Section Covers","text":"<p>This section focuses on best practices, tools, and workflows that bridge development and operations, ensuring efficiency, scalability, and maintainability in software projects. Whether you're setting up documentation pipelines, managing version control, or automating workflows, you'll find practical guides and hands-on tutorials here.  </p>"},{"location":"guides/development_and_devops/overview/#topics-included","title":"Topics Included","text":""},{"location":"guides/development_and_devops/overview/#introduction-to-dev-container","title":"Introduction to Dev-Container","text":"<p>Read the Tutorial </p> <p>Modern Data Science and software teams require consistency, reproducibility, and fast on-boarding. A Dev Container packages your code, dependencies, and tooling into a single, versioned environment. This tutorial will walk you thorugh the process of Clone &amp; Run a Dev Container </p>"},{"location":"guides/development_and_devops/overview/#logging-manager-package","title":"Logging Manager Package","text":"<p>Read the Tutorial </p> <p><code>logging_manager</code> is a lightweight, reusable Python package designed to standardize and simplify logging setup across multiple projects.</p>"},{"location":"guides/development_and_devops/overview/#test-driven-development-tdd","title":"Test-Driven Development (TDD)","text":"<p>Read the Tutorial </p> <p>TDD helps developers write reliable, maintainable, and bug-resistant code by testing before implementation. This guide covers:  </p> <ul> <li> <p>The benefits of TDD and real-world examples where it prevents issues.  </p> </li> <li> <p>The Red-Green-Refactor cycle and how to implement it step-by-step.  </p> </li> <li> <p>How to structure test functions using the Given-When-Then (AAA) pattern for clarity. </p> </li> </ul>"},{"location":"guides/development_and_devops/overview/#versioning-tutorial","title":"Versioning Tutorial","text":"<p>Read the Tutorial </p> <p>Effective version control is essential for managing changes in software projects. This guide walks through:  </p> <ul> <li>Best practices for semantic versioning in development.  </li> <li>Setting up automated versioning using tools like Git, GitHub Actions, and tagging strategies.  </li> <li>Integrating versioning into CI/CD workflows for smooth deployments.  </li> </ul>"},{"location":"guides/development_and_devops/overview/#mkdocs-tutorial","title":"MkDocs Tutorial","text":"<p>Read the Tutorial </p> <p>Learn how to set up, configure, and deploy documentation using MkDocs. This tutorial covers:  </p> <ul> <li>Structuring your documentation.  </li> <li>Customizing MkDocs themes and plugins.  </li> <li>Deploying documentation on GitHub Pages or other hosting platforms.  </li> </ul>"},{"location":"guides/development_and_devops/tdd_tutorial/","title":"Test-Driven Development (TDD)","text":""},{"location":"guides/development_and_devops/tdd_tutorial/#why-tdd-matters-motivation-benefits","title":"Why TDD Matters: Motivation &amp; Benefits","text":"<p>Test-Driven Development (TDD) is more than just a testing technique\u2014it\u2019s a development philosophy that helps data scientists and developers to write more reliable and maintainable code.  </p>"},{"location":"guides/development_and_devops/tdd_tutorial/#real-world-scenarios-where-tdd-saves-the-day","title":"Real-World Scenarios Where TDD Saves the Day","text":"<p>Catching Bugs Early: A developer implements a new feature but unknowingly breaks an existing one. With TDD, automated tests would have immediately flagged the issue, preventing a costly debugging session.  </p> <p>Reducing Last-Minute Fixes: Without TDD, testing often happens at the end of development, leading to late-stage surprises and rushed patches before deployment. TDD ensures that tests guide the development process, minimizing late fixes.  </p> <p>Encouraging Better Code Design: Writing tests first forces developers to think about design before implementation, often leading to cleaner, more modular, and reusable code.  </p> <p>Building Confidence for Refactoring: With a strong test suite, developers can refactor code fearlessly, knowing that if something breaks, tests will catch it immediately.  </p> <p>By integrating TDD into development workflows, teams can write higher-quality software with fewer surprises down the road.  </p>"},{"location":"guides/development_and_devops/tdd_tutorial/#what-is-tdd-how-to-implement-it","title":"What is TDD &amp; How to Implement It?","text":""},{"location":"guides/development_and_devops/tdd_tutorial/#test-driven-development-in-5-steps","title":"Test-Driven Development in 5 Steps","text":"<p>TDD follows a simple yet powerful Red-Green-Refactor cycle. According to IBM Developer, here\u2019s how it works:  </p> <ol> <li> <p>Write a Failing Test (Red) \u2192 Define a test for the functionality before writing any code. The test should fail since the implementation doesn\u2019t exist yet.  </p> </li> <li> <p>Write the Minimum Code to Pass the Test (Green) \u2192 Implement the simplest solution that makes the test pass.  </p> </li> <li> <p>Refactor the Code (Refactor) \u2192 Clean up the implementation without changing its behavior to improve readability, efficiency, and maintainability.  </p> </li> <li> <p>Repeat the Cycle \u2192 Add new tests, write code to pass them, and refactor.  </p> </li> <li> <p>Run All Tests Regularly \u2192 Ensuring existing features remain functional as the software evolves.  </p> </li> </ol> <p>This iterative process keeps the codebase in check while ensuring that new features don\u2019t introduce hidden defects.  </p>"},{"location":"guides/development_and_devops/tdd_tutorial/#structuring-test-functions","title":"Structuring Test Functions","text":"<p>When writing tests, keeping them structured is essential for clarity and maintainability. The best practice is to follow the Given-When-Then pattern, also known as Arrange-Act-Assert pattern.  </p>"},{"location":"guides/development_and_devops/tdd_tutorial/#the-importance-of-test-structure","title":"The Importance of Test Structure","text":"<p>Brian Okken explains structuring test functions best in his book Python Testing with pytest:  </p> <p>\"I recommend making sure you keep assertions at the end of test functions. This is such a common recommendation that it has at least two names: Arrange-Act-Assert and Given-When-Then. Bill Wake originally named the Arrange-Act-Assert pattern in 2001. Kent Beck later popularized the practice as part of test-driven development (TDD). Behavior-driven development (BDD) uses the terms Given-When-Then, a pattern from Ivan Moore, popularized by Dan North. Regardless of the names of the steps, the goal is the same: separate a test into stages.\" </p>"},{"location":"guides/development_and_devops/tdd_tutorial/#givenwhenthen-pattern-explained","title":"Given/When/Then Pattern Explained","text":"<ul> <li>Given (Arrange): Set up the initial conditions for the test (mock data, dependencies, environment).  </li> <li>When (Act): Perform the actual action you are testing (calling a function, making a request).  </li> <li>Then (Assert): Verify that the expected outcome occurred.  </li> </ul>"},{"location":"guides/development_and_devops/tdd_tutorial/#example-applying-the-given-when-then-pattern-in-python","title":"Example: Applying the Given-When-Then Pattern in Python","text":"<p>Below is a simple TDD-based test using <code>pytest</code> that follows Given-When-Then:  </p> <pre><code>import pytest\n\n# Function to test\ndef add_numbers(a, b):\n    return a + b\n\n# Test using Given-When-Then structure\ndef test_add_numbers():\n    # Given (Arrange): two integer numbers.\n    num1 = 3\n    num2 = 5\n\n    # When (Act): use the function add_number to sum both values.\n    result = add_numbers(num1, num2)\n\n    # Then (Assert): we are expecting 3 + 5 = 8 as a result.\n    assert result == 8\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/","title":"Guide: Setting Up Automatic Versioning in GitHub with GitHub Actions","text":"<p>This guide explains how to implement automatic versioning for your projects using GitHub Actions. This setup ensures that:</p> <ul> <li>A new Git tag is created whenever a version change is detected.</li> <li>A GitHub Release is automatically generated, attaching the <code>CHANGELOG.md</code>.</li> <li>Versioning works dynamically for multiple projects, avoiding hardcoded project names.</li> </ul>"},{"location":"guides/development_and_devops/versioning_tutorial/#step-1-create-the-necessary-files","title":"Step 1: Create the Necessary Files","text":""},{"location":"guides/development_and_devops/versioning_tutorial/#1-add-a-version-file-for-your-package","title":"1. Add a <code>VERSION</code> File for Your Package","text":"<p>Each package should have its own <code>VERSION</code> file.</p> <p>Example for <code>database_toolkit/</code>:</p> <pre><code>echo \"1.0.0\" &gt; database_toolkit/VERSION\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#2-add-a-changelogmd-file-for-your-package","title":"2. Add a <code>CHANGELOG.md</code> File for Your Package","text":"<pre><code>touch database_toolkit/CHANGELOG.md\n</code></pre> <p>Example content:</p> <pre><code># **Changelog**\n\nAll notable changes to this project will be documented in this file.\n\n## **[Unreleased]**\n- Describe upcoming changes here.\n\n## **[0.1.0]** - YYYY-MM-DD\n### **Added**\n- Initial release of ` project_name `.\n- Implemented core utilities.\n- Added unit tests.\n\n### **Fixed**\n- N/A\n\n### **Changed**\n- N/A\n\n### **Removed**\n- N/A\n\n---\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#3-create-a-projects-file","title":"3. Create a <code>PROJECTS</code> File","text":"<p>This file will list all the packages that require versioning. Heads-up: the PROJECT file should live in the root of the repository.</p> <pre><code>echo \"database_toolkit\" &gt; PROJECTS\n</code></pre> <p>If you have multiple projects, list them one per line:</p> <pre><code>database_toolkit\nanother_project\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#step-2-create-the-github-actions-workflow","title":"Step 2: Create the GitHub Actions Workflow","text":""},{"location":"guides/development_and_devops/versioning_tutorial/#1-create-the-workflow-file","title":"1.  Create the Workflow File","text":"<p>In the root of your repository, create the following Workflow File:</p> <pre><code>mkdir -p .github/workflows\nnano .github/workflows/versioning.yml\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#2-add-the-following-configuration","title":"2. Add the Following Configuration","text":"<pre><code>name: Automatic Versioning\n\non:\n  push:\n    branches:\n      - main  # Runs when merging into main\n\npermissions:\n  contents: write\n\njobs:\n  tag_version:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Read project names\n        id: read_projects\n        run: echo \"PROJECTS=$(cat PROJECTS)\" &gt;&gt; $GITHUB_ENV\n\n      - name: Process Each Project\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Set the token for authentication\n        run: |\n          for project in ${{ env.PROJECTS }}; do\n            echo \"Processing project: $project\"\n\n            # Read version file\n            VERSION=$(cat $project/VERSION)\n            echo \"Detected version $VERSION for $project\"\n\n            # Check if the tag already exists in remote\n            if git ls-remote --tags origin | grep -q \"refs/tags/v${VERSION}\"; then\n              echo \"Tag v${VERSION} already exists for $project, skipping...\"\n              continue\n            fi\n\n            # Create a Git tag\n            git config --global user.name \"github-actions\"\n            git config --global user.email \"github-actions@github.com\"\n            git tag -a \"v${VERSION}\" -m \"Release version ${VERSION} for ${project}\"\n            git push origin \"v${VERSION}\"\n\n            # Create GitHub release\n            gh release create \"v${VERSION}\" --title \"Release ${VERSION}\" --notes-file $project/CHANGELOG.md\n          done\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#step-3-commit-and-push-the-workflow","title":"Step 3: Commit and Push the Workflow","text":"<pre><code>git add PROJECTS .github/workflows/versioning.yml database_toolkit/VERSION database_toolkit/CHANGELOG.md\ngit commit -m \"Add automatic versioning workflow\"\ngit push origin adding_versioning\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#step-4-merge-the-workflow-into-main","title":"Step 4: Merge the Workflow into <code>main</code>","text":"<pre><code>git checkout main\ngit merge adding_versioning\ngit push origin main\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#step-5-test-the-workflow","title":"Step 5: Test the Workflow","text":""},{"location":"guides/development_and_devops/versioning_tutorial/#1-update-the-version-in-version-file","title":"1. Update the Version in <code>VERSION</code> File","text":"<pre><code>echo \"1.0.1\" &gt; database_toolkit/VERSION\ngit add database_toolkit/VERSION\ngit commit -m \"Bump version to 1.0.1\"\ngit push origin main\n</code></pre>"},{"location":"guides/development_and_devops/versioning_tutorial/#2-check-the-github-actions-execution","title":"2.  Check the GitHub Actions Execution","text":"<ul> <li>Go to GitHub &gt; Actions.</li> <li>The workflow should run and:</li> <li>Detect the version change.</li> <li>Create a new Git tag (<code>v1.0.1</code>).</li> <li>Publish a GitHub Release, attaching <code>CHANGELOG.md</code>.</li> </ul> <p>Now you have a fully automated versioning system in your GitHub repository! </p>"},{"location":"guides/software_architecture/SOLID_principles/","title":"SOLID Principles","text":"<p>The SOLID principles are foundational for creating maintainable and scalable software. These principles guide developers in designing software structures that:</p> <ul> <li>Tolerate change</li> <li>Are easy to understand</li> <li>Serve as reusable components in various software systems</li> </ul> <p>Applying SOLID principles leads to cleaner, more adaptable code and a smoother development experience.</p>"},{"location":"guides/software_architecture/SOLID_principles/#single-responsibility-principle-srp","title":"Single Responsibility Principle (SRP)","text":"<p>Definition: A class should have only one reason to change, meaning it should have a single responsibility.</p>"},{"location":"guides/software_architecture/SOLID_principles/#counter-example-srp-violation","title":"Counter-Example (SRP Violation)","text":"<pre><code># A class handling multiple responsibilities\nclass InvoiceProcessor:\n    def calculate_total(self, items):\n        # logic for calculate total items\n        pass\n\n    def generate_pdf(self, invoice):\n        # logic for generate a pdf\n        pass\n\n    def send_email(self, invoice, email)\n        # logic to send a email\n        pass   \n</code></pre> <p>Problem: If you need to change how PDFs are generate, you might risk breaking other functionality like email sender.</p>"},{"location":"guides/software_architecture/SOLID_principles/#corrected-example-applying-srp","title":"Corrected Example (Applying SRP)","text":"<pre><code># Good example: Separating responsibilities\nclass InvoiceCalculator:\n    def calculate_total(self, items):\n        # logic for generating report\n        pass\n\nclass InvoicePDFGenerator:\n    def generate_pdf(self, invoice):\n        # logic for generate a pdf\n        pass\n\nclass EmailSender:\n    def send_email(self, invoice, email):\n        # logic for send emails\n        pass\n</code></pre>"},{"location":"guides/software_architecture/SOLID_principles/#key-takeaway","title":"Key Takeaway","text":"<p>A module should be responsible to one and only one actor. This ensures clarity, maintainability, and flexibility.</p>"},{"location":"guides/software_architecture/SOLID_principles/#open-closed-principle-ocp","title":"Open-Closed Principle (OCP)","text":"<p>Definition: Software entities should be open for extension but closed for modification.</p>"},{"location":"guides/software_architecture/SOLID_principles/#counter-example-ocp-violation","title":"Counter-Example (OCP Violation)","text":"<pre><code>class Shape:\n    def __init__(self, shape_type: str, dimension: float):\n        self.shape_type = shape_type\n        self.dimension = dimension\n\n    def calculate_area(self) -&gt; float:\n        if self.shape_type == \"circle\":\n            return 3.14 * (self.dimension ** 2)\n        elif self.shape_type == \"square\":\n            return self.dimension ** 2\n        else:\n            raise ValueError(\"Unknown shape type\")\n</code></pre> <p>Problem: Adding a new shape requires modifying the <code>calculate_area</code> method, violating OCP.</p>"},{"location":"guides/software_architecture/SOLID_principles/#corrected-example-applying-ocp","title":"Corrected Example (Applying OCP)","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    @abstractmethod\n    def calculate_area(self) -&gt; float:\n        pass\n\nclass Circle(Shape):\n    def __init__(self, radius: float):\n        self.radius = radius\n\n    def calculate_area(self) -&gt; float:\n        return 3.14 * (self.radius ** 2)\n\nclass Square(Shape):\n    def __init__(self, side: float):\n        self.side = side\n\n    def calculate_area(self) -&gt; float:\n        return self.side ** 2\n</code></pre>"},{"location":"guides/software_architecture/SOLID_principles/#key-takeaway_1","title":"Key Takeaway","text":"<p>OCP ensures new functionality is added through extensions rather than modifications, making the system more maintainable.</p>"},{"location":"guides/software_architecture/SOLID_principles/#liskov-substitution-principle-lsp","title":"Liskov Substitution Principle (LSP)","text":"<p>Definition: Derived classes must be substitutable for their base classes without affecting correctness.</p>"},{"location":"guides/software_architecture/SOLID_principles/#counter-example-lsp-violation","title":"Counter-Example (LSP Violation)","text":"<pre><code>class Bird:\n    def fly(self) -&gt; str:\n        return \"I'm flying!\"\n\nclass Penguin(Bird):\n    def fly(self) -&gt; str:\n        raise NotImplementedError(\"Penguins can't fly\")\n</code></pre> <p>Problem: <code>Penguin</code> violates LSP because it inherits behavior it cannot fulfill.</p>"},{"location":"guides/software_architecture/SOLID_principles/#corrected-example-applying-lsp","title":"Corrected Example (Applying LSP)","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass Bird(ABC):\n    @abstractmethod\n    def move(self) -&gt; str:\n        pass\n\nclass Sparrow(Bird):\n    def move(self) -&gt; str:\n        return \"I'm flying!\"\n\nclass Penguin(Bird):\n    def move(self) -&gt; str:\n        return \"I'm swimming!\"\n</code></pre>"},{"location":"guides/software_architecture/SOLID_principles/#key-takeaway_2","title":"Key Takeaway","text":"<p>Subclasses should extend behavior without altering the expected behavior of the base class.</p>"},{"location":"guides/software_architecture/SOLID_principles/#interface-segregation-principle-isp","title":"Interface Segregation Principle (ISP)","text":"<p>Definition: Clients should not be forced to depend on interfaces they do not use.</p>"},{"location":"guides/software_architecture/SOLID_principles/#counter-example-isp-violation","title":"Counter-Example (ISP Violation)","text":"<pre><code>class Animal:\n    def fly(self) -&gt; None:\n        pass\n\n    def swim(self) -&gt; None:\n        pass\n\nclass Bird(Animal):\n    def fly(self) -&gt; None:\n        print(\"I'm flying!\")\n\n    def swim(self) -&gt; None:\n        raise NotImplementedError(\"Birds can't swim\")\n</code></pre> <p>Problem: <code>Bird</code> is forced to implement <code>swim()</code>, which it doesn't need.</p>"},{"location":"guides/software_architecture/SOLID_principles/#corrected-example-applying-isp","title":"Corrected Example (Applying ISP)","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass Flyable(ABC):\n    @abstractmethod\n    def fly(self) -&gt; None:\n        pass\n\nclass Swimmable(ABC):\n    @abstractmethod\n    def swim(self) -&gt; None:\n        pass\n\nclass Bird(Flyable):\n    def fly(self) -&gt; None:\n        print(\"I'm flying!\")\n\nclass Fish(Swimmable):\n    def swim(self) -&gt; None:\n        print(\"I'm swimming!\")\n</code></pre>"},{"location":"guides/software_architecture/SOLID_principles/#key-takeaway_3","title":"Key Takeaway","text":"<p>Create specific interfaces rather than forcing classes to implement unnecessary methods.</p>"},{"location":"guides/software_architecture/SOLID_principles/#dependency-inversion-principle-dip","title":"Dependency Inversion Principle (DIP)","text":"<p>Definition: High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions.</p>"},{"location":"guides/software_architecture/SOLID_principles/#counter-example-dip-violation","title":"Counter-Example (DIP Violation)","text":"<pre><code># Low-level class\nclass EmailSender:\n    def send_email(self, message: str) -&gt; None:\n        print(f\"Sending email: {message}\")\n\n# High-level class\nclass NotificationService:\n    def __init__(self):\n        self.email_sender = EmailSender()\n\n    def notify(self, message: str) -&gt; None:\n        self.email_sender.send_email(message)\n</code></pre> <p>Problem: <code>NotificationService</code> is tightly coupled to <code>EmailSender</code>, making it harder to extend or change notification types.</p>"},{"location":"guides/software_architecture/SOLID_principles/#corrected-example-applying-dip","title":"Corrected Example (Applying DIP)","text":"<pre><code>from abc import ABC, abstractmethod\n\n# Abstraction\nclass Notifier(ABC):\n    @abstractmethod\n    def notify(self, message: str) -&gt; None:\n        pass\n\n# Low-level class\nclass EmailSender(Notifier):\n    def notify(self, message: str) -&gt; None:\n        print(f\"sending email: {message}\")\n\nclass SMSNotifier(Notifier):\n    def notify(self, message: str) -&gt; None:\n        print(f\"sending SMS: {message}\")\n\n# High-level class\nclass NotificationService:\n    def __init__(self, notifier: Notifier):\n        self.notifier = notifier\n\n    def notify(self, message: str) -&gt; None:\n        self.notifier.notify(message)\n</code></pre>"},{"location":"guides/software_architecture/SOLID_principles/#key-takeaway_4","title":"Key Takeaway","text":"<p>DIP helps create modular, loosely coupled systems that are more adaptable and maintainable.</p>"},{"location":"guides/software_architecture/overview/","title":"Software Architecture: The (Invisible) Force Driving Business Success","text":""},{"location":"guides/software_architecture/overview/#beyond-code-why-software-architecture-should-be-a-strategic-asset","title":"Beyond Code: Why Software Architecture should be a strategic asset","text":"<p>Many Business leaders are used to focusing on acquiring new customers, staying ahead of competitors, and rolling out innovative features. However, these ambitions can quickly be derailed by issues like:</p> <ul> <li>Slow feature development due to complex, tangled codebases.  </li> <li>High maintenance costs as old systems require frequent fixes and rewrites.  </li> <li>Burned-out teams struggling to keep up with technical debt.</li> </ul> <p>Many businesses are not aware that they are operating in a cycle where software limitations restrict business growth rather than enabling it. But it doesn\u2019t have to be this way. This is where software architecture comes in. </p>"},{"location":"guides/software_architecture/overview/#the-role-of-software-architecture-in-business-success","title":"The Role of Software Architecture in Business Success","text":"<p>Software architecture is not just about writing code \u2013 it\u2019s about creating a foundation that enables businesses to scale, innovate, and adapt efficiently. A well-designed architecture should:</p> <ul> <li> <p>Support Change \u2013 Systems should be built to evolve, not just to function today.</p> </li> <li> <p>Enable Innovation \u2013 A flexible architecture makes it easier to integrate new technologies and explore new business opportunities.</p> </li> <li> <p>Minimize Costs and Downtime \u2013 Reducing complexity in software design prevents costly rewrites and system failures.</p> </li> <li> <p>Scale Seamlessly \u2013 As customer demands and product features grow, the system should expand without breaking or requiring excessive rework.</p> </li> <li> <p>Foster Parallel Development \u2013 Clear modular boundaries and decoupled components empower teams to work independently, reducing bottlenecks and making collaboration smoother.</p> </li> </ul>"},{"location":"guides/software_architecture/overview/#how-can-businesses-ensure-these-principles-are-built-into-their-software","title":"How Can Businesses Ensure These Principles Are Built into Their Software?","text":"<p>Developing an adaptable, scalable, and efficient software architecture is not an accident \u2013 it\u2019s a strategic effort. The key question is: What strategies can companies adopt to ensure their software architecture supports long-term business success?</p>"},{"location":"guides/software_architecture/overview/#strategies-to-align-software-architecture-with-business-goals","title":"Strategies to Align Software Architecture with Business Goals","text":"<p>To ensure that software supports - not hinders - business objectives, here are three essential strategies:</p> <p>Prioritize Software Architecture as a Business Asset Software architecture shouldn\u2019t be an afterthought - it needs dedicated time and resources. Data Science, Development, and DevOps teams should be empowered to invest in architectural planning, rather than just focusing on immediate feature development.</p> <ul> <li>If the company doesn\u2019t allocate time for this, good architecture won\u2019t emerge organically - it must be intentionally built.</li> </ul> <p>Foster Cross-Team Collaboration in Architectural Decisions Software architecture is not just a technical concern - it directly impacts business agility and efficiency. Involve both business and technical teams in architectural discussions to ensure the system aligns with evolving market needs. This means:</p> <ul> <li> <p>Making software design sessions inclusive - product managers, business analysts, and other stakeholders should participate, not just the tech team.</p> </li> <li> <p>Ensuring business needs drive architectural choices, rather than forcing business requirements to fit into rigid technical constraints.</p> </li> </ul> <p>Adopt Iterative, Feedback-Driven Architecture Development Instead of aiming for a perfect architecture from day one, start small and refine over time. A lean, iterative approach ensures that architecture evolves based on real-world needs rather than assumptions.</p> <ul> <li> <p>Build a \"Minimum Viable Architecture\" \u2013 Start with a skateboard version (inspired by Agile methodologies) rather than an over-engineered solution.</p> </li> <li> <p>Engage stakeholders early \u2013 Let business teams interact with the system, provide feedback, and influence design improvements.</p> </li> <li> <p>Continuously refine \u2013 As the system scales, fine-tune the architecture rather than making drastic, disruptive overhauls.</p> </li> </ul> <p>A strong software architecture doesn\u2019t just serve the development team - it serves the entire business. By treating it as a strategic asset, companies can innovate faster, scale efficiently, and stay ahead of the competition.</p>"},{"location":"guides/software_architecture/overview/#the-role-and-the-competencies-of-a-software-architect","title":"The role and the competencies of a Software Architect","text":"<p>A software architect is an experienced developer who continues to engage with the codebase while guiding the team - and not a disconnected strategist. The primary role of a software architect is to maximize developer productivity, ensure system scalability, and align technical decisions with business objectives - ensuring that the architecture supports evolving business needs rather than becoming a bottleneck. They facilitate technical decision-making, mentor developers, and establish best practices that keep the system flexible, maintainable, and scalable.</p> <p>Robert C. Martin (\"Clean Architecture\") explains it best:  </p> <p>\u201cA software architect is a programmer and continues to be a programmer. Never fall for the lie that suggests that software architects pull back from code to focus on higher-level issues. They do not! Software architects are the best programmers and continue to take programming tasks, while they also guide the rest of the team toward a design that maximizes productivity. They do this because they cannot do their jobs properly if they are not experiencing the problems that they are creating for the rest of the programmers.\u201d  </p> <p>In essence, great software architects lead by example - they don\u2019t just define architecture; they work within it, refine it, and ensure it empowers both developers and the business.</p>"},{"location":"guides/work_with_me/ai_foundations_and_capability_building/","title":"AI Foundations","text":"<p>Helping teams, leaders, and companies lay the right foundation for AI - before they write a single line of code.</p>"},{"location":"guides/work_with_me/ai_foundations_and_capability_building/#conduct-ai-readiness-assessments","title":"Conduct AI Readiness Assessments","text":"<p>Evaluate your infrastructure, data, talent, and tooling to ensure your company is truly ready to adopt AI - before you invest.</p> <p>You\u2019re excited about AI. Your team is curious. Maybe you\u2019ve even started experimenting. But deep down, you might be asking: \u201cAre we actually ready for this?\u201d</p> <p>That\u2019s exactly what an AI Readiness Assessment is for.</p> <p>I\u2019ll help you evaluate your current business opportunities, data quality, team skills, and existing tools so you know what\u2019s working, what\u2019s missing, and what needs to happen next.</p> <p>Whether you\u2019re at step zero or recovering from a failed AI initiative, I\u2019ll give you a clear, actionable roadmap - so your next investment in AI is a smart one.</p>"},{"location":"guides/work_with_me/ai_foundations_and_capability_building/#design-innovation-labs","title":"Design Innovation Labs","text":"<p>Design and lead the creation of internal R&amp;D or AI innovation units - so your company can explore, experiment, and execute bold ideas with confidence.</p> <p>You have many AI ideas in your company - from predictive maintenance to smart scheduling and customer churn models. But there\u2019s no clear process to take those ideas from conversation to prototype. With no structure, those ideas might live in spreadsheets, powerpoint presentations, or \u201cexperimental notebooks\u201d that will never leave the computer of somebody. And leadership keep asking: \u201cWhy aren\u2019t we doing more with AI?\u201d</p> <p>That\u2019s where I came in.</p> <p>I work with your teams to design and launch an AI Innovation Lab - starting with your business priorities and internal talent. I help you define a intake process, explore bold ideas, define success metrics, roadmaps, and the tech stack that supports fast experimentation.</p> <p>You get a repeatable innovation pipeline, not just one-off experiments.</p>"},{"location":"guides/work_with_me/ai_foundations_and_capability_building/#coaching-technical-team-for-ai-excellence","title":"Coaching Technical Team for AI Excellence","text":"<p>Mentor and upskill data scientists and ML engineers to build scalable and maintainable AI systems - not only experiments.</p> <p>Great models aren\u2019t enough - they need to live in systems, evolve with feedback, and scale with users.</p> <p>That\u2019s where I come in.</p> <p>I coach technical teams to apply product thinking to their AI work - combining scientific creativity with software best practices. We introduce versioning, modular design, testing, and lightweight MLOps so your AI projects stop being isolated experiments\u2026 and start becoming real features that users rely on.</p> <p>Because in the end, AI that works isn\u2019t a proof of concept - it\u2019s a product.</p>"},{"location":"guides/work_with_me/ai_foundations_and_capability_building/#training-non-tech-teams-for-ai-culture-adoption","title":"Training Non-Tech Teams for AI Culture Adoption","text":"<p>Deliver workshops and talks that build data and AI fluency across business teams - so everyone can participate in AI success.</p> <p>Your data scientists are building models. Your business leaders are making decisions. But if they\u2019re not speaking the same language, even the best AI won\u2019t move the needle. AI isn\u2019t just a technical transformation - it\u2019s a cultural one.</p> <p>That\u2019s where I help.</p> <p>I design workshops for business stakeholders - helping them understand what AI is (and isn\u2019t), how to collaborate with technical teams, and how to identify opportunities that make sense for the business.</p> <p>Because the success of AI isn\u2019t just about good models - it\u2019s about aligned people.</p>"},{"location":"guides/work_with_me/ai_systems_that_work/","title":"AI Systems That Work","text":"<p>From prototype to production - building AI tools that fit your architecture, scale with your needs, and deliver business value.</p>"},{"location":"guides/work_with_me/ai_systems_that_work/#from-intuition-to-hypothesis-and-prototype","title":"From Intuition to Hypothesis and Prototype","text":"<p>Explore bold, high-risk, high-reward ML ideas - and transform unstructured curiosity into structured insight.</p> <p>Some ideas aren\u2019t ready to be shipped. But they are ready to be explored. You have an intuition - about human behavior, chemical interaction, education patterns, social data - and you\u2019re wondering: Can machine learning help us make sense of this?</p> <p>That\u2019s where I come in.</p> <p>I help teams explore questions that don\u2019t yet have a roadmap. Together, we frame your idea as an ML research question, gather or simulate data, and run exploratory models to test what\u2019s worth pursuing further.</p> <p>You don\u2019t just get code. You get insight, analysis, and structure - and a clear recommendation on what to do next.</p>"},{"location":"guides/work_with_me/ai_systems_that_work/#from-prototype-to-product","title":"From Prototype to Product","text":"<p>Refactor and re-engineer AI prototypes or research code into scalable, maintainable products.</p> <p>You\u2019ve proven your model works. The numbers are there. The logic makes sense. But outside your team, no one\u2019s using it - because it\u2019s not packaged, tested, or integrated with an U.I. tool or other tools that the business already relies on.</p> <p>That\u2019s where I come in.</p> <p>I take your existing prototype - in whatever shape it\u2019s in - and turn it into a deployable AI product: one that runs in production, connects to real systems, and is robust enough to evolve over time.</p> <p>So your business doesn\u2019t just \u201chave a model.\u201d It has a working product.</p>"},{"location":"guides/work_with_me/ai_systems_that_work/#develop-simulation-digital-twin-environments","title":"Develop Simulation &amp; Digital Twin Environments","text":"<p>Develop virtual environments to simulate processes, test decisions, and train AI agents like reinforcement learning systems.</p> <p>You want to test a new operating strategy. Or see how an AI system reacts to failure. But in the real world, every test is a risk in time, cost, or safety.</p> <p>That\u2019s why I build digital twins: realistic, programmable environments where your team can experiment freely, fail safely, and learn fast.</p> <p>It\u2019s not about simulating for simulation\u2019s sake. It\u2019s about creating a low-risk sandbox where bold decisions become informed ones - before they hit your operations. Before you deploy, we simulate. So when it\u2019s live, it works.</p>"},{"location":"guides/work_with_me/ai_systems_that_work/#design-business-wide-visualization-monitoring-dashboards","title":"Design Business-Wide Visualization &amp; Monitoring Dashboards","text":"<p>Design and build centralized dashboards that unify operational data, performance metrics, and business KPIs - whether or not AI is involved.</p> <p>You don\u2019t always need machine learning to move the needle. Often, what your team really needs is clarity - the ability to see what\u2019s happening, spot trends, and make smarter decisions.</p> <p>I help companies unlock the value of the data they already have - by building insightful, interactive dashboards that tie together operations, finance, customer behavior, and more.</p> <p>Even if AI is on the roadmap, dashboards are the first step to getting everyone aligned, asking the right questions, and spotting the opportunities that were already there.</p> <p>Sometimes, smarter decisions just need better visibility.</p>"},{"location":"guides/work_with_me/ai_systems_that_work/#design-custom-integrated-ai-projects","title":"Design Custom integrated AI projects","text":"<p>Design and build custom AI systems or packages that plug into your existing business architecture.</p> <p>You\u2019ve invested in a third-party AI tool. On paper, it promises everything - predictions, dashboards, automation. But in practice? It\u2019s still not integrated.</p> <p>Your workflows are too specific. Your data model is unique. The vendor\u2019s API doesn\u2019t quite adapt. Now your team is stuck again in manual process and making decisions by gut.</p> <p>Sound familiar? This is exactly where I come in.</p> <p>I design and build custom AI modules that start with how your business works. Whether you need a forecasting engine, an internal Python package that wraps ML logic, a smarter way to use your existing data, or a microservice that slots into your backend - we build it to match your architecture and integrate cleanly.</p> <p>You keep control. Your team stays productive. And this time, there\u2019s no \u201cmanual processl\u201d in the workflow.</p>"},{"location":"guides/work_with_me/overview/","title":"AI Solutions That Work","text":"<p>I build AI products that start with business problems - and end with measurable results.</p>"},{"location":"guides/work_with_me/overview/#core-services","title":"Core Services","text":""},{"location":"guides/work_with_me/overview/#ai-systems-products","title":"AI Systems &amp; Products","text":"<ul> <li>Intuition to Prototype: Explore bold ML ideas and transform unstructured curiosity into structured insight</li> <li>Prototype to Production: Transform research code into scalable, deployable AI products</li> <li>Custom AI Integration: Build AI modules that plug into your existing architecture</li> <li>Digital Twins &amp; Simulation: Create safe environments for testing AI systems and business decisions</li> <li>Data Visualization: Design dashboards that unlock insights from your existing data</li> </ul>"},{"location":"guides/work_with_me/overview/#ai-foundations","title":"AI Foundations","text":"<ul> <li>AI Readiness Assessment: Evaluate your infrastructure, data, and team readiness before investing</li> <li>Innovation Lab Design: Create internal R&amp;D processes for systematic AI experimentation</li> <li>Technical Team Coaching: Mentor data scientists to build maintainable AI systems</li> <li>AI Culture Training: Help non-technical teams understand and collaborate on AI initiatives</li> </ul>"},{"location":"guides/work_with_me/overview/#my-approach","title":"My Approach","text":"<p>I focus on business outcomes over technical complexity. Whether you're exploring bold AI ideas, scaling existing prototypes, or building foundational capabilities - I help you move from intuition to implementation with clear, measurable results.</p> <p>From exploration to execution. From prototype to product. From individual experiments to organizational capability.</p>"}]}